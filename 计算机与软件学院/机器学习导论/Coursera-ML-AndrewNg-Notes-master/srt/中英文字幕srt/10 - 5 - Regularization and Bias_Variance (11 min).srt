1
00:00:00,390 --> 00:00:02,440
You've seen how regularization can help
现在你应该已经知道(字幕整理：中国海洋大学 黄海广,haiguang2000@qq.com )

2
00:00:02,610 --> 00:00:04,660
prevent overfitting, but how
算法正则化可以有效地防止过拟合

3
00:00:04,960 --> 00:00:06,230
does it affect the bias and
但正则化跟算法的偏差和方差

4
00:00:06,460 --> 00:00:08,070
variance of a learning algorithm?
又有什么关系呢？

5
00:00:08,630 --> 00:00:09,890
In this video, I like to
在这段视频中

6
00:00:10,020 --> 00:00:11,180
go deeper into the issue
我想更深入地

7
00:00:11,550 --> 00:00:13,300
of bias and variance, and
探讨一下偏差和方差的问题

8
00:00:13,520 --> 00:00:14,450
talk about how it interacts
讨论一下两者之间

9
00:00:15,070 --> 00:00:15,880
with, and is effected by,
是如何相互影响的

10
00:00:16,070 --> 00:00:18,470
the regularization of your learning algorithm.
以及和算法的正则化之间的相互关系

11
00:00:22,484 --> 00:00:23,704
Suppose we're fitting a high
假如我们要对这样一个

12
00:00:25,814 --> 00:00:27,314
order polynomial like that shown
高阶的多项式

13
00:00:27,744 --> 00:00:28,944
here, but to prevent
进行拟合

14
00:00:29,384 --> 00:00:30,394
overfitting, we're going to
为了防止过拟合现象

15
00:00:30,534 --> 00:00:32,164
use regularization, like that shown
我们要使用如幻灯片所示的正则化

16
00:00:32,534 --> 00:00:33,694
here, so we have this regularization
因此我们试图通过这样一个正则化项

17
00:00:34,504 --> 00:00:35,674
term to try to
来让参数的值

18
00:00:35,714 --> 00:00:37,604
keep the values of the parameters small.
尽可能小

19
00:00:38,044 --> 00:00:39,724
And as usual, the regularization sums
正则化项的求和范围

20
00:00:40,094 --> 00:00:41,514
from j equals 1 to
照例取为j等于1到m

21
00:00:41,614 --> 00:00:42,804
m rather than j equals 0
而非j等于0到m

22
00:00:42,924 --> 00:00:45,454
to m.  Let's consider three cases.
然后我们来分析以下三种情形

23
00:00:46,064 --> 00:00:47,914
The first is the case of
第一种情形是

24
00:00:47,984 --> 00:00:49,224
a very large value of the
正则化参数lambda

25
00:00:49,284 --> 00:00:51,574
regularization parameter lambda, such
取一个比较大的值

26
00:00:51,814 --> 00:00:52,964
as if lambda were
比如lambda的值

27
00:00:53,114 --> 00:00:53,924
equal to 10,000s of huge value.
取为10000甚至更大

28
00:00:53,704 --> 00:00:56,024
In this
在这种情况下

29
00:00:56,094 --> 00:00:57,234
case, all of these
所有这些参数

30
00:00:57,384 --> 00:00:58,974
parameters, theta 1, theta 2,
包括theta1 theta2

31
00:00:59,304 --> 00:01:00,034
theta 3 and so on will
theta3 等等

32
00:01:00,214 --> 00:01:02,114
be heavily penalized and
将被大大惩罚

33
00:01:02,294 --> 00:01:04,384
so, what ends up with most
其结果是

34
00:01:04,834 --> 00:01:06,164
of these parameter values being close
这些参数的值

35
00:01:06,514 --> 00:01:08,724
to 0 and the hypothesis will be
将近似等于0

36
00:01:08,904 --> 00:01:09,664
roughly h or x
并且假设模型

37
00:01:10,004 --> 00:01:11,704
just equal or approximately equal
h(x)的值将等于或者近似等于

38
00:01:11,994 --> 00:01:13,254
to theta 0, and so we
theta0的值

39
00:01:13,414 --> 00:01:15,284
end up a hypothesis that more
因此我们最终得到的假设函数

40
00:01:15,524 --> 00:01:16,974
or less looks like that. This is more or
应该是这个样子

41
00:01:17,094 --> 00:01:19,854
less a flat, constant straight line.
近似是一条平滑的直线

42
00:01:20,000 --> 00:01:21,910
And so this hypothesis has high
因此这个假设处于高偏差

43
00:01:22,250 --> 00:01:24,220
bias and a value underfits this data set.
对数据集欠拟合

44
00:01:24,560 --> 00:01:26,110
So the horizontal straight
因此这条水平直线

45
00:01:26,430 --> 00:01:27,400
line is just not a very
对这个数据集来讲

46
00:01:27,530 --> 00:01:29,690
good model for this data set.
不是一个好的假设模型

47
00:01:30,290 --> 00:01:31,460
At the other extreme beam is if we have
与之对应的另一种情况是

48
00:01:31,840 --> 00:01:33,150
a very small value of
如果我们的lambda值很小

49
00:01:33,440 --> 00:01:34,900
lambda, such as if lambda
比如说lambda的值

50
00:01:35,300 --> 00:01:37,220
were equal to 0.
等于0的时候

51
00:01:37,310 --> 00:01:38,530
In that case, given that we're
在这种情况下

52
00:01:38,670 --> 00:01:39,830
fitting a high order polynomial,
如果我们要拟合一个高阶多项式的话

53
00:01:39,980 --> 00:01:41,280
this is a
那么此时我们通常会处于

54
00:01:41,530 --> 00:01:43,180
usual overfitting setting.
过拟合的情况

55
00:01:44,340 --> 00:01:45,580
In that case, given that we're
在这种情况时

56
00:01:45,780 --> 00:01:46,830
fitting a high order polynomial,
在拟合一个高阶多项式时

57
00:01:47,760 --> 00:01:49,640
basically without regularization or with
如果没有进行正则化

58
00:01:49,820 --> 00:01:51,760
very minimal regularization, we end
或者正则化程度很微小的话

59
00:01:51,940 --> 00:01:53,770
up with our usual high variance, overfitting
我们通常会得到高方差和过拟合的结果

60
00:01:54,400 --> 00:01:55,490
setting, because basically if lambda is
因为通常来说

61
00:01:56,220 --> 00:01:57,240
equal to zero, we are just
lambda的值等于0

62
00:01:57,380 --> 00:01:59,900
fitting with our regularization so
相当于没有正则化项

63
00:02:00,030 --> 00:02:06,050
that overfits the hypothesis
因此是过拟合假设

64
00:02:07,290 --> 00:02:08,160
and is only if we have some
只有当我们取一个中间大小的

65
00:02:08,320 --> 00:02:10,310
intermediate value of lambda that is neither too large nor too small that we end up with parameters theta
既不大也不小的lambda值时

66
00:02:10,810 --> 00:02:11,970
that we end up that give us a reasonable
我们才会得到一组合理的

67
00:02:12,360 --> 00:02:13,640
fit to this data.
对数据刚好拟合的theta参数值

68
00:02:14,000 --> 00:02:14,920
So how can we automatically
那么我们应该怎样

69
00:02:15,720 --> 00:02:17,190
choose a good value
自动地选择出一个最合适的

70
00:02:17,690 --> 00:02:19,200
for the regularization parameter lambda?
正则化参数lambda呢

71
00:02:20,210 --> 00:02:22,480
Just to reiterate, here is our model and here is our learning algorithm subjective.
重申一下 我们的模型和学习参数是这样的

72
00:02:24,780 --> 00:02:27,690
For the setting where we're using regularization, let me define
让我们假设在使用正则化的情形中

73
00:02:28,520 --> 00:02:30,650
j train of theta to be something different
定义Jtrain(θ)为另一种不同的形式

74
00:02:31,520 --> 00:02:33,480
to be the optimization objective
目标同样是最优化

75
00:02:34,280 --> 00:02:35,910
but without the regularization term.
但不使用正则化项

76
00:02:36,650 --> 00:02:38,510
Previously, in earlier video
在先前的授课视频中

77
00:02:38,860 --> 00:02:39,780
when we are not using
当我们没有使用正则化时

78
00:02:40,150 --> 00:02:41,910
regularization, I define j train of theta to
我们定义的Jtrain(θ)

79
00:02:42,760 --> 00:02:45,890
be the same as j of theta as the cost function but
就是代价函数J(θ)

80
00:02:46,140 --> 00:02:48,550
when we are using regularization with this extra lambda term
但当我们使用正则化多出这个lambda项时

81
00:02:49,590 --> 00:02:51,950
we're going to
我们就将训练集误差

82
00:02:52,190 --> 00:02:53,340
define j train my training set error,
也就是Jtrain 定义为

83
00:02:53,610 --> 00:02:54,720
to be just my sum of
训练集数据预测的平方误差的求和

84
00:02:54,940 --> 00:02:56,180
squared errors on the training
或者准确的说

85
00:02:56,520 --> 00:02:58,010
set, or my average squared error
是训练集的平均误差平方和

86
00:02:58,230 --> 00:03:01,170
on the training set without taking into account that regularization chart.
但不考虑正则化项

87
00:03:02,050 --> 00:03:03,360
And similarly, I'm then
与此类似

88
00:03:03,520 --> 00:03:04,800
also going to define the
我们来定义交叉验证集误差

89
00:03:05,320 --> 00:03:07,280
cross-validation set error when the
以及测试集误差

90
00:03:07,380 --> 00:03:08,480
test set error, as before
和之前一样定义为

91
00:03:08,940 --> 00:03:10,830
to be the average sum of squared errors
对交叉验证集和测试集进行预测

92
00:03:11,430 --> 00:03:13,100
on the cross-validation and the test sets.
取平均误差平方和的形式

93
00:03:14,350 --> 00:03:16,380
So just to summarize,
总结一下

94
00:03:16,930 --> 00:03:18,170
my definitions of J train and
我们对于训练误差Jtrain

95
00:03:18,600 --> 00:03:19,520
J C V and J
交叉验证集误差Jcv

96
00:03:19,730 --> 00:03:20,930
Test are just the
和测试集误差Jtest的定义

97
00:03:21,160 --> 00:03:22,120
average squared error, or one
都是平均误差平方和

98
00:03:22,520 --> 00:03:23,720
half of the average
或者准确地说

99
00:03:24,100 --> 00:03:25,710
squared error on my training validation and
是训练集 验证集和测试集进行预测

100
00:03:25,950 --> 00:03:27,880
test sets without the extra
在不使用正则化项时

101
00:03:29,420 --> 00:03:30,400
regularization chart.
平均误差平方和的二分之一

102
00:03:30,470 --> 00:03:32,610
So, this is how we can automatically choose the regularization parameter lambda.
这就是我们自动选取正则化参数lambda的方法

103
00:03:35,060 --> 00:03:36,710
What I usually do is may
通常我的做法是

104
00:03:36,830 --> 00:03:39,150
be have some range of values of lambda I want to try it.
选取一系列我想要尝试的lambda值

105
00:03:39,330 --> 00:03:40,850
So I might be
因此首先我可能考虑

106
00:03:40,990 --> 00:03:42,160
considering not using regularization,
不使用正则化的情形

107
00:03:43,540 --> 00:03:44,670
or here are a few values I might try.
以及一系列我可能会试的值

108
00:03:44,890 --> 00:03:45,850
I might be considering along because
比如说我可能从0.01 0.02

109
00:03:46,320 --> 00:03:48,500
of O1, O2 from O4 and so on.
0.04开始 一直试下去

110
00:03:49,090 --> 00:03:50,510
And you know, I usually step these
通常来讲

111
00:03:50,770 --> 00:03:53,220
up in multiples of
我一般将步长设为2倍速度增长

112
00:03:53,420 --> 00:03:55,960
two until some maybe larger value
一直到一个比较大的值

113
00:03:56,070 --> 00:03:57,250
this in multiples of two you
在本例中以两倍步长递增的话

114
00:03:57,480 --> 00:03:59,000
I actually end up with 10.24;
我们最终取值10.24

115
00:03:59,270 --> 00:04:01,810
it's ten exactly, but you
实际上我们取的是10

116
00:04:01,980 --> 00:04:03,240
know, this is close enough and
但取10已经非常接近了

117
00:04:03,860 --> 00:04:05,320
the 35 decimal
因为小数点后的24

118
00:04:05,610 --> 00:04:07,830
places won't affect your result that much.
对最终的结果不会有太大影响

119
00:04:08,140 --> 00:04:09,920
So, this gives me, maybe
因此 这样我就得到了

120
00:04:10,640 --> 00:04:12,470
twelve different models, that I'm
12个不同的正则化参数lambda

121
00:04:12,610 --> 00:04:14,350
trying to select amongst, corresponding to
对应的12个不同的模型

122
00:04:14,540 --> 00:04:16,210
12 different values of the
我将从这个12个模型中

123
00:04:16,520 --> 00:04:20,900
regularization parameter lambda and of course, you can also go
选出一个最合适的模型，当然了，你也可以试

124
00:04:20,910 --> 00:04:23,560
to values less than 0.01
小于0.01的值

125
00:04:23,620 --> 00:04:25,110
or values larger than 10,
或者大于10的值

126
00:04:25,210 --> 00:04:27,380
but I've just truncated it here for convenience.
但在这里我就不讨论这些情况了

127
00:04:27,710 --> 00:04:28,570
Given each of these 12
得到这12组模型后

128
00:04:28,900 --> 00:04:30,050
models, what we can
接下来

129
00:04:30,280 --> 00:04:31,080
do is then the following:
我们要做的事情是

130
00:04:32,110 --> 00:04:33,410
we take this first
选用第一个模型

131
00:04:33,790 --> 00:04:35,160
model with lambda equals 0,
也就是lambda等于0

132
00:04:35,360 --> 00:04:37,420
and minimize my cos
然后最小化我们的

133
00:04:37,700 --> 00:04:39,860
function j of theta and this
代价函数J(θ)

134
00:04:40,090 --> 00:04:41,620
would give me some parameter vector theta
这样我们就得到了某个参数向量theta

135
00:04:42,160 --> 00:04:43,310
and similar to the earlier video,
与之前视频的做法类似

136
00:04:43,510 --> 00:04:45,370
let me just denote this as
我使用theta上标(1)

137
00:04:46,860 --> 00:04:47,960
theta superscript 1.
来表示第一个参数向量theta

138
00:04:49,890 --> 00:04:50,750
And then I can take my
然后我再取第二个模型

139
00:04:50,930 --> 00:04:52,520
second model, with lambda
也就是

140
00:04:53,000 --> 00:04:54,530
set to 0.01 and
lambda等于0.01的模型

141
00:04:55,160 --> 00:04:57,120
minimize my cos function, now
最小化代价方差

142
00:04:57,250 --> 00:04:58,870
using lambda equals 0.01
当然 现在lambda等于0.01

143
00:04:58,970 --> 00:05:00,080
of course, to get some
那么会得到一个

144
00:05:00,270 --> 00:05:01,290
different parameter vector theta,
完全不同的参数向量theta

145
00:05:01,840 --> 00:05:02,730
we need to know that theta 2,
用theta(2)来表示

146
00:05:02,860 --> 00:05:04,000
and for that I
同理 接下来

147
00:05:04,240 --> 00:05:05,520
end up with theta 3
我会得到theta(3)

148
00:05:05,720 --> 00:05:06,590
so that this is correct for my
这对应于我的第三个模型

149
00:05:06,660 --> 00:05:08,400
third model, and so on,
以此类推

150
00:05:08,930 --> 00:05:10,290
until for for my final model
一直到最后一个

151
00:05:10,760 --> 00:05:13,360
with lambda set to 10,
lambda等于10或10.24的模型

152
00:05:13,360 --> 00:05:16,460
or 10.24, or I end up with this theta 12.
对应theta(12)

153
00:05:17,650 --> 00:05:19,120
Next I can take
接下来我就可以用

154
00:05:19,360 --> 00:05:21,020
all of these hypotheses, all of
所有这些假设

155
00:05:21,100 --> 00:05:23,160
these parameters, and use
所有这些参数

156
00:05:23,470 --> 00:05:25,510
my cross-validation set to evaluate them.
以及交叉验证集来评价它们了

157
00:05:26,250 --> 00:05:27,750
So I can look at my
因此我可以从第一个模型

158
00:05:28,430 --> 00:05:29,730
first model, my second
第二个模型等等开始

159
00:05:30,080 --> 00:05:30,680
model, fits with these different values
对每一个不同的正则化参数lambda

160
00:05:30,710 --> 00:05:41,600
of the regularization parameter and
进行拟合

161
00:05:41,750 --> 00:05:42,630
evaluate them on my cross-validation
然后用交叉验证集来评价每一个模型

162
00:05:42,880 --> 00:05:43,460
set - basically measure the average squared error of each of these parameter
也即测出每一个参数thata在交叉验证集上的

163
00:05:43,550 --> 00:05:45,220
vectors theta on my cross-validation set.
平均误差平方和

164
00:05:45,560 --> 00:05:47,110
And I would then pick whichever one
然后我就选取这12个模型中

165
00:05:47,270 --> 00:05:48,710
of these 12 models gives me
交叉验证集误差最小的

166
00:05:48,880 --> 00:05:51,360
the lowest error on the cross-validation set.
那个模型作为最终选择

167
00:05:52,360 --> 00:05:53,100
And let's say, for the sake
对于本例而言

168
00:05:53,380 --> 00:05:54,970
of this example, that I
假如说

169
00:05:55,260 --> 00:05:56,880
end up picking theta 5,
最终我选择了theta(5)

170
00:05:56,960 --> 00:05:59,570
the fifth order polynomial, because
也就是五次多项式

171
00:05:59,960 --> 00:06:02,550
that has the Noah's cross-validation error.
因为此时的交叉验证集误差最小

172
00:06:03,320 --> 00:06:05,530
Having done that, finally, what
做完这些 最后

173
00:06:05,700 --> 00:06:06,530
I would do if I want
如果我想看看该模型

174
00:06:06,800 --> 00:06:07,940
to report a test set error
在测试集上的表现

175
00:06:08,680 --> 00:06:10,000
is to take the parameter theta
我可以用经过学习

176
00:06:10,310 --> 00:06:12,200
5 that I've
得到的模型theta(5)

177
00:06:12,350 --> 00:06:13,860
selected and look at
来测出它对测试集的

178
00:06:13,980 --> 00:06:16,020
how well it does on my test set.
预测效果如何

179
00:06:16,150 --> 00:06:17,620
And once again here is as
再次重申一下

180
00:06:17,790 --> 00:06:18,980
if we fit this parameter
这里我们依然是用

181
00:06:19,540 --> 00:06:21,750
theta to my cross-validation
交叉验证集来拟合模型

182
00:06:22,580 --> 00:06:23,770
set, which is why I
这也是为什么我之前

183
00:06:23,970 --> 00:06:25,250
am saving aside a separate
预留了一部分数据

184
00:06:25,730 --> 00:06:27,120
test set that I
作为测试集的原因

185
00:06:27,170 --> 00:06:28,370
am going to use to get
这样我就可以用这部分测试集

186
00:06:28,660 --> 00:06:29,780
a better estimate of how
比较准确地估计出

187
00:06:30,040 --> 00:06:31,250
well my a parameter vector
我的参数向量theta

188
00:06:31,500 --> 00:06:33,000
theta will generalize to previously unseen examples.
对于新样本的泛化能力

189
00:06:35,430 --> 00:06:37,180
So that's model selection applied
这就是模型选择在选取

190
00:06:37,570 --> 00:06:39,620
to selecting the regularization parameter
正则化参数lambda时的应用

191
00:06:40,570 --> 00:06:41,660
lambda. The last thing
在这段视频中

192
00:06:41,800 --> 00:06:42,830
I'd like to do in this
我想讲的最后一个问题是

193
00:06:43,080 --> 00:06:44,200
video, is get a
当我们改变

194
00:06:44,280 --> 00:06:46,390
better understanding of how
正则化参数lambda的值时

195
00:06:46,960 --> 00:06:48,650
cross-validation and training error
交叉验证集误差

196
00:06:48,990 --> 00:06:51,000
vary as we as
和训练集误差会随之

197
00:06:51,840 --> 00:06:54,140
we vary the regularization parameter lambda.
发生怎样的变化

198
00:06:54,770 --> 00:06:56,370
And so just a reminder, that
我想提醒一下

199
00:06:56,670 --> 00:06:58,070
was our original cosine function j of
我们最初的代价函数J(θ)

200
00:06:58,150 --> 00:06:59,540
theta, but for this
原来是这样的形式

201
00:06:59,710 --> 00:07:00,660
purpose we're going to define
但在这里我们把训练误差

202
00:07:01,760 --> 00:07:03,140
training error without using
定义为不包括正则化项

203
00:07:03,150 --> 00:07:05,090
the regularization parameter, and cross-validation
交叉验证集误差

204
00:07:05,770 --> 00:07:07,060
error without using the
也定义为不包括

205
00:07:07,270 --> 00:07:09,720
regularization parameter and what I'd like
正则化项

206
00:07:10,120 --> 00:07:11,680
to do is plot this J train
我要做的是

207
00:07:12,660 --> 00:07:15,330
and plot this Jcv, meaning just
绘制出Jtrain和Jcv的曲线

208
00:07:15,610 --> 00:07:16,730
how well does my
随着我增大正则化项参数

209
00:07:16,830 --> 00:07:19,160
hypothesis do for on
lambda的值

210
00:07:19,490 --> 00:07:20,670
the training set and how well
看看我的假设

211
00:07:20,830 --> 00:07:22,190
does my hypothesis do on the
在训练集上的表现如何变化

212
00:07:22,250 --> 00:07:24,000
cross-validation set as I
以及在交叉验证集上

213
00:07:24,010 --> 00:07:25,920
vary my regularization parameter
表现如何变化

214
00:07:26,390 --> 00:07:29,860
lambda so as
就像我们之前看到的

215
00:07:29,920 --> 00:07:32,340
we saw earlier, if lambda
如果正则化项参数

216
00:07:32,670 --> 00:07:34,330
is small, then we're
lambda的值很小

217
00:07:34,520 --> 00:07:36,920
not using much regularization and
那也就是说我们几乎没有使用正则化

218
00:07:37,370 --> 00:07:39,460
we run a larger risk of overfitting.
因此我们有很大可能处于过拟合

219
00:07:39,550 --> 00:07:41,280
Where as if lambda is
而如果lambda的值

220
00:07:41,530 --> 00:07:42,690
large, that is if we
取的很大的时候

221
00:07:42,910 --> 00:07:43,810
were on the right part
也就是说取值在

222
00:07:44,790 --> 00:07:47,000
of this horizontal axis, then
横坐标的右端

223
00:07:47,290 --> 00:07:48,370
with a large value of lambda
那么由于lambda的值很大

224
00:07:49,160 --> 00:07:51,660
we run the high risk of having a bias problem.
我们很有可能处于高偏差的问题

225
00:07:52,640 --> 00:07:54,250
So if you plot J train
所以 如果你画出

226
00:07:54,880 --> 00:07:56,500
and Jcv, what you
Jtrain和Jcv的曲线

227
00:07:56,580 --> 00:07:58,330
find is that for small
你就会发现

228
00:07:58,700 --> 00:08:00,770
values of lambda you can
当lambda的值取得很小时

229
00:08:01,610 --> 00:08:02,640
fit the training set relatively
对训练集的拟合相对较好

230
00:08:03,240 --> 00:08:04,290
well because you're not regularizing.
因为没有使用正则化

231
00:08:05,200 --> 00:08:06,490
So, for small values of
因此 对于lambda值很小的情况

232
00:08:06,590 --> 00:08:08,350
lambda, the regularization term basically
正则化项基本可以忽略

233
00:08:08,560 --> 00:08:09,700
goes away and you're just
你只需要对平方误差

234
00:08:10,020 --> 00:08:12,060
minimizing pretty much your squared error.
做最小化处理即可

235
00:08:12,470 --> 00:08:14,090
So when lambda is small, you
所以当lambda值很小时

236
00:08:14,230 --> 00:08:15,180
end up with a small value
你最终能得到一个

237
00:08:15,770 --> 00:08:17,390
for J train, whereas if
值很小的Jtrain

238
00:08:17,500 --> 00:08:18,780
lambda is large, then you
而如果lambda的值很大时

239
00:08:19,340 --> 00:08:22,080
have a high bias problem and you might not fit your training set so well.
你将处于高偏差问题 不能对训练集很好地拟合

240
00:08:22,240 --> 00:08:23,400
So you end up with a value up there.
因此你的误差值可能位于这个位置

241
00:08:24,150 --> 00:08:28,400
So, J train of
因此 当lambda增大时

242
00:08:28,530 --> 00:08:29,730
theta will tend to
训练集误差Jtrain的值

243
00:08:29,920 --> 00:08:31,890
increase when lambda increases
会趋于上升

244
00:08:32,650 --> 00:08:34,320
because a large value of
因为lambda的值比较大时

245
00:08:34,520 --> 00:08:35,450
lambda corresponds a high bias
对应着高偏差的问题

246
00:08:36,000 --> 00:08:37,000
where you might not even fit your
此时你连训练集都不能很好地拟合

247
00:08:37,190 --> 00:08:38,760
training set well, whereas a
反过来 当lambda的值

248
00:08:38,890 --> 00:08:40,980
small value of lambda corresponds to,
取得很小的时候

249
00:08:41,250 --> 00:08:43,100
if you can you know freely
你的数据能随意地与高次多项式

250
00:08:43,450 --> 00:08:46,290
fit to very high degree polynomials, your data, let's say.
很好地拟合

251
00:08:46,520 --> 00:08:50,460
As for the cross-validation error, we end up with a figure like this.
交叉验证集误差的曲线是这样的

252
00:08:51,680 --> 00:08:53,200
Where, over here on
在曲线的右端

253
00:08:53,530 --> 00:08:55,060
the right, if we
当lambda的值

254
00:08:55,130 --> 00:08:56,070
have a large value of lambda,
取得很大时

255
00:08:57,040 --> 00:08:58,200
we may end up underfitting.
我们会处于欠拟合问题

256
00:08:58,900 --> 00:09:00,280
And so, this is the bias regime
也对应着偏差问题

257
00:09:01,950 --> 00:09:04,750
whereas and cross
那么此时交叉验证集误差

258
00:09:05,030 --> 00:09:06,680
validation error will be
将会很大

259
00:09:06,920 --> 00:09:08,060
high and let me just leave
我写在这里

260
00:09:08,250 --> 00:09:10,760
all that. So, that's Jcv of theta because with
这是交叉验证集误差Jcv

261
00:09:11,270 --> 00:09:12,440
high bias we won't be fitting.
由于高偏差的原因我们不能很好地拟合

262
00:09:13,430 --> 00:09:15,580
We won't be doing well on the cross-validation set.
我们的假设不能在交叉验证集上表现地比较好

263
00:09:17,050 --> 00:09:20,000
Whereas here on the left, this is the high-variance regime.
而曲线的左端对应的是高方差问题

264
00:09:21,120 --> 00:09:22,620
Where if we have two smaller
此时我们的lambda值

265
00:09:23,020 --> 00:09:24,910
value of then we
取得很小很小

266
00:09:25,070 --> 00:09:26,190
may be overfitting the data
因此我们会对数据过度拟合

267
00:09:26,870 --> 00:09:28,140
and so by over fitting the
所以由于过拟合的原因

268
00:09:28,230 --> 00:09:30,320
data then it a cross validation error
交叉验证集误差Jcv

269
00:09:30,710 --> 00:09:31,610
will also be high.
结果也会很大

270
00:09:32,700 --> 00:09:34,380
And so, this is what the
好的 这就是

271
00:09:35,620 --> 00:09:37,270
cross-validation error and what
当我们改变正则化参数

272
00:09:37,510 --> 00:09:38,860
the training error may look
lambda的值时

273
00:09:39,130 --> 00:09:40,410
like on a training set
交叉验证集误差

274
00:09:40,820 --> 00:09:43,270
as we vary the parameter
和训练集误差

275
00:09:43,950 --> 00:09:45,920
lambda, as we vary the regularization parameter lambda.
随之发生的变化

276
00:09:46,110 --> 00:09:47,220
And so, once again, it will
当然 在中间取的某个

277
00:09:47,430 --> 00:09:49,100
often be some intermediate value
lambda的值

278
00:09:49,790 --> 00:09:52,220
of lambda that you know, subsequent just right
表现得刚好合适

279
00:09:52,720 --> 00:09:53,990
or that works best in
这种情况下表现最好

280
00:09:54,120 --> 00:09:55,470
terms of having a small
交叉验证集误差

281
00:09:55,570 --> 00:09:58,510
cross-validation error or a small test set error.
或者测试集误差都很小

282
00:09:58,520 --> 00:09:59,580
And whereas the curves I've drawn
当然由于我在这里画的图

283
00:09:59,900 --> 00:10:02,230
here are somewhat cartoonish and somewhat idealized.
显得太卡通 也太理想化了

284
00:10:03,250 --> 00:10:04,270
So on a real data set
对于真实的数据

285
00:10:04,810 --> 00:10:06,000
the pros you get may
你得到的曲线可能

286
00:10:06,110 --> 00:10:07,070
end up looking a little bit more
比这看起来更凌乱

287
00:10:07,290 --> 00:10:09,180
messy and just a little bit more noisy than this.
会有很多的噪声

288
00:10:09,800 --> 00:10:10,900
For some data sets you will
对某个实际的数据集

289
00:10:11,000 --> 00:10:12,270
really see these poor
你或多或少能看出

290
00:10:12,560 --> 00:10:14,000
source of trends and
像这样的一个趋势

291
00:10:14,270 --> 00:10:15,160
by looking at the plot
通过绘出这条曲线

292
00:10:15,720 --> 00:10:16,750
of the whole or cross validation
通过交叉验证集误差的变化趋势

293
00:10:17,640 --> 00:10:19,280
error, you can either
你可以用自己选择出

294
00:10:19,420 --> 00:10:21,190
manually, automatically try to
或者编写程序自动得出

295
00:10:21,500 --> 00:10:22,920
select a point that minimizes
能使交叉验证集误差

296
00:10:23,370 --> 00:10:26,410
the cross-validation error and
最小的那个点

297
00:10:26,700 --> 00:10:28,420
select the value of lambda corresponding
然后选出那个与之对应的

298
00:10:29,100 --> 00:10:30,600
to low cross-validation error.
参数lambda的值

299
00:10:31,380 --> 00:10:32,610
When I'm trying to pick the
当我在尝试为学习算法

300
00:10:32,740 --> 00:10:34,690
regularization parameter lambda
选择正则化参数

301
00:10:35,020 --> 00:10:37,120
for a learning algorithm, often I
lambda的时候

302
00:10:37,240 --> 00:10:38,340
find that plotting a figure
我通常都会得出

303
00:10:38,620 --> 00:10:40,290
like this one showed here, helps
类似这个图的结果

304
00:10:40,570 --> 00:10:42,340
me understand better what's going
帮助我更好地理解各种情况

305
00:10:42,600 --> 00:10:44,140
on and helps me verify that
同时也帮助我确认

306
00:10:44,700 --> 00:10:45,960
I am indeed picking a good
我选择的正则化参数值

307
00:10:46,140 --> 00:10:47,490
value for the regularization parameter
到底好不好

308
00:10:48,340 --> 00:10:50,140
lambda. So hopefully that
希望这节课的内容

309
00:10:50,340 --> 00:10:51,980
gives you more insight into regularization
让你更深入地理解了正则化

310
00:10:53,470 --> 00:10:54,710
and it's effects on the bias
以及它对学习算法的

311
00:10:55,220 --> 00:10:56,290
and variance of the learning algorithm.
偏差和方差的影响

312
00:10:57,790 --> 00:10:59,330
By know you've seen bias and
到目前为止你已经从不同角度

313
00:10:59,490 --> 00:11:01,230
variance from a lot of different perspectives.
见识了方差和偏差问题

314
00:11:02,000 --> 00:11:03,290
And what I'd like to do
在下一节视频中

315
00:11:03,520 --> 00:11:04,820
in the next video is take
我要做的是

316
00:11:05,050 --> 00:11:05,930
a lot of the insights
基于我们已经浏览过的

317
00:11:06,100 --> 00:11:07,890
that we've gone through and build
所有这些概念

318
00:11:08,140 --> 00:11:09,030
on them to put together
将它们结合起来

319
00:11:09,740 --> 00:11:11,590
a diagnostic that's called learning
建立我们的诊断法

320
00:11:11,870 --> 00:11:12,920
curves, which is a
也称为学习曲线

321
00:11:12,970 --> 00:11:14,120
tool that I often use
这种方法通常被用来

322
00:11:14,540 --> 00:11:15,740
to try to diagnose if a
诊断一个学习算法

323
00:11:16,010 --> 00:11:17,450
learning algorithm may be suffering
到底是处于偏差问题

324
00:11:17,860 --> 00:11:19,150
from a bias problem or a
还是方差问题

325
00:11:19,380 --> 00:11:20,770
variance problem or a little bit of both.
还是两者都有。

