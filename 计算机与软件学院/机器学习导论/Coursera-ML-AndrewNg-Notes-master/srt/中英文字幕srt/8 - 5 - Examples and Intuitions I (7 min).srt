1
00:00:00,130 --> 00:00:00,980
In this and the next
在接下来两节视频中
(字幕整理：中国海洋大学 黄海广,haiguang2000@qq.com )

2
00:00:01,240 --> 00:00:02,030
video I want to work
我要通过讲解

3
00:00:02,140 --> 00:00:03,650
through a detailed example, showing
一个具体的例子来解释

4
00:00:04,530 --> 00:00:05,920
how a neural network can compute
神经网络是如何计算

5
00:00:06,220 --> 00:00:07,740
a complex nonlinear function of
关于输入的复杂的非线性函数

6
00:00:07,970 --> 00:00:09,780
the input and hopefully, this will
希望这个例子可以

7
00:00:09,950 --> 00:00:10,950
give you a good sense of why
让你了解为什么

8
00:00:11,510 --> 00:00:12,470
Neural Networks can be used
神经网络可以用来

9
00:00:13,050 --> 00:00:14,810
to learn complex, nonlinear hypotheses.
学习复杂的非线性假设

10
00:00:16,790 --> 00:00:18,210
Consider the following problem where
考虑下面的问题

11
00:00:18,900 --> 00:00:20,560
we have input features x1
我们有二进制的

12
00:00:20,770 --> 00:00:21,680
and x2 that are binary
输入特征

13
00:00:22,310 --> 00:00:23,760
values, so either zero or one.
x1 x2 要么取0 要么取1

14
00:00:23,990 --> 00:00:25,320
So x1 and x2 can each
所以x1和x2只能

15
00:00:25,510 --> 00:00:27,160
take on only one of two possible values.
有两种取值

16
00:00:28,660 --> 00:00:29,670
In this example, I've drawn
在这个例子中 我只画出了

17
00:00:29,990 --> 00:00:31,420
only two positive examples and
两个正样本和

18
00:00:31,530 --> 00:00:33,240
two negative examples, but you
两个负样本

19
00:00:33,320 --> 00:00:34,370
can think of this as a
但你可以认为这是一个

20
00:00:34,540 --> 00:00:36,210
simplified version of a
更复杂的学习问题的

21
00:00:36,290 --> 00:00:37,710
more complex learning problem where
简化版本

22
00:00:37,920 --> 00:00:38,910
we may have a bunch
在这个复杂问题中 我们可能

23
00:00:39,030 --> 00:00:40,320
of positive examples in the upper
在右上角有一堆正样本

24
00:00:40,480 --> 00:00:41,350
right and the lower left and
在左下方有

25
00:00:41,470 --> 00:00:43,090
a bunch of negative examples to notify
一堆用圆圈

26
00:00:43,580 --> 00:00:46,110
the circles, and what
表示的负样本

27
00:00:46,140 --> 00:00:46,900
we'd like to do is learn a nonlinear, you know,
我们想要学习一种非线性的

28
00:00:48,330 --> 00:00:50,090
decision boundary that we
决策边界来

29
00:00:50,210 --> 00:00:52,210
need to separate the positive and the negative examples.
区分正负样本

30
00:00:53,750 --> 00:00:54,590
So how can a neural
那么 神经网络是

31
00:00:55,070 --> 00:00:56,160
network do this and rather than
如何做到的呢？

32
00:00:56,710 --> 00:00:57,550
use an example on the
为了描述方便我不用右边这个例子

33
00:00:57,600 --> 00:00:59,260
right. I'm going to use this, maybe easier
我用左边这个例子

34
00:00:59,680 --> 00:01:01,670
to examine example on the left.
这样更容易说明

35
00:01:02,620 --> 00:01:03,940
Concretely, what this is
具体来讲 这里需要计算的是

36
00:01:04,110 --> 00:01:05,570
is really computing the target
目标函数y

37
00:01:05,990 --> 00:01:09,810
label y equals x1 XOR x2.
等于x1异或x2

38
00:01:10,070 --> 00:01:11,650
Or this is actually the
或者 y也可以等于

39
00:01:11,910 --> 00:01:13,880
x1 XNOR x2 function
x1 异或非 x2

40
00:01:14,700 --> 00:01:15,750
where XNOR is the alternative
其中异或非表示

41
00:01:16,400 --> 00:01:18,420
notation for "not x1 or x2".
x1异或x2后取反

42
00:01:19,350 --> 00:01:20,730
So x1, XOR or
X1异或X2

43
00:01:20,760 --> 00:01:22,730
x2 - that's true only
为真当且仅当

44
00:01:23,210 --> 00:01:24,820
if exactly one of
这两个值

45
00:01:25,190 --> 00:01:27,900
x1 or x2 is equal to 1.
X1或者X2中有且仅有一个为1

46
00:01:27,960 --> 00:01:29,160
It turns out that the specific
如果我

47
00:01:29,450 --> 00:01:30,680
example I'm going to use works out
用XNOR作为例子

48
00:01:30,810 --> 00:01:32,840
a little bit better if we
比用NOT作为例子

49
00:01:33,020 --> 00:01:35,000
use the XNOR example, instead.
结果会好一些

50
00:01:35,460 --> 00:01:36,290
These two are the same, of course.
但这两个其实是相同的

51
00:01:36,720 --> 00:01:38,540
It means not x1 XOR
这就意味着在x1

52
00:01:38,780 --> 00:01:40,140
x2, and so we're going
异或x2后再取反 即

53
00:01:40,320 --> 00:01:42,360
to have positive examples
当它们同时为真

54
00:01:42,950 --> 00:01:44,150
if either both are true or
或者同时为假的时候

55
00:01:44,530 --> 00:01:46,470
both are false and we'll
我们将获得

56
00:01:46,620 --> 00:01:49,600
have that's y equals 1, y equals 1 and
y等于1

57
00:01:49,990 --> 00:01:51,480
we're going to have y equals 0 if
y为0的结果

58
00:01:51,860 --> 00:01:52,650
only one of them is
如果它们中仅有一个

59
00:01:52,760 --> 00:01:53,830
true and we want
为真 y则为0

60
00:01:54,000 --> 00:01:54,710
to figure out if we can
我们想要知道是否能

61
00:01:54,860 --> 00:01:57,210
get a neural network to fit to this sort of training set.
找到一个神经网络模型来拟合这种训练集

62
00:01:59,160 --> 00:02:00,200
In order to build up
为了建立

63
00:02:00,450 --> 00:02:01,610
to a network that fits the
能拟合XNOR运算

64
00:02:02,080 --> 00:02:04,900
XNOR example, we're going
的神经网络 我们先

65
00:02:05,350 --> 00:02:06,590
to start to a slightly simpler one
讲解一个稍微简单

66
00:02:07,050 --> 00:02:09,710
and show a network that fits the AND function.
的神经网络 它拟合了“且运算”

67
00:02:10,760 --> 00:02:12,150
Concretely, lets say we
假设我们

68
00:02:12,310 --> 00:02:14,070
have inputs x1 and
有输入x1和

69
00:02:14,240 --> 00:02:17,190
x2 that are again binary. So, it's either zero or one.
x2 并且都是二进制 即要么为0要么为1

70
00:02:17,820 --> 00:02:18,680
And let's say our target
我们的目标

71
00:02:18,760 --> 00:02:20,980
labels y are you know, is
函数y正如你所知道的

72
00:02:21,910 --> 00:02:23,470
equal to x1 and x2.
等于x1且x2

73
00:02:23,860 --> 00:02:24,870
This is a logical AND.
这是一个逻辑与

74
00:02:30,740 --> 00:02:31,820
So can we get a
那么 我们怎样得到一个

75
00:02:32,060 --> 00:02:34,330
one unit network to compute
具有单个神经元的神经网络来计算

76
00:02:35,060 --> 00:02:36,120
this logical AND function?
这个逻辑与呢

77
00:02:37,400 --> 00:02:38,530
In order to do so, I'm
为了做到这一点

78
00:02:38,690 --> 00:02:40,000
going to actually draw in
我也需要画出偏置单元

79
00:02:40,580 --> 00:02:42,780
the bias unit as well, the plus one unit.
即这个里面有个+1的单元

80
00:02:45,030 --> 00:02:46,500
Now, let me just assign some
现在 让我给这个网络

81
00:02:46,770 --> 00:02:48,050
values to the weights or
分配一些权重

82
00:02:48,160 --> 00:02:50,130
the parameters of this network.
或参数

83
00:02:50,450 --> 00:02:52,220
I am going to write down the parameters on this diagram.
我在图上写出这些参数

84
00:02:52,820 --> 00:02:54,090
Write minus 30 here
这里是-30

85
00:02:56,360 --> 00:02:57,740
plus 20 and plus
正20

86
00:02:58,710 --> 00:02:59,600
20 and what this means
正20 即我给

87
00:02:59,970 --> 00:03:01,320
is that I'm assigning a value
x0前面的

88
00:03:01,860 --> 00:03:03,790
of minus thirty to the
系数赋值

89
00:03:04,120 --> 00:03:05,770
value associated with x0.
为-30.

90
00:03:06,120 --> 00:03:07,230
This is plus 1 going
这个正1会

91
00:03:07,530 --> 00:03:08,840
to this unit and a
作为这个单元的值

92
00:03:09,420 --> 00:03:10,890
parameter value of plus 20
关于20的参数值

93
00:03:11,250 --> 00:03:12,960
that multiplies in x1 in
且x1乘以+20

94
00:03:13,070 --> 00:03:14,300
a value of plus 20 for
以及x2乘以+20

95
00:03:14,680 --> 00:03:15,980
the parameter that multiplies into x2.
都是这个单元的输入

96
00:03:17,190 --> 00:03:18,860
So, concretely, this is saying
所以

97
00:03:19,060 --> 00:03:20,340
that my hypotheses h of
我的假设?(x)

98
00:03:20,420 --> 00:03:21,780
x is equal to
等于

99
00:03:22,410 --> 00:03:24,500
g of  -30 + 20x1 + 20x2.
g(-30 + 20x1 + 20x2)

100
00:03:25,490 --> 00:03:31,390
So sometimes it's just
在图上画出

101
00:03:31,640 --> 00:03:33,240
convenient to draw these
这些参数和

102
00:03:33,810 --> 00:03:34,880
weights and draw these parameters
权重是很方便很直观的

103
00:03:35,620 --> 00:03:38,250
up here, you know, in the diagram of the neural network.
其实 在这幅神经网络图中

104
00:03:38,790 --> 00:03:40,230
And of course this minus 30
这个-30

105
00:03:40,390 --> 00:03:42,500
this is actually theta 1
其实是θ(1)10

106
00:03:43,670 --> 00:03:44,830
of 1,0.
这个是

107
00:03:45,290 --> 00:03:47,390
This is theta 1
θ(1)11

108
00:03:47,600 --> 00:03:50,550
of 1,1 and that's theta
这是

109
00:03:51,560 --> 00:03:52,990
1 of 1,2
θ(1)12

110
00:03:53,290 --> 00:03:54,320
but it's just easier think about
但把它想成

111
00:03:54,590 --> 00:03:56,660
it as, you know, associating these
这些边的

112
00:03:56,840 --> 00:03:58,430
parameters with the edges of the network.
权重会更容易理解

113
00:04:01,170 --> 00:04:04,170
Let's look at what this little single neuron network will compute.
让我们来看看这个小神经元是怎样计算的

114
00:04:05,050 --> 00:04:06,290
Just to remind you, the sigmoid
回忆一下 s型

115
00:04:06,720 --> 00:04:08,820
activation function g of z looks like this.
激励函数g(z)看起来是这样的

116
00:04:09,110 --> 00:04:10,810
It starts from 0, rises
它从0开始 光滑

117
00:04:11,160 --> 00:04:12,270
smoothly, crosses 0.5, and
上升 穿过0.5

118
00:04:12,750 --> 00:04:14,720
then it asymptotes at one.
渐进到1.

119
00:04:15,730 --> 00:04:16,510
And to give you some landmarks,
我们给出一些坐标

120
00:04:17,350 --> 00:04:18,850
if the horizontal axis value
如果横轴值

121
00:04:19,460 --> 00:04:21,770
z is equal to 4.6, then
z等于4.6 则

122
00:04:23,840 --> 00:04:25,910
the sigmoid function is equal to 0.99.
S形函数等于0.99

123
00:04:26,220 --> 00:04:27,950
This is very close
这是非常接近

124
00:04:28,150 --> 00:04:29,560
to 1 and kind of symmetrically
1的 并且由于对称性

125
00:04:30,350 --> 00:04:32,270
if it is negative 4.6, then
如果z为-4.6

126
00:04:33,090 --> 00:04:34,970
the sigmoid function there is
S形函数

127
00:04:35,080 --> 00:04:37,820
equal to 0.01 which is very close to 0.
等于0.01 非常接近0

128
00:04:39,440 --> 00:04:40,700
Let's look at the four possible input
让我们来看看四种可能的输入值

129
00:04:41,040 --> 00:04:41,680
values for x1 and x2
x1和x2的四种可能输入

130
00:04:41,730 --> 00:04:43,470
and look at whether the hypothesis will
看看我们的假设

131
00:04:43,620 --> 00:04:47,090
open in that case.
在各种情况下的输出

132
00:04:47,220 --> 00:04:47,910
If x1 and x2 are both
如果X1和X2均为

133
00:04:48,150 --> 00:04:49,160
equal to 0 - if
0  那么

134
00:04:49,460 --> 00:04:50,560
you look at this, if
你看看这个 如果

135
00:04:50,710 --> 00:04:51,650
x1 and x2 are both equal
x1和x2都等于

136
00:04:52,010 --> 00:04:54,780
to 0 then the hypotheses of point g of -30.
为0 则假设会输出g(-30)

137
00:04:55,120 --> 00:04:56,790
So, it's like very
g(-30)在图的

138
00:04:57,290 --> 00:04:58,510
far to the left of this diagram.
很左边的地方

139
00:04:58,750 --> 00:05:01,380
This will be very close to 0.
非常接近于0

140
00:05:01,590 --> 00:05:03,160
If x1 equals 0 and
如果x1等于0且

141
00:05:03,330 --> 00:05:05,100
x2 equals 1 then this
x2等于1 那么

142
00:05:05,550 --> 00:05:07,610
formula here evaluates to
此公式等于

143
00:05:07,830 --> 00:05:09,470
g, thus the sigmoid function applied
g关于

144
00:05:09,890 --> 00:05:12,000
to -10 and again,
-10取值

145
00:05:12,450 --> 00:05:13,640
that's, you know, to the far left
也在很左边的位置

146
00:05:13,880 --> 00:05:14,970
of this plot and so,
所以

147
00:05:15,150 --> 00:05:16,540
that's again very close to 0.
也是非常接近0

148
00:05:16,660 --> 00:05:19,180
This is also g of -10.
这个也是g(-10)

149
00:05:19,270 --> 00:05:21,320
That is if x1
也就是说 如果x1

150
00:05:22,000 --> 00:05:24,110
is equal to 1 and
等于1并且

151
00:05:24,560 --> 00:05:26,110
x(2)0, this is -30 plus 20, which is -10.
x2等于0 这就是-30加20等于-10

152
00:05:26,230 --> 00:05:28,450
And finally if
最后 如??果

153
00:05:28,590 --> 00:05:29,940
x1 equals 1, x2 equals
x1等于1 x2等于

154
00:05:30,670 --> 00:05:31,970
1, then you have g of
1 那么这等于

155
00:05:32,770 --> 00:05:34,020
-30 +20 +20,
-30 +20 +20

156
00:05:34,190 --> 00:05:35,370
so that's g of
所以这是

157
00:05:35,440 --> 00:05:36,480
+10, which is
取+10时

158
00:05:36,710 --> 00:05:38,140
therefore very close to 1.
非常接近1

159
00:05:39,040 --> 00:05:40,210
And if you look
如果你看看

160
00:05:40,490 --> 00:05:42,700
in this column, this is
在这一列 这就是

161
00:05:43,010 --> 00:05:45,280
exactly the logical "and" function.
逻辑“与”的计算结果

162
00:05:45,820 --> 00:05:47,790
So, this is computing h of
所以 这里得到的h

163
00:05:47,890 --> 00:05:49,870
x is, you know,
h关于x取值

164
00:05:50,260 --> 00:05:54,910
approximately x1 and x2.
近似等于x1和x2的与运算的值

165
00:05:55,200 --> 00:05:56,210
In other words, it outputs
换句话说 假设输出

166
00:05:56,650 --> 00:05:57,820
1 if and only
1 当且仅当

167
00:05:58,270 --> 00:05:59,470
if x1 and x2 are
x1 x2

168
00:06:00,950 --> 00:06:02,410
both equal to 1.
都等于1

169
00:06:03,360 --> 00:06:04,840
So by writing out our little
所以 通过写出

170
00:06:05,320 --> 00:06:07,070
truth table like this,
这张真值表

171
00:06:07,780 --> 00:06:09,060
we manage to figure out what's
我们就弄清楚了

172
00:06:09,350 --> 00:06:11,170
the logical function that our
神经网络

173
00:06:11,650 --> 00:06:12,870
neural network computes.
计算出的逻辑函数

174
00:06:16,990 --> 00:06:18,350
This network shown here computes
这里的神经网络

175
00:06:18,880 --> 00:06:20,280
the OR function just to
实现了或函数的功能

176
00:06:20,370 --> 00:06:21,810
show you how I worked that out.
接下来我告诉你是怎么看出来的

177
00:06:22,530 --> 00:06:23,230
If you are to write out
如果你把

178
00:06:23,680 --> 00:06:25,240
the hypotheses you find that
假设写出来

179
00:06:25,360 --> 00:06:26,690
it's computing g of
会发现它等于

180
00:06:27,110 --> 00:06:29,980
-10 +20 x1
g关于-10 +20x1

181
00:06:30,170 --> 00:06:32,040
+20 x2. And so
+20x2的取值

182
00:06:32,270 --> 00:06:33,380
if you fill in these values you
如果把这些值都填上

183
00:06:33,520 --> 00:06:35,110
find that's g of
会发现

184
00:06:35,460 --> 00:06:37,080
-10 which is approximately 0,
这是g(-10) 约等于0

185
00:06:37,820 --> 00:06:38,840
g of 10 which is
这是g(10)

186
00:06:39,040 --> 00:06:40,550
approximately 1, and so on.
约等于1

187
00:06:40,930 --> 00:06:42,650
These are approximately 1, and approximately
这个也约等于1

188
00:06:43,550 --> 00:06:45,410
1, and these numbers is
这些数字

189
00:06:46,160 --> 00:06:47,650
essentially the logical OR
本质上就是逻辑或

190
00:06:47,860 --> 00:06:50,210
function.  So, hopefully
运算得到的值 所以 我希望

191
00:06:50,590 --> 00:06:52,010
with this, you now understand how
通过这个例子 你现在明白了

192
00:06:52,350 --> 00:06:53,930
single neurons in a
神经网络里单个的

193
00:06:54,020 --> 00:06:54,980
neural network can be used
神经元在计算

194
00:06:55,180 --> 00:06:58,390
to compute logical functions like AND and OR and so on.
如AND和OR逻辑运算时是怎样发挥作用的

195
00:06:59,000 --> 00:07:00,280
In the next video, we'll continue
在接下来的视频中 我们将继续

196
00:07:00,790 --> 00:07:03,870
building on these examples and work through a more complex example.
讲解一个更复杂的例子

197
00:07:04,730 --> 00:07:05,610
We'll get to show you how
我们将告诉你

198
00:07:06,170 --> 00:07:07,570
a neural network, now with
一个多层的神经网络

199
00:07:07,820 --> 00:07:09,780
multiple layers of units can
怎样被用于

200
00:07:09,960 --> 00:07:10,960
be used to compute more complex
计算更复杂的函数

201
00:07:11,400 --> 00:07:13,870
functions like the XOR function or the XNOR function.
如 XOR 函数或 XNOR 函数

