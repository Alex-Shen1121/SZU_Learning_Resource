{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import *\n",
    "from functools import reduce\n",
    "import re\n",
    "import os\n",
    "import random\n",
    "import jieba\n",
    "from itertools import chain\n",
    "import math\n",
    "import operator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 保留中文内容\n",
    "# 爬虫下来的文档中会有冗余信息\n",
    "def find_chinese(file):\n",
    "    pattern = re.compile(r'[^\\u4e00-\\u9fa5]')\n",
    "    chinese = re.sub(pattern, '', file)\n",
    "    return chinese"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 准备数据：从文本中构建词向量\n",
    "def loadDataSet2():\n",
    "    # 0 --> 党政办公室\n",
    "    # 1 --> 教务部\n",
    "    # 2 --> 招生办公室\n",
    "    # 3 --> 研究生院\n",
    "    # 4 --> 科学技术部\n",
    "    postingList = []\n",
    "    classVec = []\n",
    "    for file in os.listdir(f'data/'):\n",
    "        text = open(f'data/{file}', 'r').read()\n",
    "        text = find_chinese(text)\n",
    "        \n",
    "        text =  list(jieba.cut(text, cut_all=False))\n",
    "        postingList.append(text)\n",
    "\n",
    "        if file.startswith('党政办公室'):\n",
    "            classVec.append(0)\n",
    "        elif file.startswith('教务部'):\n",
    "            classVec.append(1)\n",
    "        elif file.startswith('招生办公室'):\n",
    "            classVec.append(2)\n",
    "        elif file.startswith('研究生院'):\n",
    "            classVec.append(3)\n",
    "        elif file.startswith('科学技术部'):\n",
    "            classVec.append(4)\n",
    "            \n",
    "    return postingList, classVec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 准备数据：从文本中构建词向量\n",
    "def loadDataSet(goal):\n",
    "    # 0 --> 党政办公室\n",
    "    # 1 --> 教务部\n",
    "    # 2 --> 招生办公室\n",
    "    # 3 --> 研究生院\n",
    "    # 4 --> 科学技术部\n",
    "    postingList = []\n",
    "    classVec = []\n",
    "    for file in os.listdir(f'data1/{goal}'):\n",
    "        text = open(f'/Users/alex_shen/SynologyDrive/PcBackup/深圳大学/课程/大三下/信息检索/实验/202102-信息检索-HW5/data1/{goal}/{file}', 'r').read()\n",
    "        text = find_chinese(text)\n",
    "        \n",
    "        text =  list(jieba.cut(text, cut_all=False))\n",
    "        postingList.append(text)\n",
    "\n",
    "        if file.startswith('党政办公室'):\n",
    "            classVec.append(0)\n",
    "        elif file.startswith('教务部'):\n",
    "            classVec.append(1)\n",
    "        elif file.startswith('招生办公室'):\n",
    "            classVec.append(2)\n",
    "        elif file.startswith('研究生院'):\n",
    "            classVec.append(3)\n",
    "        elif file.startswith('科学技术部'):\n",
    "            classVec.append(4)\n",
    "            \n",
    "    return postingList, classVec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 构建一个包含所有文档词汇且不重复的词汇表\n",
    "def createVocabList(dataSet):\n",
    "    vocabSet = set([])  # 创建一个空集\n",
    "    for document in dataSet:\n",
    "        vocabSet = vocabSet | set(document)  # 创建两个集合的并集\n",
    "    return list(vocabSet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concatenateTextOfAllDocsInClass(postingList, classVec, c):\n",
    "    text_c = []\n",
    "    for index, cat in enumerate(classVec):\n",
    "        if cat == c:\n",
    "            text_c.extend(postingList[index])\n",
    "    return text_c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainNB(myVocabList, postingList, classVec):\n",
    "    V = list(chain(*myVocabList))\n",
    "    N = len(postingList)\n",
    "    condprob = {term: [0] * 5 for term in V}\n",
    "    prior = [0] * 5\n",
    "    for c in range(5):\n",
    "        Nc = classVec.count(c)\n",
    "        prior[c] = Nc / N\n",
    "        text_c = concatenateTextOfAllDocsInClass(postingList, classVec, c)\n",
    "        for t in V:\n",
    "            T_ct = text_c.count(t)\n",
    "            condprob[t][c] = (T_ct + 1) / (len(text_c) + len(V))\n",
    "    return V, prior, condprob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def applyMultinomialNB(V, prior, condprob, text):\n",
    "    p = []\n",
    "    for c in range(5):\n",
    "        p.append(math.log(prior[c], 2))\n",
    "        for t in text:\n",
    "            if t in V:\n",
    "                p[c] += math.log(condprob[t][c], 2)\n",
    "    return p.index(max(p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readFIles(files: list(list())):\n",
    "    # 读取data文件夹下的所有文件\n",
    "    for file in os.listdir('data'):\n",
    "        text = open(f'data/{file}', 'r').read()\n",
    "        text = find_chinese(text)\n",
    "        \n",
    "        # 根据文件名进行分类\n",
    "        if file.startswith('党政办公室'):\n",
    "            files[0].append(text)\n",
    "        elif file.startswith('教务部'):\n",
    "            files[1].append(text)\n",
    "        elif file.startswith('招生办公室'):\n",
    "            files[2].append(text)\n",
    "        elif file.startswith('研究生院'):\n",
    "            files[3].append(text)\n",
    "        elif file.startswith('科学技术部'):\n",
    "            files[4].append(text)\n",
    "\n",
    "# 利用jieba进行分析\n",
    "def cutWords(files: list(list()), terms: list(list()), term_list: set()):\n",
    "    for index, file in enumerate(files):\n",
    "        for text in file:\n",
    "            words = list(jieba.cut(text, cut_all=False))\n",
    "            terms[index].append(words)\n",
    "            term_list.update(set(words))\n",
    "            \n",
    "# 计算MI值\n",
    "def cal_MI(N11, N01, N10, N00):\n",
    "    N = N11+N01+N10+N00\n",
    "    sum = 0\n",
    "    sum += 0 if N11 == 0 else N11/N*log(N*(N11)/(N11+N10)/(N11+N01), 2)\n",
    "    sum += 0 if N01 == 0 else N01/N*log(N*N01/(N01+N00)/(N11+N01), 2)\n",
    "    sum += 0 if N10 == 0 else N10/N*log(N*N10/(N10+N11)/(N10+N00), 2)\n",
    "    sum += 0 if N00 == 0 else N00/N*log(N*N00/(N00+N01)/(N00+N10), 2)\n",
    "    return sum\n",
    "# 计算X^2值\n",
    "def cal_X2(N11, N01, N10, N00):\n",
    "    sum = (N11+N10+N01+N00)*((N11*N00-N10*N01)**2)/(N11+N01+1)/(N10+N00+1)/(N11+N10+1)/(N01+N00+1)\n",
    "    return sum\n",
    "\n",
    "# 重建小顶堆\n",
    "def sift(li,low,high):\n",
    "    i = low\n",
    "    #找孩子 左孩子\n",
    "    j = 2 * i +1\n",
    "    tmp = li[low] #把堆顶存起来\n",
    "    while  j <= high: #只要j位置有数\n",
    "        #右孩子要右，右孩子比较大 并且右孩子大于左孩子     右孩子不越界 j+1 <=hight \n",
    "        if j+1 <=high  and li[j+1][1] < li[j][1]:\n",
    "        #j 指向右孩子  这个指的是两个孩子更大的数\n",
    "            j = j + 1\n",
    "        if li[j][1] < tmp[1]:  #目前要是,tmp大就放过去,还是j大放上面去\n",
    "            li[i] = li[j]\n",
    "            i = j       #可以交换了    现在j等于新的i     i等于原来的j\n",
    "            j = 2 * i + 1\n",
    "        else :  #tmp 更大,把tmp放到i的位置上  假如堆点是6\n",
    "            li[i] = tmp  # 6放到原来8的\n",
    "            break\n",
    "    else: #就是2没法和别的比了,就放到该去的地方去\n",
    "        li[i] = tmp  #把tmp放到叶子节点上去\n",
    "\n",
    "#最后非叶子节点\n",
    "#n的最后一个下标是n-1,找他的父亲就孩子,(n-1-1)/2\n",
    "#孩子下标是 n\n",
    "#孩子找父亲 (n-1)/2 里面的n的最后一个下标是n-1 所有(n-1-1)/2 -> (n-2)/2\n",
    "\n",
    "def topk(li,k):\n",
    "    # 前K元素建立堆\n",
    "    heap = li[0:k]\n",
    "    for i in range((k-2)//2,-1,-1):\n",
    "        sift(heap,i,k-1)\n",
    "\n",
    "    #1.建堆\n",
    "    for i in range(k,len(li)):\n",
    "        if li[i][1] > heap[0][1]:\n",
    "            heap[0] = li[i]\n",
    "            sift(heap,0,k-1)\n",
    "\n",
    "    #2. 堆排序\n",
    "    for i in range(k-1,-1,-1):\n",
    "        heap[0], heap[i] = heap[i], heap[0]\n",
    "        sift(heap,0,i -1)\n",
    "    return [word[0] for word in heap]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_X_2():\n",
    "    term_list = set()\n",
    "    # 0 --> 党政办公室\n",
    "    # 1 --> 教务部\n",
    "    # 2 --> 招生办公室\n",
    "    # 3 --> 研究生院\n",
    "    # 4 --> 科学技术部\n",
    "    files = [[] for i in range(5)]\n",
    "    terms = [[] for i in range(5)]\n",
    "\n",
    "    # 依次打开data文件夹下的文件\n",
    "    readFIles(files)\n",
    "\n",
    "    # 进行中文分词，并生成词典\n",
    "    cutWords(files, terms, term_list)\n",
    "\n",
    "    # 计算每个单词的互信息\n",
    "    MI = [{} for i in range(5)]\n",
    "    X2 = [{} for i in range(5)]\n",
    "    for word in term_list:\n",
    "        cat_t, cat_f = [[] for i in range(5)], [[] for i in range(5)]\n",
    "\n",
    "        # 统计每一个category中的出现情况\n",
    "        # cat_t[i] 表示第i个category中出现的word的doc的个数\n",
    "        # cat_f[i] 表示第i个category中不出现的word的doc的个数\n",
    "        for index in range(5):\n",
    "            cat_t[index] = list(\n",
    "                word in words for words in terms[index]).count(True)\n",
    "            cat_f[index] = list(\n",
    "                word in words for words in terms[index]).count(False)\n",
    "\n",
    "        # 计算混淆矩阵\n",
    "        for index in range(5):\n",
    "            N11 = cat_t[index]\n",
    "            N01 = cat_f[index]\n",
    "            N10 = sum(cat_t)-N11\n",
    "            N00 = sum(cat_f) - N01\n",
    "            # 计算MI值\n",
    "            socre2 = cal_X2(N11, N01, N10, N00)\n",
    "\n",
    "            X2[index][word] = socre2\n",
    "\n",
    "    res = []\n",
    "    for i in range(5):\n",
    "        res.append(topk(list(X2[i].items()), 15))\n",
    "\n",
    "    return res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "整体 正确率： 0.94\n",
      "党政办公室 正确率： 1.0\n",
      "教务部 正确率： 0.7\n",
      "招生办公室 正确率： 1.0\n",
      "研究生院 正确率： 1.0\n",
      "科学技术部 正确率： 1.0\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    postingList, classVec = loadDataSet('train')\n",
    "    myVocabList = get_X_2();\n",
    "    \n",
    "    \n",
    "    V, prior, condprob = trainNB(myVocabList, postingList, classVec)\n",
    "    \n",
    "    test_postingList, test_classVec = loadDataSet('test')\n",
    "    \n",
    "    correct = [0] * 5\n",
    "    for index, text in enumerate(test_postingList):\n",
    "        res = applyMultinomialNB(V, prior, condprob, text)\n",
    "        if res == test_classVec[index]:\n",
    "            correct[test_classVec[index]] += 1\n",
    "    \n",
    "    # 0 --> 党政办公室\n",
    "    # 1 --> 教务部\n",
    "    # 2 --> 招生办公室\n",
    "    # 3 --> 研究生院\n",
    "    # 4 --> 科学技术部\n",
    "    print('整体 正确率：', sum(correct) / len(test_postingList))\n",
    "    print('党政办公室 正确率：', correct[0] / test_classVec.count(0))\n",
    "    print('教务部 正确率：', correct[1] / test_classVec.count(1))\n",
    "    print('招生办公室 正确率：', correct[2] / test_classVec.count(2))\n",
    "    print('研究生院 正确率：', correct[3] / test_classVec.count(3))\n",
    "    print('科学技术部 正确率：', correct[4] / test_classVec.count(4))\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "2b38f256b5e5cfcb08c8fab1bb2c0ed1d264b1b0baf1af9aa926f047321c09a2"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
