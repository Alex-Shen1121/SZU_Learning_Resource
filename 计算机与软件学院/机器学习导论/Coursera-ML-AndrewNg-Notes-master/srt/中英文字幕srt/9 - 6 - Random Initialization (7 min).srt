1
00:00:00,540 --> 00:00:01,820
In the previous videos, we put
在前面的视频中

2
00:00:01,950 --> 00:00:03,220
together almost all
我们总结了

3
00:00:03,270 --> 00:00:04,620
the pieces you need in order
所有

4
00:00:04,820 --> 00:00:07,170
to implement and train in your network.
需要在网络中实施和训练的部分

5
00:00:07,940 --> 00:00:09,060
There's just one last idea I
现在是最后一个

6
00:00:09,120 --> 00:00:09,980
need to share with you, which
我需要分享给你们的想法

7
00:00:10,200 --> 00:00:11,570
is the idea of random initialization.
这就是随机初始化的思想

8
00:00:13,220 --> 00:00:14,360
When you're running an algorithm like
当你运行一个算法

9
00:00:14,510 --> 00:00:15,990
gradient descent or also the
例如梯度下降算法

10
00:00:16,280 --> 00:00:17,810
advanced optimization algorithms, we
或者是先进的优化算法时

11
00:00:17,940 --> 00:00:20,770
need to pick some initial value for the parameters theta.
我们需要给变量θ一些初始值

12
00:00:21,610 --> 00:00:22,990
So for the advanced optimization algorithm, you know,
所以对于那些先进的优化算法

13
00:00:23,570 --> 00:00:24,620
it assumes that you will
假设

14
00:00:24,780 --> 00:00:26,090
pass it some initial value
给变量θ

15
00:00:26,700 --> 00:00:27,640
for the parameters theta.
传递一些初始值

16
00:00:29,010 --> 00:00:30,680
Now let's consider gradient descent.
现在让我们考虑梯度下降

17
00:00:31,320 --> 00:00:34,090
For that, you know, we also need to initialize theta to something.
同样 我们需要把θ初始化成一些值

18
00:00:34,580 --> 00:00:36,030
And then we can slowly take steps
接下来使用梯度下降方法

19
00:00:36,680 --> 00:00:38,830
go downhill, using graded descent,
慢慢地执行这些步骤使其下降

20
00:00:38,910 --> 00:00:40,920
to go downhill to minimize the function J of theta.
使θ的函数J下降到最小

21
00:00:41,990 --> 00:00:43,960
So what do we set the initial value of theta to?
那么θ的初始值该设置为多少呢？

22
00:00:44,240 --> 00:00:47,000
Is it possible to set
是否可以

23
00:00:47,520 --> 00:00:48,930
the initial value of theta
将θ的初始值设为

24
00:00:49,250 --> 00:00:50,450
to the vector of all zeroes.
全部是0的向量

25
00:00:51,870 --> 00:00:54,800
Whereas this worked okay when we were using logistic regression.
当使用逻辑回归

26
00:00:55,630 --> 00:00:56,690
Initializing all of your
初始化所有变量为0

27
00:00:56,760 --> 00:00:57,970
parameters to zero actually
这样做是否可行

28
00:00:58,310 --> 00:01:00,290
does not work when you're trading a neural network.
实际上在训练神经网络时这样做是不可行的

29
00:01:01,410 --> 00:01:03,150
Consider training the following neural network.
以训练这个神经网络为例

30
00:01:03,650 --> 00:01:06,430
And let's say we initialized all of the parameters in the network to zero.
照之前所说将网络中的所有变量初始化为0

31
00:01:07,970 --> 00:01:09,210
And if you do that then
如果是这样的话

32
00:01:09,780 --> 00:01:10,920
what that means is that
具体来说就是

33
00:01:11,160 --> 00:01:13,870
at the initialization this blue weight, that I'm covering blue
当初始化这条蓝色权重

34
00:01:15,390 --> 00:01:16,540
is going to equal to that weight.
使这条被涂为蓝色的权重等于那条蓝色的权重

35
00:01:17,510 --> 00:01:17,510
So, they're both zero.
他们都是0

36
00:01:18,580 --> 00:01:19,880
And this weight that I'm covering
这条被涂上红色的权重

37
00:01:20,330 --> 00:01:21,940
in in red, is equal to that weight.
同样等于

38
00:01:22,550 --> 00:01:23,040
Which I'm covering it in red.
被涂上红色的这条权重

39
00:01:23,790 --> 00:01:25,280
And also this weight, well
同样这个权重

40
00:01:25,620 --> 00:01:26,500
which I'm covering it in green
这个被涂成绿色的权重也一样

41
00:01:26,680 --> 00:01:28,940
is going to be equal to the value of that weight.
等于那条绿色的权重

42
00:01:30,030 --> 00:01:32,820
And what that means is that both of your hidden units: a1 and a2
那样是否就意味着这两个隐藏单元a1 a2

43
00:01:32,950 --> 00:01:35,940
are going to be computing the same function
以同一个输入函数

44
00:01:36,660 --> 00:01:36,810
of your inputs.
计算

45
00:01:37,810 --> 00:01:38,900
And thus, you end up with
这样

46
00:01:39,500 --> 00:01:40,870
for everyone of your training your examples.
对每个例子进行训练

47
00:01:41,430 --> 00:01:43,640
You end up with a(2)1 equals a(2)2.
最后a(2)1与a(2)2结果必然相等

48
00:01:46,950 --> 00:01:48,700
and moreover because, I'm not
更多的原因

49
00:01:48,960 --> 00:01:50,050
going to show this too much
我就不详细讲述了

50
00:01:50,310 --> 00:01:51,420
detail, but because these out
而由于

51
00:01:51,580 --> 00:01:52,990
going weights are the same you
这些权重相同

52
00:01:53,080 --> 00:01:54,630
can also show that the
同样可以得出

53
00:01:54,710 --> 00:01:56,560
delta values are also going to be the same.
这些δ值也相同

54
00:01:56,790 --> 00:01:57,790
So concretely, you end up
具体地说

55
00:01:57,970 --> 00:02:00,070
with delta 1 1,
δ(2)1=δ(2)2

56
00:02:00,760 --> 00:02:02,900
delta 2 1, equals delta 2 2.
δ(2)1=δ(2)2

57
00:02:06,120 --> 00:02:07,150
And if you work through the
同时

58
00:02:07,230 --> 00:02:08,480
map further, what you can
从图中进一步可以得出

59
00:02:08,760 --> 00:02:09,990
show is that the partial derivatives
这些变量的偏导数

60
00:02:11,560 --> 00:02:14,080
with respect to your parameters will satisfy the following.
满足以下条件

61
00:02:15,120 --> 00:02:16,710
That the partial derivative
成本函数的

62
00:02:17,550 --> 00:02:19,260
of the cost
偏导数

63
00:02:19,580 --> 00:02:21,020
function with respect to
求出的导数

64
00:02:21,800 --> 00:02:23,680
writing out the derivatives respect to
代表了

65
00:02:23,900 --> 00:02:25,320
these two blue weights neural network.
神经网络中两条蓝色的权重

66
00:02:26,190 --> 00:02:27,290
You'll find that these two partial
可以注意到

67
00:02:27,680 --> 00:02:30,340
derivatives are going to be equal to each other.
这两个偏导数互为相等

68
00:02:31,970 --> 00:02:33,180
And so, what this means, is
这也就意味着

69
00:02:33,320 --> 00:02:35,820
that even after say, one gradient descent update.
甚至可以说 一旦更新梯度下降方法

70
00:02:36,690 --> 00:02:38,200
You're going to update, say this
第一个蓝色权重也会更新

71
00:02:38,470 --> 00:02:40,800
first blue weight with, you know, learning rate times this.
等于学习率乘以这个式子

72
00:02:41,580 --> 00:02:42,500
And you're going to update the second
第二条蓝色权重更新为

73
00:02:42,920 --> 00:02:44,620
blue weight to a sum learning rate times this.
学习率乘上这个式子

74
00:02:44,820 --> 00:02:45,870
But what this means is
但是 这就意味着

75
00:02:45,980 --> 00:02:47,090
that even after one gradient
一旦更新梯度下降

76
00:02:47,420 --> 00:02:49,330
descent update, those two
这两条

77
00:02:49,680 --> 00:02:50,710
blue weights, those two blue
蓝色权重的值

78
00:02:51,430 --> 00:02:53,050
color parameters will end
最后

79
00:02:53,240 --> 00:02:54,960
up the same as each other.
将互为相等

80
00:02:55,190 --> 00:02:56,210
So they'll be some non-zero
因此 即使权重现在不都为0

81
00:02:56,750 --> 00:02:57,720
value now, but this value
但参数的值

82
00:02:58,550 --> 00:02:59,520
will be equal to that value.
最后也互为相等

83
00:03:00,360 --> 00:03:02,790
And similarly, even after one gradient descent update.
同样地 即使更新一个梯度下降

84
00:03:03,690 --> 00:03:05,740
This value will equal to that value.
这条红色的权重也会等于这条红色的权重

85
00:03:06,170 --> 00:03:07,200
There will be some non-zero values.
也许会有些非0的值

86
00:03:07,640 --> 00:03:09,450
Just that the two red values will be equal to each other.
仅仅是两条红色的值会互为相等

87
00:03:10,240 --> 00:03:11,760
And similarly the two green
同样两条绿色的权重

88
00:03:12,060 --> 00:03:13,720
weights, they'll both change values
开始它们有不同的值

89
00:03:13,860 --> 00:03:16,350
but they'll both end up the same value as each other.
最后这两个权重也会互为相等

90
00:03:17,590 --> 00:03:19,020
So after each update, the parameters corresponding
所以每次更新后

91
00:03:19,740 --> 00:03:20,890
to the inputs going to each
两个隐藏单元的输入参数

92
00:03:21,060 --> 00:03:22,870
of the two hidden units identical.
相同

93
00:03:23,700 --> 00:03:24,490
That's just saying that the two
这只是说

94
00:03:24,710 --> 00:03:25,590
green weights must be sustained,
两条绿色的权重必须持续

95
00:03:25,640 --> 00:03:26,310
the two red weights must be
两条红色的权重必须持续

96
00:03:26,550 --> 00:03:27,750
sustained, the two blue weights
两条蓝色的权重

97
00:03:28,010 --> 00:03:30,000
are still the same and what
仍然相同

98
00:03:30,160 --> 00:03:31,590
that means is that even after
这就意味着

99
00:03:31,770 --> 00:03:33,070
one iteration of say, gradient
即使重复的说

100
00:03:33,460 --> 00:03:34,860
descent, you find that
梯度下降

101
00:03:35,600 --> 00:03:37,250
your two hidden units are still
你们会发现两个隐藏单元

102
00:03:37,800 --> 00:03:40,380
computing exactly the same function that the input.
仍然使用完全相同的输入函数计算

103
00:03:40,830 --> 00:03:43,040
So you still have this a(1)2 equals a(2)2.
因此a(1)2仍然等于a(2)2

104
00:03:43,510 --> 00:03:45,200
And so you're back to this case.
回到这个例子

105
00:03:45,930 --> 00:03:47,380
And as keep running gradient descent.
保持梯度下降

106
00:03:48,390 --> 00:03:50,940
The blue weights, the two blue weights will stay the same as each other.
这两条蓝色的权重仍然相同

107
00:03:51,190 --> 00:03:52,920
The two red weights will stay the same as each other.
两条红色的权重 两条绿色的权重

108
00:03:53,060 --> 00:03:54,990
The two green weights will stay the same as each other.
也是同样的情况

109
00:03:55,160 --> 00:03:56,860
And what this means
这也就意味着

110
00:03:57,130 --> 00:03:58,260
is that your neural network really
这个神经网络

111
00:03:58,470 --> 00:03:59,980
can't compute very interesting functions.
的确不能计算非常有意思的功能

112
00:04:00,700 --> 00:04:01,910
Imagine that you had
想象一下

113
00:04:02,240 --> 00:04:03,670
not only two hidden
不止有两个隐藏单元

114
00:04:04,010 --> 00:04:05,470
units but imagine
而是

115
00:04:05,640 --> 00:04:07,100
that you had many many hidden units.
有很多很多的隐藏单元

116
00:04:08,080 --> 00:04:09,160
Then what this is saying is that
这就是说

117
00:04:09,430 --> 00:04:10,680
all of your hidden units are
所有的隐藏单元

118
00:04:10,740 --> 00:04:12,320
computing the exact same
被完全相同的功能计算

119
00:04:12,540 --> 00:04:16,300
feature, all of your hidden units are computing all of the exact same function of the input.
所有的隐藏单元被完全相同的输入函数计算

120
00:04:17,030 --> 00:04:18,980
And this is a highly redundant representation.
这代表了高度冗余

121
00:04:20,140 --> 00:04:21,010
Because that means that your
因为 这意味着

122
00:04:21,110 --> 00:04:24,160
final logistic regression unit, you know, really only gets to see one feature.
最后的逻辑回归单元只会得到一个功能

123
00:04:24,730 --> 00:04:25,460
Because all of these are the same
因为所有的逻辑回归单元都一样

124
00:04:26,330 --> 00:04:28,690
and this prevents your neural network from learning something interesting.
这样阻止了神经网络学习一些有趣的事

125
00:04:31,600 --> 00:04:32,830
In order to get around this
为了解决这个问题

126
00:04:32,960 --> 00:04:34,050
problem, the way we initialize
神经网络变量

127
00:04:34,590 --> 00:04:35,680
the parameters of a neural network
初始化的方式

128
00:04:36,050 --> 00:04:37,660
therefore, is with random initialization.
采用随机初始化

129
00:04:41,820 --> 00:04:43,130
Concretely, the problem we
具体地

130
00:04:43,250 --> 00:04:44,470
saw on the previous slide
在上一张幻灯片中看到的

131
00:04:44,760 --> 00:04:46,240
is sometimes called the problem
有时被我们称为对称权重的问题是

132
00:04:46,640 --> 00:04:49,040
of symmetric weights, that is if the weights all being the same.
所有的权重相同

133
00:04:49,810 --> 00:04:51,470
And so this random initialization
所以这种随机初始化

134
00:04:52,590 --> 00:04:54,240
is how we perform symmetry breaking.
解决的是如何打破这种对称性

135
00:04:55,520 --> 00:04:56,480
So what we do is we
所以 我们需要做的是

136
00:04:56,680 --> 00:04:58,200
initialize each value of
对θ的每个值

137
00:04:58,310 --> 00:04:59,460
theta to a random
进行初始化

138
00:04:59,830 --> 00:05:01,300
number between minus epsilon and epsilon.
范围在负?到正?之间

139
00:05:02,080 --> 00:05:03,200
So this is a notation to
这个符号意味着

140
00:05:03,310 --> 00:05:05,350
mean numbers between minus epsilon and plus epsilon.
负?到正?之间

141
00:05:06,330 --> 00:05:07,430
So my weights on my
因此

142
00:05:07,540 --> 00:05:08,660
parameters are all going
变量的权重通常初始化为

143
00:05:08,710 --> 00:05:11,470
to be randomly initialized between minus epsilon and plus epsilon.
负?到正?之间的任意一个数

144
00:05:12,300 --> 00:05:13,330
The way I write code to do
我在octave里编写了这样的代码

145
00:05:13,420 --> 00:05:16,770
this in octave, this I've said you know theta 1 to be equal to this.
我之前讲过的θ1等于这个等式

146
00:05:17,550 --> 00:05:19,620
So this rand 10 by 11.
所以 这个10*11的矩阵

147
00:05:19,910 --> 00:05:21,060
That's how you compute
该怎样计算

148
00:05:21,640 --> 00:05:23,620
a random 10 by 11
一个任意的10*11维矩阵

149
00:05:24,670 --> 00:05:26,640
dimensional matrix, and all
矩阵中的所有值

150
00:05:27,070 --> 00:05:30,380
of the values are between 0 and 1.
都介于0到1之间

151
00:05:30,580 --> 00:05:31,350
So these are going to
所以

152
00:05:31,520 --> 00:05:32,700
be real numbers that take on
这些实数

153
00:05:32,870 --> 00:05:34,860
any continuous values between 0 and 1.
取0到1之间的连续值

154
00:05:35,450 --> 00:05:36,290
And so, if you take a
因此

155
00:05:36,320 --> 00:05:37,440
number between 0 and
如果取0到1之间的一个数

156
00:05:37,550 --> 00:05:38,310
1, multiply it by 2
和

157
00:05:38,590 --> 00:05:39,550
times an epsilon, and
2ε相乘

158
00:05:39,600 --> 00:05:41,050
minus an epsilon, then you
再减去ε

159
00:05:41,160 --> 00:05:42,270
end up with a number that's
然后得到

160
00:05:42,690 --> 00:05:44,160
between minus epsilon and plus epsilon.
一个在负ε到正ε间的数

161
00:05:45,640 --> 00:05:46,970
And incidentally, this epsilon here
顺便说一句

162
00:05:47,230 --> 00:05:48,410
has nothing to do
这个ε在这没有什么用处

163
00:05:48,730 --> 00:05:49,860
with the epsilon that we were
通常在进行梯度检查中

164
00:05:50,070 --> 00:05:51,710
using when we were doing gradient checking.
使用

165
00:05:52,590 --> 00:05:54,070
So when we were doing numerical gradient checking,
因此在进行数字化梯度检查时

166
00:05:54,850 --> 00:05:57,060
there we were adding some values of epsilon to theta.
会加一些ε值给θ

167
00:05:57,430 --> 00:05:59,560
This is, you know, an unrelated value of epsilon.
这些值与ε不相关

168
00:05:59,780 --> 00:06:00,590
Which is why I am denoting
这就是为什么我要在这里用ε表示

169
00:06:00,990 --> 00:06:02,200
in it epsilon, just to distinguish
仅仅是为了区分

170
00:06:02,480 --> 00:06:04,970
it from the value of epsilon we were using in gradient checking.
在梯度检查中使用的ε值

171
00:06:06,490 --> 00:06:07,590
Absolutely, if you want to
当然

172
00:06:07,690 --> 00:06:09,620
initialize theta 2
如果想要初始化θ2

173
00:06:09,640 --> 00:06:10,820
to a random 1 by
为任意一个1乘11的矩阵

174
00:06:10,920 --> 00:06:13,430
11 matrix, you can do so using this piece of code here.
可以使用这里的这段代码

175
00:06:15,910 --> 00:06:17,460
So, to summarize, to
总结来说

176
00:06:17,660 --> 00:06:18,910
train a neural network, what you
为了训练神经网络

177
00:06:19,060 --> 00:06:20,850
should do is randomly initialize the
应该对权重进行随机初始化

178
00:06:20,930 --> 00:06:21,810
weights to, you know, small
初始化为

179
00:06:22,120 --> 00:06:23,370
values close to 0, between
负ε到正ε间

180
00:06:23,740 --> 00:06:24,740
minus epsilon and plus epsilon,
接近于0的小数

181
00:06:25,160 --> 00:06:27,150
say, and then implement
然后进行反向传播

182
00:06:27,620 --> 00:06:29,330
back-propagation; do gradient checking;
执行梯度检查

183
00:06:30,220 --> 00:06:31,300
and use either gradient
使用梯度下降

184
00:06:31,660 --> 00:06:32,620
descent or one of the
或者

185
00:06:32,880 --> 00:06:34,860
advanced optimization algorithms to try
使用先进的优化算法

186
00:06:35,100 --> 00:06:36,250
to minimize J of theta
试着使J最小

187
00:06:36,790 --> 00:06:37,860
as a function of the
作为变量θ的功能

188
00:06:38,050 --> 00:06:39,610
parameters theta starting from just
从任意选择变量的初始值

189
00:06:39,890 --> 00:06:41,900
randomly chosen initial value for the parameters.
开始

190
00:06:42,970 --> 00:06:45,440
And by doing symmetry breaking, which is this process.
通过打破对称性这一进程

191
00:06:46,000 --> 00:06:47,110
Hopefully, gradient descent or the
希望梯度下降或是

192
00:06:47,580 --> 00:06:48,820
advanced optimization algorithms will be
先进的优化算法

193
00:06:48,980 --> 00:06:50,710
able to find a good value of theta.
可以找到θ的最优值

