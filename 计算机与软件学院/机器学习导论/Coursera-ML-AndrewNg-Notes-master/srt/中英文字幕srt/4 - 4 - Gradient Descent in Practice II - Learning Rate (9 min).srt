1
00:00:00,040 --> 00:00:01,057
In this video, I wanna give
在本段视频中 我想告诉大家
(字幕整理：中国海洋大学 黄海广,haiguang2000@qq.com )

2
00:00:01,072 --> 00:00:04,041
you more practical tips for getting gradient descent to work.
一些关于梯度下降算法的实用技巧

3
00:00:05,003 --> 00:00:06,024
The ideas in this video will
我将集中讨论

4
00:00:06,046 --> 00:00:08,025
center around the learning rate alpha.
学习率 α

5
00:00:09,092 --> 00:00:11,024
Concretely, here's the gradient
具体来说 这是梯度下降算法的

6
00:00:11,064 --> 00:00:13,040
descent update rule and what
更新规则

7
00:00:13,065 --> 00:00:14,036
I want to do in this video
这里我想要

8
00:00:14,086 --> 00:00:16,062
is tell you about what
告诉大家

9
00:00:16,078 --> 00:00:18,046
I think of as debugging and some
如何调试

10
00:00:18,060 --> 00:00:19,064
tips for making sure that
也就是我认为应该如何确定

11
00:00:19,085 --> 00:00:21,017
Gradient Descent is working correctly
梯度下降是正常工作的

12
00:00:22,039 --> 00:00:23,037
and second, I want to tell you
此外我还想告诉大家

13
00:00:23,058 --> 00:00:25,051
how to choose the rates
如何选择学习率 α

14
00:00:25,089 --> 00:00:26,071
out for, but this is how
也就是我平常

15
00:00:27,007 --> 00:00:28,053
I go about choosing it.
如何选择这个参数

16
00:00:29,021 --> 00:00:30,041
Here's something that I often do
我通常是怎样确定

17
00:00:30,064 --> 00:00:32,071
to make sure gradient descent is working correctly.
梯度下降正常工作的

18
00:00:34,011 --> 00:00:35,056
The job of gradient descent is
梯度下降算法所做的事情

19
00:00:35,082 --> 00:00:37,003
to find a value of
就是为你找到

20
00:00:37,010 --> 00:00:38,050
theta for you that, you
一个 θ 值

21
00:00:38,063 --> 00:00:40,082
know, hopefully minimizes the cost function j of theta.
并希望它能够最小化代价函数 J(θ)

22
00:00:42,067 --> 00:00:43,086
What I often do is therefore
我通常会在

23
00:00:44,029 --> 00:00:45,089
pluck the cost function j
梯度下降算法运行时

24
00:00:46,010 --> 00:00:48,089
of theta as gradient descent runs.
绘出代价函数 J(θ) 的值

25
00:00:49,075 --> 00:00:51,009
So, the x-axis here is
这里的 x 轴是表示

26
00:00:51,031 --> 00:00:52,032
the number of iteration of gradient
梯度下降算法的

27
00:00:52,085 --> 00:00:53,096
descent and as gradient descent
迭代步数

28
00:00:54,025 --> 00:00:55,079
runs, you'll hopefully get a
你可能会得到

29
00:00:55,096 --> 00:00:58,025
plot that maybe looks like this.
这样一条曲线

30
00:00:59,067 --> 00:01:00,088
Notice that the x-axis is
注意 这里的 x 轴

31
00:01:01,017 --> 00:01:02,092
a number of iterations previously
是迭代步数

32
00:01:03,057 --> 00:01:04,076
we were looking at plots of
在我们以前看到的

33
00:01:05,006 --> 00:01:06,065
J of theta where the
J(θ) 曲线中

34
00:01:07,004 --> 00:01:08,009
X-axis, where the horizontal axis,
x 轴 也就是横轴

35
00:01:08,095 --> 00:01:12,026
was the parameter vector theta but this is not where this is.
曾经用来表示参数 θ 但这里不是

36
00:01:13,007 --> 00:01:14,073
Concretely, what this point
具体来说

37
00:01:15,009 --> 00:01:17,073
is is I'm going
这一点的含义是这样的

38
00:01:17,090 --> 00:01:19,050
to rank gradient descent for hundred iterations.
当我运行完100步的梯度下降迭代之后

39
00:01:20,057 --> 00:01:22,006
And whatever value I get
无论我得到

40
00:01:22,062 --> 00:01:23,090
for theta after a hundred
什么 θ 值

41
00:01:24,010 --> 00:01:25,042
of the rations and get,
总之 100步迭代之后

42
00:01:25,060 --> 00:01:26,076
you know, some value of theta
我将得到

43
00:01:27,015 --> 00:01:28,095
after a hundred iterations and I'm
一个 θ 值

44
00:01:29,009 --> 00:01:30,035
going to evaluate the cost
根据100步迭代之后

45
00:01:30,067 --> 00:01:32,057
function J of theta for
得到的这个 θ 值

46
00:01:32,084 --> 00:01:33,078
the value of theta I get
我将算出

47
00:01:34,012 --> 00:01:36,001
after a hundred iterations and this
代价函数 J(θ) 的值

48
00:01:36,021 --> 00:01:37,060
vertical height is the
而这个点的垂直高度就代表

49
00:01:37,068 --> 00:01:39,073
value of J of theta for
梯度下降算法

50
00:01:39,090 --> 00:01:40,075
the value of theta I got
100步迭代之后

51
00:01:41,010 --> 00:01:42,015
after a hundred other ratios of
得到的 θ

52
00:01:42,021 --> 00:01:43,084
gradient descent and this
算出的 J(θ) 值

53
00:01:44,004 --> 00:01:45,070
point here, that corresponds
而这个点

54
00:01:46,051 --> 00:01:48,012
to the value of J of
则是梯度下降算法

55
00:01:48,023 --> 00:01:49,071
theta for the theta
迭代200次之后

56
00:01:50,006 --> 00:01:51,081
that I get after I've
得到的 θ

57
00:01:52,004 --> 00:01:53,068
run grade and descent for two hundred iterations.
算出的 J(θ) 值

58
00:01:55,023 --> 00:01:56,018
So what this plot is showing,
所以这条曲线

59
00:01:56,071 --> 00:01:58,009
is it's showing the value of
显示的是

60
00:01:58,020 --> 00:02:01,020
your cost function after iteration of grade and descent.
梯度下降算法迭代过程中代价函数 J(θ) 的值

61
00:02:02,001 --> 00:02:03,012
And, if grade and descent is
如果梯度下降算法

62
00:02:03,034 --> 00:02:04,098
working properly, then J
正常工作

63
00:02:05,018 --> 00:02:06,093
of theta should decrease.
那么每一步迭代之后

64
00:02:10,006 --> 00:02:10,065
after every iteration.
J(θ) 都应该下降

65
00:02:17,081 --> 00:02:19,028
And one useful thing
这条曲线

66
00:02:19,053 --> 00:02:20,037
that this sort of plot can
的一个用处在于

67
00:02:20,050 --> 00:02:21,075
tell you also is that
它可以告诉你

68
00:02:22,050 --> 00:02:23,087
if you look at the specific figure
如果你看一下

69
00:02:24,015 --> 00:02:25,040
that I've drawn, it looks like
我画的这条曲线

70
00:02:26,003 --> 00:02:27,034
by the time you've gotten out
当你达到

71
00:02:27,058 --> 00:02:28,078
to three hundred iterations,
300步迭代之后

72
00:02:29,072 --> 00:02:31,000
between three and four hundred
也就是300步到400步迭代之间

73
00:02:31,031 --> 00:02:32,080
iterations, in this segment, it
也就是曲线的这一段

74
00:02:32,090 --> 00:02:35,069
looks like J of theta hasn't gone down much more.
看起来 J(θ) 并没有下降多少

75
00:02:35,081 --> 00:02:36,072
So by the time you get
所以当你

76
00:02:36,096 --> 00:02:38,059
to four hundred iterations, it looks
到达400步迭代时

77
00:02:38,081 --> 00:02:40,080
like this curve has flattened out here.
这条曲线看起来已经很平坦了

78
00:02:41,055 --> 00:02:43,018
And so, way out
也就是说

79
00:02:43,034 --> 00:02:44,040
here at four hundred iterations, it
在这里400步迭代的时候

80
00:02:44,050 --> 00:02:45,056
looks like grade and descend has
梯度下降算法

81
00:02:45,084 --> 00:02:47,075
more or less converged because your
基本上已经收敛了

82
00:02:47,087 --> 00:02:49,056
cost function isn't going down much more.
因为代价函数并没有继续下降

83
00:02:50,049 --> 00:02:51,043
So looking at this figure can
所以说 看这条曲线

84
00:02:51,059 --> 00:02:52,099
also help you judge
可以帮助你判断

85
00:02:53,041 --> 00:02:55,012
whether or not gradient descent has converged.
梯度下降算法是否已经收敛

86
00:02:57,055 --> 00:02:58,050
By the way, the number of
顺便说一下

87
00:02:58,084 --> 00:03:00,041
iterations that gradient descent takes
对于每一个特定的问题

88
00:03:00,078 --> 00:03:01,075
to converge for a physical
梯度下降算法所需的迭代次数

89
00:03:01,090 --> 00:03:03,081
application can vary a lot.
可以相差很大

90
00:03:04,019 --> 00:03:05,062
So maybe for one application gradient
也许对于某一个问题

91
00:03:06,012 --> 00:03:07,056
descent may converge after just
梯度下降算法

92
00:03:07,083 --> 00:03:09,065
thirty iterations, for a
只需要30步迭代就可以收敛

93
00:03:10,021 --> 00:03:12,027
different application gradient descent
然而换一个问题

94
00:03:12,059 --> 00:03:14,015
made the 3,000 iterations.
也许梯度下降算法就需要3000步迭代

95
00:03:15,005 --> 00:03:17,055
For another learning algorithm
对于另一个机器学习问题

96
00:03:17,097 --> 00:03:19,009
it may take three million iterations.
则可能需要三百万步迭代

97
00:03:19,081 --> 00:03:20,062
It turns out to be
实际上

98
00:03:20,072 --> 00:03:22,021
very difficult to tell in
我们很难提前判断

99
00:03:22,030 --> 00:03:24,000
advance how many iterations gradient
梯度下降算法

100
00:03:24,036 --> 00:03:25,075
descent needs to converge, and
需要多少步迭代才能收敛

101
00:03:26,015 --> 00:03:27,094
is usually by plotting this sort of plot.
通常我们需要画出这类曲线

102
00:03:28,093 --> 00:03:32,025
Plotting the cause function as we increase the number of iterations.
画出代价函数随迭代步数数增加的变化曲线

103
00:03:32,096 --> 00:03:33,087
It's usually by looking at these
通常 我会通过看这种曲线

104
00:03:34,034 --> 00:03:35,040
plots that I tried to tell
来试着判断

105
00:03:35,059 --> 00:03:37,006
if gradient descent has converged.
梯度下降算法是否已经收敛

106
00:03:38,059 --> 00:03:39,081
It is also possible to come
另外 也可以

107
00:03:40,012 --> 00:03:42,040
up with automatic convergence test; namely
进行一些自动的收敛测试

108
00:03:42,074 --> 00:03:44,006
to have an algorithm to try
也就是说用一种算法

109
00:03:44,028 --> 00:03:46,027
to tell you if gradient descent
来告诉你梯度下降算法

110
00:03:46,059 --> 00:03:48,040
has converged and here's maybe
是否已经收敛

111
00:03:48,062 --> 00:03:50,015
a pretty typical example of an
自动收敛测试

112
00:03:50,024 --> 00:03:52,031
automatic convergence test and
一个非常典型的例子是

113
00:03:52,053 --> 00:03:53,094
so, you test the clear convergence
如果代价函数 J(θ)

114
00:03:54,096 --> 00:03:56,024
if your cause function jf theta
的下降小于

115
00:03:57,002 --> 00:03:58,015
decreases by less than
一个很小的值 ε

116
00:03:58,037 --> 00:04:01,025
some small value epsilon, some
那么就认为已经收敛

117
00:04:01,040 --> 00:04:02,031
small value ten to the
比如可以选择

118
00:04:02,040 --> 00:04:03,081
minus three in one iteration,
1e-3

119
00:04:05,025 --> 00:04:06,065
but I find that usually
但我发现

120
00:04:07,006 --> 00:04:09,053
choosing what this threshold is is pretty difficult.
通常要选择一个合适的阈值 ε 是相当困难的

121
00:04:10,071 --> 00:04:11,087
So, in order to check
因此 为了检查

122
00:04:12,003 --> 00:04:13,075
your gradient descent has converged, I
梯度下降算法是否收敛

123
00:04:14,009 --> 00:04:15,012
actually tend to look at
我实际上还是

124
00:04:15,034 --> 00:04:16,072
plots like like this
通过看

125
00:04:17,005 --> 00:04:18,012
figure on the left rather than
左边的这条曲线图

126
00:04:18,031 --> 00:04:20,063
rely on an automatic convergence test.
而不是依靠自动收敛测试

127
00:04:21,076 --> 00:04:22,063
Looking at this sort of
此外 这种曲线图

128
00:04:22,077 --> 00:04:24,013
figure can also tell you or
也可以

129
00:04:24,031 --> 00:04:25,055
give you an advanced warning if maybe
在算法没有正常工作时

130
00:04:25,081 --> 00:04:27,049
gradient descent is not working correctly.
提前警告你

131
00:04:28,068 --> 00:04:29,076
Concretely, if you plug
具体地说

132
00:04:30,019 --> 00:04:31,041
jf theta as a function
如果代价函数 J(θ)

133
00:04:31,064 --> 00:04:34,044
of number of iterations, then, if
随迭代步数

134
00:04:34,085 --> 00:04:35,050
you see a figure like this,
的变化曲线是这个样子

135
00:04:35,081 --> 00:04:36,072
where J of theta is actually
J(θ) 实际上在不断上升

136
00:04:37,012 --> 00:04:38,088
increasing, then that gives
那么这就很明确的表示

137
00:04:39,011 --> 00:04:41,075
you a clear sign that gradient descent is not working.
梯度下降算法没有正常工作

138
00:04:42,088 --> 00:04:44,006
And a figure like this
而这样的曲线图

139
00:04:44,051 --> 00:04:46,093
usually means that you should be using a learning rate alpha.
通常意味着你应该使用较小的学习率 α

140
00:04:48,026 --> 00:04:49,042
If J of theta is actually
如果 J(θ) 在上升

141
00:04:49,062 --> 00:04:51,020
increasing, the most common
那么最常见的原因是

142
00:04:51,057 --> 00:04:52,082
cause for that is if
你在最小化

143
00:04:53,018 --> 00:04:54,035
you're trying to minimize
这样的

144
00:04:54,086 --> 00:04:57,089
the function that maybe looks like this.
一个函数

145
00:04:59,033 --> 00:05:00,037
That's if your learning rate is
这时如果你的学习率太大

146
00:05:00,045 --> 00:05:01,045
too big then if you
当你从这里开始

147
00:05:01,060 --> 00:05:02,093
start off there, gradient descent
梯度下降算法

148
00:05:03,019 --> 00:05:05,026
may overshoot the minimum, send
可能将冲过最小值达到这里

149
00:05:05,044 --> 00:05:06,075
you there, then if only there's
而如果你的学习率太大

150
00:05:07,007 --> 00:05:08,014
too big, you may overshoot again,
你可能再次冲过最小值

151
00:05:08,050 --> 00:05:10,037
it will send you there and
达到这里

152
00:05:10,050 --> 00:05:11,091
so on so that what
然后一直这样下去

153
00:05:12,025 --> 00:05:13,061
you really wanted was really
而你真正想要的是

154
00:05:13,081 --> 00:05:16,036
start here and for to slowly go downhill.
从这里开始慢慢的下降

155
00:05:17,093 --> 00:05:19,025
But if the learning is too
但是 如果学习率过大

156
00:05:19,044 --> 00:05:20,095
big then gradient descent can
那么梯度下降算法

157
00:05:21,025 --> 00:05:22,057
instead keep on over
将会不断的

158
00:05:22,075 --> 00:05:24,030
shooting the minimum so
冲过最小值

159
00:05:24,044 --> 00:05:25,067
that you actually end up
然后你将得到

160
00:05:26,016 --> 00:05:27,017
getting worse and worse instead
越来越糟糕的结果

161
00:05:27,020 --> 00:05:28,072
of getting the higher values of
得到越来越大的

162
00:05:28,077 --> 00:05:29,079
the cost function j of theta
代价函数 J(θ) 值

163
00:05:30,070 --> 00:05:31,051
so do you end up with a
所以如果你得到了

164
00:05:31,067 --> 00:05:33,013
plot like and if you
这样一个曲线图

165
00:05:33,022 --> 00:05:34,011
see a plot like this the
如果你看到这样一个曲线图

166
00:05:34,018 --> 00:05:35,086
fix usually is to just
通常的解决方法是

167
00:05:36,008 --> 00:05:37,068
use a smaller value of alpha.
使用较小的 α 值

168
00:05:38,016 --> 00:05:39,063
Oh, and also of course make
当然也要确保

169
00:05:39,079 --> 00:05:41,061
sure that your code does not have a bug in it.
你的代码中没有错误

170
00:05:41,079 --> 00:05:43,012
But usually to watch it
但通常最可能

171
00:05:43,020 --> 00:05:44,054
out of the firms is the
出现的错误是

172
00:05:44,060 --> 00:05:46,031
most common, could be a common problem.
α 值过大

173
00:05:49,005 --> 00:05:50,041
Similarly, sometimes, you may
同样的 有时你可能

174
00:05:50,056 --> 00:05:51,089
also see j of theta
看到这种形状的

175
00:05:52,012 --> 00:05:53,007
do something like this and it
J(θ) 曲线

176
00:05:53,018 --> 00:05:54,005
go down for a while then
它先下降 然后上升

177
00:05:54,016 --> 00:05:56,013
go up then go down for a while then go up.
接着又下降 然后又上升

178
00:05:56,032 --> 00:05:57,012
Go down for a while, it
然后再次下降

179
00:05:57,022 --> 00:05:58,091
goes up and so on and
再次上升 如此往复

180
00:05:58,093 --> 00:05:59,094
and to fix for something like
而解决这种情况的方法

181
00:06:00,013 --> 00:06:02,075
this is also to use a smaller value of algorithm.
通常同样是选择较小 α 值

182
00:06:04,008 --> 00:06:04,095
I'm not going to prove it
我不打算证明这一点

183
00:06:05,007 --> 00:06:06,081
here, but undeniable assumptions about
但对于我们讨论的线性回归

184
00:06:07,010 --> 00:06:09,075
the cost function, which does proof of linear regression.
可以很容易从数学上证明

185
00:06:10,082 --> 00:06:12,047
You can show of mathematicians have
只要学习率足够小

186
00:06:12,057 --> 00:06:13,058
shown that if your learning
那么每次迭代之后

187
00:06:13,091 --> 00:06:15,006
rate offer is small enough
代价函数 J(θ)

188
00:06:15,083 --> 00:06:18,043
then j of theta should decrease on every single iteration.
都会下降

189
00:06:19,002 --> 00:06:20,085
So, if this doesn't happen, probably
因此如果代价函数没有下降

190
00:06:21,033 --> 00:06:22,019
means algorithm is too big then
那可能以为着学习率过大

191
00:06:22,026 --> 00:06:23,089
you should send a smaller, but of
这时你就应该尝试一个较小的学习率

192
00:06:23,097 --> 00:06:24,079
course, you all So you don't
当然 你也不希望

193
00:06:24,088 --> 00:06:25,068
want your learning rate to be
学习度太小

194
00:06:25,073 --> 00:06:26,095
too small because if you
因为如果这样

195
00:06:27,006 --> 00:06:27,091
do that, if you were
如果你这么做

196
00:06:28,002 --> 00:06:30,042
to do that, then gradient descent can be slow to converge.
那么梯度下降算法可能收敛得很慢

197
00:06:31,049 --> 00:06:32,051
And if alpha were too
如果学习率 α 太小

198
00:06:32,080 --> 00:06:34,005
small, you might end up
你可能

199
00:06:34,074 --> 00:06:36,075
starting out here, say, and,
从这里开始

200
00:06:36,095 --> 00:06:37,091
you know, end up taking just
然后很缓慢很缓慢

201
00:06:38,022 --> 00:06:39,069
minuscule, minuscule baby steps.
向最低点移动

202
00:06:40,074 --> 00:06:40,074
Right?
这样一来

203
00:06:40,088 --> 00:06:42,022
And just taking a lot
你需要迭代很多次

204
00:06:42,098 --> 00:06:46,031
of iterations before you finally get to the minimum.
才能到达最低点

205
00:06:47,008 --> 00:06:47,098
And so, if alpha is too
因此 如果学习率 α 太小

206
00:06:48,011 --> 00:06:49,050
small, gradient descent can
梯度下降算法

207
00:06:49,056 --> 00:06:51,022
make very slow progress and be slow to converge.
的收敛将会很缓慢

208
00:06:53,081 --> 00:06:55,010
To summarize, if the learning
总结一下

209
00:06:55,037 --> 00:06:57,006
rate is too small, you can
如果学习率 α 太小

210
00:06:57,026 --> 00:06:59,037
have a slow convergence problem, and
你会遇到收敛速度慢的问题

211
00:06:59,062 --> 00:07:00,093
if the learning rate is too
而如果学习率 α 太大

212
00:07:01,010 --> 00:07:02,033
large, j of theta may
代价函数 J(θ) 可能不会在

213
00:07:02,047 --> 00:07:03,043
not decrease on every iteration
每次迭代都下降

214
00:07:04,039 --> 00:07:05,056
and may not even converge.
甚至可能不收敛

215
00:07:07,010 --> 00:07:08,020
In some cases, if the learning
在某些情况下

216
00:07:08,052 --> 00:07:10,007
rate is too large, slow convergence
如果学习率 α 过大

217
00:07:10,099 --> 00:07:14,070
is also possible, but the
也可能出现收敛缓慢的问题

218
00:07:14,080 --> 00:07:16,001
more common problem you see
但更常见的情况是

219
00:07:16,027 --> 00:07:17,037
is that just that j of
你会发现代价函数 J(θ)

220
00:07:17,043 --> 00:07:19,026
theta may not decrease on every iteration.
并不会在每次迭代之后都下降

221
00:07:20,054 --> 00:07:21,089
And in order to debug all
而为了调试

222
00:07:22,013 --> 00:07:24,017
of these things, often plotting that
所有这些情况

223
00:07:24,043 --> 00:07:25,073
j of theta as a function
绘制J(θ)随迭代步数变化的曲线

224
00:07:26,006 --> 00:07:28,069
of the number of iterations can help you figure out what's going on.
通常可以帮助你弄清楚到底发生了什么

225
00:07:29,026 --> 00:07:30,075
Concretely, what I actually
具体来说

226
00:07:31,022 --> 00:07:32,013
do when I run gradient
当我运行梯度下降算法时

227
00:07:32,051 --> 00:07:34,072
descent is I would try a range of values.
我通常会尝试一系列α值

228
00:07:35,000 --> 00:07:36,017
So just try running gradient descent
所以在运行梯度下降算法制

229
00:07:36,057 --> 00:07:37,068
with a range of values for
请尝试不同的 α 值

230
00:07:37,098 --> 00:07:39,061
alpha, like 0.001, 0.01,
比如0.001, 0.01

231
00:07:39,086 --> 00:07:41,024
so these are a
这里每隔10倍

232
00:07:41,044 --> 00:07:43,006
factor of 10 differences, and
取一个值

233
00:07:43,027 --> 00:07:44,027
for these differences of this
然后对于这些不同的 α 值

234
00:07:44,042 --> 00:07:45,060
of alpha, just plot j of
绘制 J(θ)

235
00:07:45,075 --> 00:07:46,080
theta as a function of number
随迭代步数变化的曲线

236
00:07:47,002 --> 00:07:48,074
of iterations and then pick
然后选择

237
00:07:49,017 --> 00:07:50,093
the value of alpha that, you
看上去使得 J(θ)

238
00:07:51,004 --> 00:07:54,022
know, seems to be causing j of theta to decrease rapidly.
快速下降的一个 α 值

239
00:07:55,061 --> 00:07:58,008
In fact, what I do actually isn't these steps of ten.
事实上 我通常并不是隔10倍取一个值

240
00:07:58,058 --> 00:07:59,051
So, you know, this is
你可以看到

241
00:07:59,088 --> 00:08:01,077
a scale factor of ten if you reach the top.
这里是每隔10倍取一个值

242
00:08:02,050 --> 00:08:03,045
What I'll actually do is try
我通常取的

243
00:08:03,087 --> 00:08:08,048
this range of values and
是这些 α 值

244
00:08:08,061 --> 00:08:09,076
so on where this is,
一直这样下去

245
00:08:09,097 --> 00:08:12,018
you know, opening 001
你看 先取0.001

246
00:08:12,018 --> 00:08:13,025
then increase the linear rate to
然后将学习率增加3倍

247
00:08:13,050 --> 00:08:15,031
3.4 to get 0.03 and then
得到0.003

248
00:08:15,050 --> 00:08:16,031
to step up this is another
然后这一步

249
00:08:17,032 --> 00:08:20,025
roughly 3 fold increase point
从0.003到0.01

250
00:08:21,070 --> 00:08:22,043
of 0.03 to 0.01s and so these
又大约增加了3倍

251
00:08:22,075 --> 00:08:24,081
are roughly, you know,
所以 在为梯度下降算法

252
00:08:26,001 --> 00:08:27,075
trying out gradient descents with each
选择合适的学习率时

253
00:08:28,001 --> 00:08:29,011
value I try being about
我大致是

254
00:08:29,037 --> 00:08:30,089
3X bigger than the previous value.
按3的倍数来取值的

255
00:08:32,012 --> 00:08:33,025
So what I'll do is a range
所以我会尝试一系列α值

256
00:08:33,040 --> 00:08:34,058
of values until I've made sure
直到我找到

257
00:08:34,087 --> 00:08:35,096
that I've found one value that
一个值

258
00:08:36,011 --> 00:08:36,088
is too small and made sure
它不能再小了

259
00:08:37,008 --> 00:08:38,013
I found one value that is
同时找到另一个值

260
00:08:38,025 --> 00:08:39,038
too large, and then I sort
它不能再大了

261
00:08:39,063 --> 00:08:40,096
of try to pick the largest
然后我尽量挑选

262
00:08:41,040 --> 00:08:42,069
possible value or just something
其中最大的那个 α 值

263
00:08:43,011 --> 00:08:45,008
slightly smaller than the
或者一个比最大值

264
00:08:45,021 --> 00:08:47,039
largest reasonable value that I found.
略小一些的合理的值

265
00:08:47,075 --> 00:08:48,078
And when I do that
而当我做了以上工作时

266
00:08:49,026 --> 00:08:50,035
usually it just gives me
我通常就可以得到

267
00:08:50,052 --> 00:08:52,001
a good learning rate for my problem.
一个不错的学习率

268
00:08:53,023 --> 00:08:53,091
And if you do this
如果也你这样做

269
00:08:54,008 --> 00:08:55,003
too, hopefully you will be
那么你也能够

270
00:08:55,012 --> 00:08:56,019
able to choose a good
为你的梯度下降算法

271
00:08:56,046 --> 00:08:57,034
learning rate for your implementation
找到一个合适的

272
00:08:58,050 --> 00:08:58,086
of gradient descent.
学习率值

