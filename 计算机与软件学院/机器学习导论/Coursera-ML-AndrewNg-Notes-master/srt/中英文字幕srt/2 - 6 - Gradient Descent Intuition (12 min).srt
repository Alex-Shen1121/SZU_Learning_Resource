1
00:00:00,000 --> 00:00:04,353
In the previous video, we gave a
mathematical definition of gradient
在之前的视频中  我们给出了一个数学上关于梯度
(字幕整理：中国海洋大学 黄海广,haiguang2000@qq.com )

2
00:00:04,353 --> 00:00:09,464
descent. Let's delve deeper, and in this
video, get better intuition about what the
下降的定义 本次视频我们更深入研究一下 更直观地感受一下这个

3
00:00:09,464 --> 00:00:14,701
algorithm is doing, and why the steps of
the gradient descent algorithm might make
算法是做什么的 以及梯度下降算法的更新过程有什么意义

4
00:00:14,701 --> 00:00:20,639
sense. Here's the gradient descent
algorithm that we saw last time. And, just
这是我们上次视频中看到的梯度下降算法

5
00:00:20,639 --> 00:00:26,427
to remind you, this parameter, or this
term, alpha, is called the learning rate.
提醒一下 这个参数 α  术语称为学习速率

6
00:00:26,427 --> 00:00:32,444
And it controls how big a step we take
when updating my parameter theta J. And
它控制我们以多大的幅度更新这个参数θj.

7
00:00:32,444 --> 00:00:41,360
this second term here is the derivative
term. And what I want to do in this video
第二部分是导数项   而我在这个视频中要做的就是

8
00:00:41,360 --> 00:00:47,360
is give you better intuition about what each of
these two terms is doing and why, when put
给你一个更直观的认识 这两部分有什么用 以及 为什么当把

9
00:00:47,360 --> 00:00:53,077
together, this entire update makes sense.
In order to convey these intuitions, what
这两部分放一起时 整个更新过程是有意义的  为了更好地让你明白

10
00:00:53,077 --> 00:00:58,460
I want to do is use a slightly simpler
example where we want to minimize the
我要做是用一个稍微简单的例子 比如我们想最小化的那个

11
00:00:58,460 --> 00:01:03,022
function of just one parameter. So, so we
have a, say we have a cost function J of
函数只有一个参数的情形 所以 假如我们有一个代价函数J

12
00:01:03,022 --> 00:01:07,294
just one parameter, theta one, like we
did, you know, a few videos back. Where
只有一个参数 θ1  就像我们前几次视频中讲的

13
00:01:07,294 --> 00:01:11,913
theta one is a real number, okay? Just so we can have 1D plots, which
θ1是一个实数 对吧？那么我们可以画出一维的曲线

14
00:01:11,913 --> 00:01:16,416
are a little bit simpler to look at. And
let's try to understand why gradient
看起来很简单 让我们试着去理解 为什么梯度下降法

15
00:01:16,416 --> 00:01:23,940
descent would do on this function.
[sound]. So, let's say here's my function.
会在这个函数上起作用 所以 假如这是我的函数

16
00:01:24,660 --> 00:01:31,696
J of theta one, and so that's my, and
where theta one is a real number. Right,
关于θ1的函数J θ1是一个实数 对吧？

17
00:01:31,696 --> 00:01:39,202
now let's say I've initialized gradient
descent with theta one at this location.
现在我们已经对这个点上用于梯度下降法的θ1 进行了初始化

18
00:01:39,202 --> 00:01:46,989
So image that we start off at that point
on my function. What gradient descent will
想象一下在我的函数图像上 从那个点出发 那么梯度下降

19
00:01:46,989 --> 00:01:56,935
do, is it will update. Theta one gets
updated as theta one minus Alpha times DD
要做的事情是不断更新 θ1等于θ1减α倍的

20
00:01:56,935 --> 00:02:04,694
theta one J of theta one, right? and oh an
just as an aside you know this, this
d/dθ1J(θ1)这个项 对吧？哦 顺便插一句 你知道

21
00:02:04,694 --> 00:02:11,636
derivative term, right? If you're
wondering why I changed the notation from
这个微分项是吧？可能你想问为什么我改变了符号

22
00:02:11,636 --> 00:02:16,132
these partial derivative symbols. If you
don't know what the difference is between
之前用的是偏导数的符号 如果你不知道偏导数的符号

23
00:02:16,132 --> 00:02:20,523
these partial derivative symbols and the
dd theta don't worry about it. Technically
和d/dθ之间的区别是什么 不用担心 从技术上讲

24
00:02:20,523 --> 00:02:24,491
in mathematics we call this a partial
derivative, we call this a derivative,
在数学中 我们称这是一个偏导数 这是一个导数

25
00:02:24,491 --> 00:02:28,299
depending on the number of, of parameters
in the function J, but that's a
这取决于函数J的参数数量 但是这是一个

26
00:02:28,299 --> 00:02:32,428
mathematical technicality, so, you know
For the purpose of this lecture, think of
数学上的区别 就本课的目标而言 可以默认为

27
00:02:32,428 --> 00:02:36,768
these partial symbols, and DD theta one as
exactly the same thing. And, don't worry
这些偏导数符号 和d/dθ1是完全一样的东西 不用担心

28
00:02:36,768 --> 00:02:41,056
about whether there are any differences.
I'm gonna try to use the mathematically
是否存在任何差异 我会尽量使用数学上的

29
00:02:41,056 --> 00:02:45,190
precise notation. But for our purposes,
these notations are really the same thing.
精确的符号 但就我们的目的而言 这些符号是没有区别的

30
00:02:45,360 --> 00:02:49,627
So, let's see what this, this equation
will do. And so we're going to compute
好的 那么我们来看这个方程 我们要计算

31
00:02:49,627 --> 00:02:54,293
this derivative of, I'm not sure if you've
seen derivatives in calculus before. But
这个导数 我不确定之前你是否在微积分中学过导数

32
00:02:54,293 --> 00:02:58,666
what a derivative, at this point, does, is
basically saying, you know, let's. Take
但对于这个问题 求导的目的 基本上可以说

33
00:02:58,666 --> 00:03:02,877
the tangent to that point, like that
straight line, the red line, just,
取这一点的切线 就是这样一条红色的直线

34
00:03:02,877 --> 00:03:06,976
just touching this function and
let's look at the slope of this red line. That's
刚好与函数相切于这一点 让我们看看这条红色直线的斜率

35
00:03:06,976 --> 00:03:11,352
where the derivative is. It says
what's the slope of the line that is just
其实这就是导数 也就是说 直线的斜率 也就是这条

36
00:03:11,352 --> 00:03:15,563
tangent to the function, okay, and the
slope of the line is of course is just
刚好与函数曲线相切的这条直线 这条直线的斜率正好是

37
00:03:15,563 --> 00:03:20,789
right, you know just the height divided by
this horizontal thing. Now. This line has
这个高度除以这个水平长度 现在 这条线有

38
00:03:20,789 --> 00:03:28,378
a positive slope, so it has a positive
derivative. And so, my update to theta is
一个正斜率 也就是说它有正导数 因此 我得到的新的θ

39
00:03:28,378 --> 00:03:36,258
going to be, theta one gives the update that
theta one minus alpha times some positive
θ1更新后等于θ1减去一个正数乘以α.

40
00:03:36,258 --> 00:03:43,103
number. Okay? Alpha, the learning
rate is always a positive number. And so
α 也就是学习速率也是一个正数 所以

41
00:03:43,103 --> 00:03:47,932
I'm gonna to take theta one, this update
as theta one minus something. So I'm gonna
我要使θ1减去一个东西

42
00:03:47,932 --> 00:03:52,644
end up moving theta one to the left. I'm
gonna decrease theta one and we can see
所以相当于我将θ1向左移 使θ1变小了 我们可以看到

43
00:03:52,644 --> 00:03:57,473
this is the right thing to do because I
actually went ahead in this direction you
这么做是对的 因为实际上我往这个方向移动

44
00:03:57,473 --> 00:04:02,582
know to get me closer to the minimum over
there. So, gradient descent so far seems
确实让我更接近那边的最低点 所以 梯度下降到目前为止似乎

45
00:04:02,582 --> 00:04:08,115
to be doing the right thing. Let's look at
another example. So let's take my same
是在做正确的事 让我们来看看另一个例子 让我们用同样的函数J

46
00:04:08,115 --> 00:04:13,787
function j. Just trying to draw the same
function j of theta one. And now let's say
同样再画出函数J(θ1)的图像 而这次

47
00:04:13,787 --> 00:04:19,181
I had instead initialized my parameter
over there on the left. So theta one is
我们把参数初始化到左边这点 所以θ1在这里

48
00:04:19,181 --> 00:04:24,161
here. I'm gonna add that point on the
surface. Now, my derivative term, d, d
同样把这点对应到曲线上 现在 导数项d/dθ1J(θ1)

49
00:04:24,161 --> 00:04:29,567
theta one j of theta one, when evaluated
at this point, gonna look at right. The
在这点上计算时 看上去会是这样

50
00:04:29,567 --> 00:04:35,035
slope of that line. So this derivative
term is a slope of this line. But this
这条线的斜率 这个导数是这条线的斜率

51
00:04:35,035 --> 00:04:42,745
line is slanting down, so this line has
negative slope. Right? Or alternatively I
但是这条线向下倾斜 所以这条线具有负斜率 对吧？

52
00:04:42,745 --> 00:04:48,718
say that this function has negative
derivative, just means negative slope at
或者说 这个函数有负导数 也就意味着在那一点上有负斜率

53
00:04:54,770 --> 00:05:02,840
theta is updated as theta minus alpha
times a negative number. And so I have
θ被更新为θ减去α乘以一个负数 因此我是在用

54
00:05:02,840 --> 00:05:07,881
theta one minus a negative number which
means I'm actually going to increase theta,
θ1减去一个负数 这意味着我实际上是在增加θ1

55
00:05:07,881 --> 00:05:13,106
right? Because this is minus of a negative
number means I'm adding something to theta
对不对？因为这是减去一个负数 意味着给θ加上一个数

56
00:05:13,106 --> 00:05:17,900
and what that means is that I'm going to
end up increasing theta. And so we'll
这就意味着最后我实际上增加了θ的值 因此 我们将

57
00:05:17,900 --> 00:05:23,002
start here and increase theta, which again
seems like the thing I want to do to try
从这里开始 增加θ 似乎这也是我希望得到的 也就是

58
00:05:23,002 --> 00:05:28,335
to get me closer to the minimum. So, this
hopefully explains the intuition behind
让我更接近最小值了 所以 我希望这样很直观地给你解释了

59
00:05:28,335 --> 00:05:33,874
what the derivative term is doing. Let's
next take a look at the learning rate term
导数项的意义 让我们接下来再看一看学习速率α

60
00:05:33,874 --> 00:05:39,956
alpha, and try to figure out what that's
doing. So, here's my gradient descent
我们来研究一下它有什么用 这就是我梯度下降法的

61
00:05:39,956 --> 00:05:46,641
update rule. Right, there's this equation
And let's look at what can happen, if
更新规则 就是这个等式 让我们来看看如果α 太小或 α 太大

62
00:05:46,641 --> 00:05:52,845
Alpha is either too small, or if Alpha is
too large. So this first example, what
会出现什么情况 这第一个例子

63
00:05:52,845 --> 00:05:59,583
happens if Alpha is too small. So here's
my function J. J of theta. Lets
α太小会发生什么呢 这是我的函数J(θ)

64
00:05:59,583 --> 00:06:04,230
just start here. If alpha is too small
then what I'm going to do is gonna
就从这里开始 如果α太小了 那么我要做的是要去

65
00:06:04,230 --> 00:06:09,322
multiply the update by some small number.
So end up taking, you know, it's like a baby step
用一个比较小的数乘以更新的值 所以最终 它就像一个小宝宝的步伐

66
00:06:09,322 --> 00:06:13,841
like that. Okay, so that's one step
[inaudible]. Then from this new point
这是一步  然后从这个新的起点开始

67
00:06:13,841 --> 00:06:18,870
we're gonna take another step. But if
the alpha is too small lets take another
迈出另一步 但是由于α 太小 因此只能迈出另一个

68
00:06:18,870 --> 00:06:25,342
little baby step. And so if And so if my
learning rate is too small. I'm gonna end
小碎步 所以如果我的学习速率太小 结果就是

69
00:06:25,342 --> 00:06:30,589
up, you know, taking these tiny, tiny baby
steps to try to get to the minimum and I'm
只能这样像小宝宝一样一点点地挪动 去努力接近最低点

70
00:06:30,589 --> 00:06:35,837
gonna need a lot of steps to get to the
minimum and so. If alpha's too small, can
这样就需要很多步才能到达最低点 所以如果α 太小的话

71
00:06:35,837 --> 00:06:41,019
be slow because it's gonna take these
tiny, tiny baby steps. And it's gonna need
可能会很慢 因为它会一点点挪动 它会需要

72
00:06:41,019 --> 00:06:45,829
a lot of steps before it gets anyway
close to the global minimum. Now,
很多步才能到达全局最低点

73
00:06:45,829 --> 00:06:52,236
how about if the alpha is too large.
So here's my function J of theta.
那么如果α 太大又会怎样呢 这是我的函数J(θ)

74
00:06:52,236 --> 00:06:57,590
Turns out if alpha is too large, then
grading descent can overshoot a minimum
如果α 太大 那么梯度下降法可能会越过最低点

75
00:06:57,590 --> 00:07:03,362
and may even fail to converge or even diverge. So here is what I mean. Let's say [inaudible]
ireful minimum So the derivative council
甚至可能无法收敛 我的意思是 比如我们从这个点开始

76
00:07:03,362 --> 00:07:08,647
It's actually close to the minimum. So the derivative points to the right, but if alpha is too big, I'm gonna
实际上这个点已经接近最低点  因此导数指向右侧 但如果α 太大的话

77
00:07:08,686 --> 00:07:14,140
take a huge step. Maybe I'm gonna take a huge step like that. Right? So I end up taking a huge step.
我会迈出很大一步 也许像这样巨大的一步 对吧？所以我最终迈出了一大步

78
00:07:14,140 --> 00:07:20,051
Now, my cost function has got worse. cause it starts off from this value then now my value has gotten worse. Now my
现在 我的代价函数变得更糟 因为离这个最低点越来越远

79
00:07:20,051 --> 00:07:25,190
derivatives, you know, points to the left, it's actually decrease theta. But look, if my learning rate is too big,
现在我的导数指向左侧 实际上在减小θ 但是你看 如果我的学习速率过大

80
00:07:25,190 --> 00:07:29,792
I may take a huge step going from here all
the way out there so I end up. going all
我会移动一大步 从这点一下子又到那点了

81
00:07:29,792 --> 00:07:35,372
there. Right? And if my learning rate was too
big I can take another huge step on the
对吗？如果我的学习率太大 下一次迭代

82
00:07:35,372 --> 00:07:41,034
next acceleration and kind of overshoot
and overshoot and so on until you notice
又移动了一大步 越过一次 又越过一次 一次次越过最低点 直到你发现

83
00:07:41,034 --> 00:07:46,765
I'm actually getting further and further
away from the minimum. And so if alpha is
实际上 离最低点越来越远 所以 如果α太大

84
00:07:46,765 --> 00:07:51,905
too large it can fail to converge or even
diverge. Now, I have another question for
它会导致无法收敛 甚至发散 现在 我还有一个问题

85
00:07:51,905 --> 00:07:56,057
you. So, this is a tricky one. And when I
was first learning this stuff, it actually
这问题挺狡猾的 当我第一次学习这个地方时

86
00:07:56,057 --> 00:08:00,005
took me a long time to figure this out.
What if your pre-emptive theta one is
我花了很长一段时间才理解这个问题 如果我们预先把θ1

87
00:08:00,005 --> 00:08:04,106
already at a local minimum? What do you
think one step of gradient descent will
放在一个局部的最低点 你认为下一步梯度下降法会怎样工作？

88
00:08:04,106 --> 00:08:10,857
do? So let's suppose you initialize theta
one at a local minimum. So you know
所以假设你将θ1初始化在局部最低点

89
00:08:10,857 --> 00:08:16,713
suppose this is your initial value of theta one
over here and it's already at a local
假设这是你的θ1的初始值 在这儿 它已经在一个局部的

90
00:08:16,713 --> 00:08:22,718
optimum or the local minimum. It sends
out that at local optimum your derivative
最优处或局部最低点 结果是局部最优点的导数

91
00:08:22,718 --> 00:08:28,796
would be equal to zero. Since it's that
slope where it's that tangent point so the
将等于零 因为它是那条切线的斜率

92
00:08:28,796 --> 00:08:35,528
slope of this line will be equal to zero
and thus this derivative term. Is equal to
而这条线的斜率将等于零 因此 此导数项等于0

93
00:08:35,528 --> 00:08:40,941
zero. And so, in your gradient descent
update, you have theta one, gives update
因此 在你的梯度下降更新过程中 你有一个θ1

94
00:08:40,941 --> 00:08:46,284
that theta one, minus alpha times zero.
And so, what this means is that, if you're
然后用θ1 减α 乘以0来更新θ1  所以这意味着什么

95
00:08:46,284 --> 00:08:51,222
already at a local optimum, it leaves
theta one unchanged 'cause this, you know,
这意味着你已经在局部最优点 它使得θ1不再改变

96
00:08:51,222 --> 00:08:56,132
gets the update's theta one equals theta one.
So if your parameter is already at a local
也就是新的θ1等于原来的θ1  因此 如果你的参数已经处于

97
00:08:56,132 --> 00:09:00,694
minimum, one step of gradient descent
does absolutely nothing. It doesn't change
局部最低点 那么梯度下降法更新其实什么都没做 它不会改变参数的值

98
00:09:00,694 --> 00:09:05,257
the parameter, which is, which is what you
want. Cuz it keeps your solution at the
这也正是你想要的 因为它使你的解始终保持在

99
00:09:05,257 --> 00:09:09,706
local optimum. This also explains why
gradient descent can converge the local
局部最优点 这也解释了为什么即使学习速率α 保持不变时

100
00:09:09,706 --> 00:09:14,326
minimum, even with the learning rate Alpha
fixed. Here's what I mean by that. Let's
梯度下降也可以收敛到局部最低点 我想说的是这个意思

101
00:09:14,326 --> 00:09:21,550
look at an example. So here's a cost
function J with theta. That maybe I want
我们来看一个例子 这是代价函数J(θ)

102
00:09:21,550 --> 00:09:26,811
to minimize and let's say I initialize my
algorithm my gradient descent algorithm, you know,
我想找到它的最小值 首先初始化我的梯度下降算法

103
00:09:26,811 --> 00:09:32,080
out there at that magenta point. If I take
one step of gradient descent you know,
在那个品红色的点初始化 如果我更新一步梯度下降

104
00:09:32,080 --> 00:09:36,941
maybe it'll take me to that point cuz my
derivative's pretty steep out there, right?
也许它会带我到这个点 因为这个点的导数是相当陡的

105
00:09:36,941 --> 00:09:42,051
Now I'm at this green point and if I take
another step of gradient descent, you
现在 在这个绿色的点 如果我再更新一步

106
00:09:42,051 --> 00:09:47,036
notice that my derivative, meaning the
slope, is less steep at the green point when
你会发现我的导数 也即斜率 是没那么陡的

107
00:09:47,036 --> 00:09:51,959
compared to at the magenta point out
there, right? Because as I approach the
相比于在品红点 对吧？因为随着我接近最低点

108
00:09:51,959 --> 00:09:56,883
minimum my derivative gets closer and
closer to zero as I approach the minimum.
我的导数越来越接近零

109
00:09:56,883 --> 00:10:01,794
So, after one step of gradient descent,
my new derivative is a little bit smaller.
所以 梯度下降一步后 新的导数会变小一点点

110
00:10:01,794 --> 00:10:06,635
So I wanna take another step of gradient
descent. I will naturally take a somewhat
然后我想再梯度下降一步 在这个绿点我自然会用一个稍微

111
00:10:06,635 --> 00:10:11,598
smaller step from this green point than I
did from the magenta point. Now I'm at the new
跟刚才在那个品红点时比 再小一点的一步

112
00:10:11,598 --> 00:10:16,038
point, the red point, and then now even
closer to global minimums, so the
现在到了新的点 红色点 更接近全局最低点了

113
00:10:16,038 --> 00:10:21,229
derivative here will be even smaller than
it was at the green point. So when I take
因此这点的导数会比在绿点时更小 所以

114
00:10:21,229 --> 00:10:26,420
another step of gradient descent, you know, now
my derivative term is even smaller, and so
我再进行一步梯度下降时 我的导数项是更小的

115
00:10:26,420 --> 00:10:31,360
the magnitude of the update to theta
one is even smaller, so you can take
θ1更新的幅度就会更小

116
00:10:31,360 --> 00:10:39,145
small step like so, and as gradient descent
runs. You will automatically take smaller
所以你会移动更小的一步 像这样 随着梯度下降法的运行

117
00:10:39,145 --> 00:10:46,343
and smaller steps until eventually you are
taking very small steps, you know, and you
你移动的幅度会自动变得越来越小 直到最终移动幅度非常小

118
00:10:46,343 --> 00:10:52,737
find the converge to the to the local
minimum. So, just to recap. In gradient
你会发现 已经收敛到局部极小值 所以回顾一下

119
00:10:52,737 --> 00:10:57,716
descent as we approach the local minimum,
grading descent will automatically take
在梯度下降法中 当我们接近局部最低点时 梯度下降法会自动采取

120
00:10:57,716 --> 00:11:02,634
smaller steps and that's because as we
approach the local minimum, by definition,
更小的幅度 这是因为当我们接近局部最低点时

121
00:11:02,634 --> 00:11:07,122
local minimum is when you have this
derivative equal to zero. So as we
很显然在局部最低时导数等于零 所以当我们

122
00:11:07,122 --> 00:11:12,408
approach the local minimum this derivative
term will automatically get smaller and
接近局部最低时 导数值会自动变得越来越小

123
00:11:12,408 --> 00:11:16,957
so gradient descent will automatically
take smaller step. So, this is what
所以梯度下降将自动采取较小的幅度

124
00:11:16,957 --> 00:11:21,506
gradient descent looks like, and so actually
there is no need to decrease alpha
这就是梯度下降的做法 所以实际上没有必要再另外减小α

125
00:11:21,506 --> 00:11:26,258
overtime. So, that's the gradient descent
algorithm, and you can use it to minimize,
这就是梯度下降算法 你可以用它来最小化

126
00:11:26,258 --> 00:11:30,713
to try to minimize any cost function J.
Not the cost function J to be defined for
最小化任何代价函数J 不只是线性回归中的代价函数J

127
00:11:30,713 --> 00:11:34,738
linear regression. In the next video,
we're going to take the function J, and
在接下来的视频中 我们要用代价函数J

128
00:11:34,738 --> 00:11:38,549
set that back to be exactly linear
regression's cost function. The, the
回到它的本质 线性回归中的代价函数

129
00:11:38,549 --> 00:11:43,057
square cost function that we came up with
earlier. And taking gradient descent, and
也就是我们前面得出的平方误差函数 结合梯度下降法

130
00:11:43,057 --> 00:11:47,351
the square cost function, and putting
them together. That will give us our first
以及平方代价函数 我们会得出第一个机器学习算法

131
00:11:47,351 --> 00:11:50,948
learning algorithm, that'll give us our
linear regression algorithm.
即线性回归算法
【果壳教育无边界字幕组】翻译：10号少年  校对：Femtoyue  审核：所罗门捷列夫

