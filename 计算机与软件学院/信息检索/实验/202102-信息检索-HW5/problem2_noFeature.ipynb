{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from numpy import *\n",
    "from functools import reduce\n",
    "import re\n",
    "import os\n",
    "import random\n",
    "import jieba\n",
    "import math\n",
    "import operator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# 保留中文内容\n",
    "# 爬虫下来的文档中会有冗余信息\n",
    "def find_chinese(file):\n",
    "    pattern = re.compile(r'[^\\u4e00-\\u9fa5]')\n",
    "    chinese = re.sub(pattern, '', file)\n",
    "    return chinese"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# 准备数据：从文本中构建词向量\n",
    "def loadDataSet2():\n",
    "    # 0 --> 党政办公室\n",
    "    # 1 --> 教务部\n",
    "    # 2 --> 招生办公室\n",
    "    # 3 --> 研究生院\n",
    "    # 4 --> 科学技术部\n",
    "    postingList = []\n",
    "    classVec = []\n",
    "    for file in os.listdir(f'data/'):\n",
    "        text = open(f'data/{file}', 'r').read()\n",
    "        text = find_chinese(text)\n",
    "        \n",
    "        text =  list(jieba.cut(text, cut_all=False))\n",
    "        postingList.append(text)\n",
    "\n",
    "        if file.startswith('党政办公室'):\n",
    "            classVec.append(0)\n",
    "        elif file.startswith('教务部'):\n",
    "            classVec.append(1)\n",
    "        elif file.startswith('招生办公室'):\n",
    "            classVec.append(2)\n",
    "        elif file.startswith('研究生院'):\n",
    "            classVec.append(3)\n",
    "        elif file.startswith('科学技术部'):\n",
    "            classVec.append(4)\n",
    "            \n",
    "    return postingList, classVec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# 准备数据：从文本中构建词向量\n",
    "def loadDataSet(goal):\n",
    "    # 0 --> 党政办公室\n",
    "    # 1 --> 教务部\n",
    "    # 2 --> 招生办公室\n",
    "    # 3 --> 研究生院\n",
    "    # 4 --> 科学技术部\n",
    "    postingList = []\n",
    "    classVec = []\n",
    "    for file in os.listdir(f'data1/{goal}'):\n",
    "        text = open(f'/Users/alex_shen/SynologyDrive/PcBackup/深圳大学/课程/大三下/信息检索/实验/202102-信息检索-HW5/data1/{goal}/{file}', 'r').read()\n",
    "        text = find_chinese(text)\n",
    "        \n",
    "        text =  list(jieba.cut(text, cut_all=False))\n",
    "        postingList.append(text)\n",
    "\n",
    "        if file.startswith('党政办公室'):\n",
    "            classVec.append(0)\n",
    "        elif file.startswith('教务部'):\n",
    "            classVec.append(1)\n",
    "        elif file.startswith('招生办公室'):\n",
    "            classVec.append(2)\n",
    "        elif file.startswith('研究生院'):\n",
    "            classVec.append(3)\n",
    "        elif file.startswith('科学技术部'):\n",
    "            classVec.append(4)\n",
    "            \n",
    "    return postingList, classVec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# 构建一个包含所有文档词汇且不重复的词汇表\n",
    "def createVocabList(dataSet):\n",
    "    vocabSet = set([])  # 创建一个空集\n",
    "    for document in dataSet:\n",
    "        vocabSet = vocabSet | set(document)  # 创建两个集合的并集\n",
    "    return list(vocabSet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def concatenateTextOfAllDocsInClass(postingList, classVec, c):\n",
    "    text_c = []\n",
    "    for index, cat in enumerate(classVec):\n",
    "        if cat == c:\n",
    "            text_c.extend(postingList[index])\n",
    "    return text_c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def trainNB(myVocabList, postingList, classVec):\n",
    "    V = myVocabList\n",
    "    N = len(postingList)\n",
    "    condprob = {term: [0] * 5 for term in V}\n",
    "    prior = [0] * 5\n",
    "    for c in range(5):\n",
    "        # 计算先验概率\n",
    "        Nc = classVec.count(c)\n",
    "        prior[c] = Nc / N\n",
    "        # 拼接所有文档的词汇\n",
    "        text_c = concatenateTextOfAllDocsInClass(postingList, classVec, c)\n",
    "        # 计算条件概率\n",
    "        for t in V:\n",
    "            T_ct = text_c.count(t)\n",
    "            condprob[t][c] = (T_ct + 1) / (len(text_c) + len(V))\n",
    "    return V, prior, condprob\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# 计算最大概率类别\n",
    "def applyMultinomialNB(V, prior, condprob, text):\n",
    "    p = []\n",
    "    for c in range(5):\n",
    "        p.append(math.log(prior[c], 2))\n",
    "        for t in text:\n",
    "            if t in V:\n",
    "                p[c] += math.log(condprob[t][c], 2)\n",
    "    return p.index(max(p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Dumping model to file cache /var/folders/m0/dsy0kdw13rb2g65q1x0bqn_00000gn/T/jieba.cache\n",
      "Loading model cost 0.344 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "整体 正确率： 0.86\n",
      "党政办公室 正确率： 0.9\n",
      "教务部 正确率： 0.5\n",
      "招生办公室 正确率： 1.0\n",
      "研究生院 正确率： 1.0\n",
      "科学技术部 正确率： 0.9\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    # # 随机取80%作为训练集，20%作为测试集\n",
    "    # postingList, classVec = loadDataSet2()\n",
    "    # c = list(zip(postingList, classVec))\n",
    "    # random.shuffle(c)\n",
    "    # postingList, classVec = zip(*c)\n",
    "    # train_postingList, train_classVec = list(postingList[:int(len(postingList) * 0.8)]), list(classVec[:int(len(classVec) * 0.8)])\n",
    "    # test_postingList, test_classVec = list(postingList[int(len(postingList) * 0.8):]), list(classVec[int(len(classVec) * 0.8):])\n",
    "    # myVocabList = createVocabList(train_postingList)\n",
    "    # V, prior, condprob = trainNB(myVocabList, train_postingList, train_classVec)\n",
    "    \n",
    "    # 每类文档取20个作为训练集，10个作为测试集\n",
    "    train_postingList, train_classVec = loadDataSet('train')\n",
    "    myVocabList = createVocabList(train_postingList) \n",
    "    \n",
    "    test_postingList, test_classVec = loadDataSet('test')\n",
    "    V, prior, condprob = trainNB(myVocabList, train_postingList, train_classVec)\n",
    "    \n",
    "    correct = [0] * 5\n",
    "    for index, text in enumerate(test_postingList):\n",
    "        res = applyMultinomialNB(V, prior, condprob, text)\n",
    "        if res == test_classVec[index]:\n",
    "            correct[test_classVec[index]] += 1\n",
    "    \n",
    "    # 0 --> 党政办公室\n",
    "    # 1 --> 教务部\n",
    "    # 2 --> 招生办公室\n",
    "    # 3 --> 研究生院\n",
    "    # 4 --> 科学技术部\n",
    "    print('整体 正确率：', sum(correct) / len(test_postingList))\n",
    "    print('党政办公室 正确率：', correct[0] / test_classVec.count(0))\n",
    "    print('教务部 正确率：', correct[1] / test_classVec.count(1))\n",
    "    print('招生办公室 正确率：', correct[2] / test_classVec.count(2))\n",
    "    print('研究生院 正确率：', correct[3] / test_classVec.count(3))\n",
    "    print('科学技术部 正确率：', correct[4] / test_classVec.count(4))\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "2b38f256b5e5cfcb08c8fab1bb2c0ed1d264b1b0baf1af9aa926f047321c09a2"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}