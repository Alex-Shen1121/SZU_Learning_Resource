1
00:00:00,332 --> 00:00:04,284
In the next few videos, we'll talk about large scale machine learning.
在接下来的几个视频，我们将谈论大规模机器学习。（字幕翻译：中国海洋大学 黄海广 haiguang2000@qq.com）

2
00:00:04,284 --> 00:00:08,316
That is, algorithms but viewing with big data sets.
也就是大数据集的算法。

3
00:00:08,316 --> 00:00:12,839
If you look back at a recent 5 or 10-year history of machine learning.
如果你看看最近5年或10年机器学习的历史。

4
00:00:12,839 --> 00:00:17,853
One of the reasons that learning algorithms work so much better now than even say, 5-years ago,
一个原因就是：现在的一个学习算法比5年前更好，

5
00:00:17,853 --> 00:00:22,657
is just the sheer amount of data that we have now and that we can train our algorithms on.
只是说我们现在，我们在数据量极其庞大可以训练我们的算法。

6
00:00:22,657 --> 00:00:29,741
In these next few videos, we'll talk about algorithms for dealing when we have such massive data sets.
在接下来的视频，我们将谈论时，当我们有如此庞大的数据集的算法处理。

7
00:00:32,926 --> 00:00:35,527
So why do we want to use such large data sets?
我们为什么要用这么大的数据集？

8
00:00:35,527 --> 00:00:40,564
We've already seen that one of the best ways to get a high performance machine learning system,
我们已经看到了一个得到一个高性能的机器学习系统的最佳途径，

9
00:00:40,564 --> 00:00:46,168
is if you take a low-bias learning algorithm, and train that on a lot of data.
如果你采取低偏差的学习算法，并对大量数据进行训练。

10
00:00:46,168 --> 00:00:53,561
And so, one early example we have already seen was this example of classifying between confusable words.
一个早期的例子，我们已经看到了一个分类容易混淆的词的例子。

11
00:00:53,561 --> 00:01:00,726
So, for breakfast, I ate two (TWO) eggs and we saw in this example, these sorts of results,
上面的例子分类的结果，早餐我吃了（TWO）个鸡蛋

12
00:01:00,726 --> 00:01:06,436
where, you know, so long as you feed the algorithm a lot of data, it seems to do very well.
如你所知，只要你给的算法很多数据，它似乎做的很好。

13
00:01:06,436 --> 00:01:10,419
And so it's results like these that has led to the saying in machine learning that
所以这样的结果表明，在机器学习中，

14
00:01:10,419 --> 00:01:15,151
often it's not who has the best algorithm that wins. It's who has the most data.
决定因素往往不是那些拥有最好的算法，而是这是谁的数据多。

15
00:01:15,151 --> 00:01:19,568
So you want to learn from large data sets, at least when we can get such large data sets.
所以你想从大型数据集的学习，至少我们得获得这样的大型数据集。

16
00:01:19,568 --> 00:01:27,027
But learning with large data sets comes with its own unique problems, specifically, computational problems.
但大型数据集的学习都有自己独特的问题，特别是计算问题。

17
00:01:27,027 --> 00:01:33,870
Let's say your training set size is M equals 100,000,000.
我们说你的训练集的大小为M = 100000000。

18
00:01:33,870 --> 00:01:37,934
And this is actually pretty realistic for many modern data sets.
其实这是很现实的许多现代数据集。

19
00:01:37,934 --> 00:01:40,518
If you look at the US Census data set, if there are, you know,
如果你看看美国人口普查数据，如果有的话，你知道的，

20
00:01:40,518 --> 00:01:44,663
300 million people in the US, you can usually get hundreds of millions of records.
在美国有三亿人口，你通常可以得到成千上万的记录。

21
00:01:44,663 --> 00:01:47,856
If you look at the amount of traffic that popular websites get,
如果你看流行的网站的流量数据，

22
00:01:47,856 --> 00:01:52,509
you easily get training sets that are much larger than hundreds of millions of examples.
你轻易得到的训练集，一亿数据还大得多。

23
00:01:52,509 --> 00:01:57,407
And let's say you want to train a linear regression model, or maybe a logistic regression model,
让我们说你想训练一个线性回归模型，或者一个Logistic回归模型，

24
00:01:57,407 --> 00:02:01,692
in which case this is the gradient descent rule.
在这种情况下，这是梯度下降规则。

25
00:02:01,692 --> 00:02:05,372
And if you look at what you need to do to compute the gradient,
如果你看看你需要做什么来计算梯度，

26
00:02:05,372 --> 00:02:09,992
which is this term over here, then when M is a hundred million,
这个词在这里是，当m是一亿，

27
00:02:09,992 --> 00:02:13,976
you need to carry out a summation over a hundred million terms,
你需要进行求和一亿条，

28
00:02:13,976 --> 00:02:18,977
in order to compute these derivatives terms and to perform a single step of decent.
为了计算这些导数和单步下降。

29
00:02:18,977 --> 00:02:25,627
Because of the computational expense of summing over a hundred million entries
因为总的超过一亿次的计算

30
00:02:25,627 --> 00:02:28,628
in order to compute just one step of gradient descent,
为了计算只是一步梯度下降，

31
00:02:28,628 --> 00:02:31,530
in the next few videos we've spoken about techniques
在接下来的几个视频我们已经谈论

32
00:02:31,530 --> 00:02:38,413
for either replacing this with something else or to find more efficient ways to compute this derivative.
或者更换这东西或找到更有效的方法来计算这个导数。

33
00:02:38,413 --> 00:02:41,709
By the end of this sequence of videos on large scale machine learning,
通过观看大规模机器学习的视频，

34
00:02:41,709 --> 00:02:47,045
you know how to fit models, linear regression, logistic regression, neural networks and so on
你知道如何适应模型，线性回归，logistic回归，神经网络等

35
00:02:47,045 --> 00:02:50,990
even today's data sets with, say, a hundred million examples.
甚至说，今天的数据集达到一亿例。

36
00:02:50,990 --> 00:02:56,035
Of course, before we put in the effort into training a model with a hundred million examples,
当然，在我们尝试把一亿个样本放入训练模型，

37
00:02:56,035 --> 00:03:01,276
We should also ask ourselves, well, why not use just a thousand examples.
我们也应该问问自己，为什么不使用一千个样本？

38
00:03:01,276 --> 00:03:04,923
Maybe we can randomly pick the subsets of a thousand examples
也许我们可以随机选择一千个样本的子集，

39
00:03:04,923 --> 00:03:10,254
out of a hundred million examples and train our algorithm on just a thousand examples.
通过一亿个样本得到的训练算法用于一千个样本。

40
00:03:10,254 --> 00:03:16,076
So before investing the effort into actually developing and the software needed to train these massive models
所以投资前，努力发展和实际需要训练这些大规模模型的软件

41
00:03:16,076 --> 00:03:22,461
is often a good sanity check, if training on just a thousand examples might do just as well.
往往是一个很好的检查，如果在一千个例子训练可能会做的一样。

42
00:03:22,461 --> 00:03:29,731
The way to sanity check of using a much smaller training set might do just as well,
的使用一个非常小的训练集的完整性检查的方式可能会做的一样，

43
00:03:29,731 --> 00:03:33,958
that is if using a much smaller n equals 1000 size training set,
如果使用一个非常小的训练集，n等于1000，

44
00:03:33,958 --> 00:03:37,797
that might do just as well, it is the usual method of plotting the learning curves,
会做的一样好，它是通常学习曲线的绘制方法，

45
00:03:37,797 --> 00:03:46,872
so if you were to plot the learning curves and if your training objective were to look like this,
如果你要画的学习曲线，如果你的训练目的是看起来像这样，

46
00:03:46,872 --> 00:03:49,553
that's J train theta.
这是Jtrain（θ），

47
00:03:49,553 --> 00:03:56,422
And if your cross-validation set objective, Jcv of theta would look like this,
如果你的交叉验证，设定目标，JCV（θ）看起来像这样

48
00:03:56,422 --> 00:04:00,310
then this looks like a high-variance learning algorithm,
这看起来像一个高方差学习算法，

49
00:04:00,310 --> 00:04:05,913
and we will be more confident that adding extra training examples would improve performance.
我们会更有信心，增加额外的培训例子会提高性能。

50
00:04:05,913 --> 00:04:10,462
Whereas in contrast if you were to plot the learning curves,
而相反如果你策划的学习曲线，

51
00:04:10,462 --> 00:04:20,339
if your training objective were to look like this, and if your cross-validation objective were to look like that,
如果你训练的目的是这个样子的，如果你的交叉验证的目的是那个样子，

52
00:04:20,339 --> 00:04:24,292
then this looks like the classical high-bias learning algorithm.
然后像经典的高偏差学习算法。

53
00:04:24,292 --> 00:04:28,084
And in the latter case, you know, if you were to plot this up to,
在后一种情况下，你知道，如果你这样画，

54
00:04:28,084 --> 00:04:33,437
say, m equals 1000 and so that is m equals 500 up to m equals 1000,
M等于1000，所以这是m等于500到m等于1000，

55
00:04:33,437 --> 00:04:39,400
then it seems unlikely that increasing m to a hundred million will do much better
看起来不太可能增加m到一亿会做得更好

56
00:04:39,400 --> 00:04:42,736
and then you'd be just fine sticking to n equals 1000,
然后你得坚持n等于1000，

57
00:04:42,736 --> 00:04:47,000
rather than investing a lot of effort to figure out how the scale of the algorithm.
而不是投入了很多努力弄清如何算法的尺度。

58
00:04:47,000 --> 00:04:51,029
Of course, if you were in the situation shown by the figure on the right,
当然，如果你在右图这种情况，

59
00:04:51,029 --> 00:04:53,885
then one natural thing to do would be to add extra features,
自然而然将会添加额外的特征，

60
00:04:53,885 --> 00:04:58,484
or add extra hidden units to your neural network and so on,
或在你的神经网络添加额外的隐藏单元，等等，

61
00:04:58,484 --> 00:05:04,627
so that you end up with a situation closer to that on the left, where maybe this is up to n equals 1000,
你最终接近左侧的情况，而这也许是到n等于1000，

62
00:05:04,627 --> 00:05:09,553
and this then gives you more confidence that trying to add infrastructure to change the algorithm
并给你更多的信心，努力把基础设施改变算法

63
00:05:09,553 --> 00:05:14,735
to use much more than a thousand examples that might actually be a good use of your time.
使用超过一千的样本，实际上可能是很好的利用你的时间。

64
00:05:14,735 --> 00:05:19,642
So in large-scale machine learning, we like to come up with computationally reasonable ways,
如此大规模的机器学习，我们要拿出合理的计算方法，

65
00:05:19,642 --> 00:05:24,026
or computationally efficient ways, to deal with very big data sets.
或计算的有效方法，处理非常大的数据集。

66
00:05:24,026 --> 00:05:26,826
In the next few videos, we'll see two main ideas.
在接下来的几个视频，我们会看到两个主要观点。

67
00:05:26,826 --> 00:05:33,464
The first is called stochastic gradient descent and the second is called Map Reduce, for viewing with very big data sets.
第一种称为随机梯度下降，第二是图的减少，观察的非常大的数据集。

68
00:05:33,464 --> 00:05:39,986
And after you've learned about these methods, hopefully that will allow you to scale up your learning algorithms to big data
当你了解了这些方法，希望这将允许你扩大你的学习算法在大数据

69
00:05:39,986 --> 00:05:43,986
and allow you to get much better performance on many different applications.
让你在许多不同的应用得到更好的性能。

