1
00:00:00,000 --> 00:00:04,620
In this video I am going to define what is
probably the most common type of machine
本视频中，我将定义机器学习问题的最普通类型大概是什么，
(字幕整理：中国海洋大学 黄海广,haiguang2000@qq.com )

2
00:00:04,620 --> 00:00:08,910
learning problem, which is supervised
learning. I'll define supervised learning
哪些是监督学习。随后，我将给出监督学习的更正式的定义，

3
00:00:08,910 --> 00:00:13,255
more formally later, but it's probably
best to explain or start with an example
但是最好还是从例子开始解释。

4
00:00:13,255 --> 00:00:17,820
of what it is and we'll do the formal
definition later. Let's say you want to
假设你想要预测房屋价格。

5
00:00:17,820 --> 00:00:23,072
predict housing prices. A while back, a
student collected data sets from the
不久前，一个学生从波特兰俄勒冈研究所收集数据。

6
00:00:23,072 --> 00:00:28,745
Institute of Portland Oregon. And let's
 say you plot a data set and it looks like
假设你绘制了一个数据集，就像这样。

7
00:00:28,745 --> 00:00:34,347
this. Here on the horizontal axis, the
size of different houses in square feet,
水平轴上，不同房屋的尺寸是平方英尺

8
00:00:34,347 --> 00:00:39,879
and on the vertical axis, the price of
 different houses in thousands of dollars.
垂直轴上，是不同房子的价格，千万美元。

9
00:00:39,879 --> 00:00:45,168
So. Given this data, let's say you have a
friend who owns a house that is, say 750
给定数据，假设有一个朋友有一栋房子，750平方英尺，

10
00:00:45,168 --> 00:00:50,708
square feet and hoping to sell the house
and they want to know how much they can
想要卖掉这个房子，他们想知道能卖多少钱。

11
00:00:50,708 --> 00:00:56,116
get for the house. So how can the learning
 algorithm help you? One thing a learning
那么，学习算法能帮你什么呢？

12
00:00:56,116 --> 00:01:01,524
algorithm might be able to do is put a
 straight line through the data or to fit a
学习算法可做的一件事可能是根据数据画一条直线或者说

13
00:01:01,524 --> 00:01:07,111
straight line to the data and, based on
that, it looks like maybe the house can be
用一条直线拟合数据。基于此，看上去房子可以卖大约150000美元。

14
00:01:07,111 --> 00:01:13,239
sold for maybe about $150,000. But maybe this
isn't the only learning algorithm you can
但是，可能这不是你能使用的唯一的学习算法。

15
00:01:13,239 --> 00:01:18,536
use. There might be a better one. For
example, instead of sending a straight
可能有一个更好的。例如，不是用一条直线拟合数据，

16
00:01:18,536 --> 00:01:23,620
line to the data, we might decide that
it's better to fit a quadratic
可能觉得用一个二次函数或二阶多项式来拟合数据更好。

17
00:01:23,620 --> 00:01:29,110
function or a second-order polynomial to
this data. And if you do that, and make a
假设这么做，

18
00:01:29,110 --> 00:01:34,667
prediction here, then it looks like, well,
maybe we can sell the house for closer to
在这儿做一个预测，然后看上去我们可以以接近200000美元的价格卖掉这个房子。

19
00:01:34,667 --> 00:01:39,184
$200,000. One of the things we'll talk
about later is how to choose and how to
后面我们要讨论的一件事就是如何选择，如何

20
00:01:39,184 --> 00:01:43,792
decide do you want to fit a straight line
to the data or do you want to fit the
决定你是用直线拟合数据还是用二次函数拟合数据。

21
00:01:43,792 --> 00:01:48,631
quadratic function to the data and there's
no fair picking whichever one gives your
当然，无论取哪个模型给你的朋友卖房子都是不公平的。

22
00:01:48,631 --> 00:01:53,182
friend the better house to sell. But each
 of these would be a fine example of a
但这是一个学习算法的很好的例子。

23
00:01:53,182 --> 00:01:57,834
learning algorithm. So this is an example
 of a supervised learning algorithm. And
这也是监督学习算法的例子。

24
00:01:57,834 --> 00:02:03,736
the term supervised learning refers to the
 fact that we gave the algorithm a data set
监督学习是指我们给算法一个数据集，并且给定正确答案。

25
00:02:03,736 --> 00:02:09,089
in which the "right answers" were
given. That is, we gave it a data set of
也就是说，我们给定一个房屋数据集，

26
00:02:09,089 --> 00:02:14,580
houses in which for every example in this
 data set, we told it what is the right
在这个数据集中的每个例子，我们都给出正确的价格，

27
00:02:14,580 --> 00:02:20,002
price so what is the actual price that,
that house sold for and the toss of the
也即这个房子卖出的实际价格，

28
00:02:20,002 --> 00:02:25,423
algorithm was to just produce more of
these right answers such as for this new
算法的目的就是给出更多的正确答案，例如对待售房子，

29
00:02:25,423 --> 00:02:30,579
house, you know, that your friend may be
trying to sell. To define with a bit more
比如你朋友想要卖的房子给出估价。

30
00:02:30,579 --> 00:02:35,257
terminology this is also called a
regression problem and by regression
用更专业的术语来定义，它也称为回归问题，

31
00:02:35,257 --> 00:02:40,467
problem I mean we're trying to predict a
continuous value output. Namely the price.
之所以称之为回归问题，我的意思就是预测连续的输出值，也就是价格。

32
00:02:40,467 --> 00:02:44,720
So technically I guess prices can be
rounded off to the nearest cent. So maybe
技术上而言，假设价格被圆整到分。

33
00:02:44,720 --> 00:02:49,246
prices are actually discrete values, but
usually we think of the price of a house
因此，价格实际上是一个离散值，但通常我们认为房子的价格是一个实数，

34
00:02:49,246 --> 00:02:53,608
as a real number, as a scalar value, as
a continuous value number and the term
作为一个标量，作为一个连续值，

35
00:02:53,608 --> 00:02:58,080
regression refers to the fact that we're
 trying to predict the sort of continuous
回归指我们设法预测连续值的属性。

36
00:02:58,080 --> 00:03:02,060
values attribute. Here's another
supervised learning example, some friends
这里是另一个监督学习的例子，我和一些朋友

37
00:03:02,060 --> 00:03:06,427
and I were actually working on this
 earlier. Let's see you want to look at
早些时候做过这个问题。假设你想要看医疗记录，

38
00:03:06,427 --> 00:03:11,675
medical records and try to predict of a
breast cancer as malignant or benign. If
并且想设法预测乳腺癌是恶性的还是良性的。

39
00:03:11,675 --> 00:03:16,856
someone discovers a breast tumor, a lump
in their breast, a malignant tumor is a
假设某人发现了一个乳腺瘤，在乳腺上有个肿块，一个恶性瘤就是

40
00:03:16,856 --> 00:03:22,300
tumor that is harmful and dangerous and a
benign tumor is a tumor that is harmless.
它是有害的且危险的；良性瘤就是它是无害的。

41
00:03:22,300 --> 00:03:27,876
So obviously people care a lot about this.
Let's see a collected data set and suppose
显然，人们很关心这个。我们来看收集到的数据集，

42
00:03:27,876 --> 00:03:33,164
in your data set you have on your
horizontal axis the size of the tumor and
假设，在你的数据集中，水平轴是瘤的尺寸，

43
00:03:33,164 --> 00:03:39,317
on the vertical axis I'm going to plot one
or zero, yes or no, whether or not these are
垂直轴，可以是1或0，也可以是Y或N。

44
00:03:39,317 --> 00:03:45,184
examples of tumors we've seen before are
malignant–which is one–or zero if not malignant
设已经看到的肿瘤例子是恶性的对应1，良性的对应0。

45
00:03:45,184 --> 00:03:50,392
or benign. So let's say our data set looks
like this where we saw a tumor of this
那么，你的数据集就像这样，我们看到这些尺寸的瘤是良性的，

46
00:03:50,392 --> 00:03:56,283
size that turned out to be benign. One of
 this size, one of this size. And so on.
这个尺寸，这个尺寸，等等。

47
00:03:56,283 --> 00:04:02,227
And sadly we also saw a few malignant
 tumors, one of that size, one of that
很遗憾，我们也看到了一些恶性的瘤，这个尺寸，这个尺寸，

48
00:04:02,227 --> 00:04:08,572
size, one of that size... So on. So this
 example... I have five examples of benign
这个尺寸，等等。因此，这个例子，这里我给出了五个良性肿瘤的例子，

49
00:04:08,572 --> 00:04:15,159
tumors shown down here, and five examples
of malignant tumors shown with a vertical
五个恶性肿瘤的例子，恶性瘤对应垂直轴上的1.

50
00:04:15,159 --> 00:04:21,504
axis value of one. And let's say we have
a friend who tragically has a breast
假设我们有个朋友，很不幸，有乳腺癌，

51
00:04:21,504 --> 00:04:28,097
tumor, and let's say her breast tumor size
is maybe somewhere around this value. The
假设她的乳腺瘤的尺寸可能是这个值附近。

52
00:04:28,097 --> 00:04:32,930
machine learning question is, can you
 estimate what is the probability, what is
机器学习问题就是，你能否估计出瘤是良性的还是恶性的概率。

53
00:04:32,930 --> 00:04:37,819
the chance that a tumor is malignant
versus benign? To introduce a bit more
引入一个更专业的术语，

54
00:04:37,819 --> 00:04:42,719
terminology this is an example of a
classification problem. The term
这就是一个分类问题。

55
00:04:42,719 --> 00:04:47,342
classification refers to the fact that
here we're trying to predict a discrete
分类是指我们设法预测一个离散的输出值，

56
00:04:47,342 --> 00:04:52,321
value output: zero or one, malignant or
benign. And it turns out that in
0或1，恶性或良性。

57
00:04:52,321 --> 00:04:58,331
classification problems sometimes you can
have more than two values for the two
在分类问题中，对于输出，你也可以多于两个值。

58
00:04:58,331 --> 00:05:03,852
possible values for the output. As a
 concrete example maybe there are three
在实际例子中就是，可能有三种类型的乳腺癌，

59
00:05:03,852 --> 00:05:09,947
types of breast cancers and so you may try
 to predict the discrete value of zero,
因此，你可能要设法预测离散值0，

60
00:05:09,947 --> 00:05:15,138
one, two, or three with zero being benign.
Benign tumor, so no cancer. And one may
1,2或3，其中0是良性的。良性瘤就是说没有癌症。

61
00:05:15,138 --> 00:05:19,836
mean, type one cancer, like, you have
 three types of cancer, whatever type one
1就是指第一种癌症，

62
00:05:19,836 --> 00:05:24,654
means. And two may mean a second type of
cancer, a three may mean a third type of
2就是指第二种癌症，3就是指第三种癌症。

63
00:05:24,654 --> 00:05:29,111
cancer. But this would also be a
classification problem, because this other
这也是一个分类问题，因为这些离散的输出值，

64
00:05:29,111 --> 00:05:33,929
discrete value set of output corresponding
to, you know, no cancer, or cancer type
对应于你知道没有癌症，或者癌症一、

65
00:05:33,929 --> 00:05:39,094
one, or cancer type two, or cancer type
three. In classification problems there is
癌症二、或癌症三。在分类问题中，

66
00:05:39,094 --> 00:05:44,413
another way to plot this data. Let me show
you what I mean. Let me use a slightly
有另一种方法来绘制这些数据。

67
00:05:44,413 --> 00:05:49,206
different set of symbols to plot this
data. So if tumor size is going to be the
我们用有点不同的符号集来绘制这组数据。假设瘤的尺寸是我用来

68
00:05:49,206 --> 00:05:54,303
attribute that I'm going to use to predict
malignancy or benignness, I can also draw
预测恶性或良性的特征。

69
00:05:54,303 --> 00:05:58,975
my data like this. I'm going to use
different symbols to denote my benign and
我也可以这样画我的数据。我将用不同的符号来表示我的良性或恶性，

70
00:05:58,975 --> 00:06:03,707
malignant, or my negative and positive
examples. So instead of drawing crosses,
或者说我的反例和正例。因此，不是画叉，

71
00:06:03,707 --> 00:06:11,595
I'm now going to draw O's for the benign
tumors. Like so. And I'm going to keep
这里我将用O来表示良性瘤，就像这样。

72
00:06:11,595 --> 00:06:18,655
using X's to denote my malignant tumors.
Okay? I hope this is beginning to make
我还是继续用叉表示恶性瘤。我希望这是可理解的。

73
00:06:18,655 --> 00:06:23,624
sense. All I did was I took, you know,
these, my data set on top and I just
我所做的就是我将我的数据集映射到这些实线。

74
00:06:23,624 --> 00:06:30,894
mapped it down. To this real line like so.
And started to use different symbols,
并且，使用不同的符号，

75
00:06:30,894 --> 00:06:35,828
circles and crosses, to denote malignant
versus benign examples. Now, in this
圆圈和叉，表示恶性和良性例子。

76
00:06:35,828 --> 00:06:41,091
example we use only one feature or one
 attribute, mainly, the tumor size in order
在这个例子中，我们只使用了一个特征或者说一个属性，瘤的尺寸，

77
00:06:41,091 --> 00:06:46,289
to predict whether the tumor is malignant
or benign. In other machine learning
来预测瘤是恶性的还是良性的。在其它的机器学习问题中，

78
00:06:46,289 --> 00:06:51,355
problems when we have more than one
feature, more than one attribute. Here's
我们会有多个特征，多个属性。

79
00:06:51,355 --> 00:06:56,749
an example. Let's say that instead of just
knowing the tumor size, we know both the
这儿有个例子。假设，不仅只指定瘤的尺寸，我们还知道病人的年纪。

80
00:06:56,749 --> 00:07:02,387
age of the patients and the tumor size. In
 that case maybe your data set will look
在这种情况下，你的数据集就是这样的，

81
00:07:02,387 --> 00:07:08,562
like this where I may have a set of patients
with those ages and that tumor size and
我可能有一组病人是这些年龄的，瘤的尺寸是这样的。

82
00:07:08,562 --> 00:07:14,980
they look like this. And a different set
of patients, they look a little different,
另一组病人，他们看上去有点不同，

83
00:07:15,600 --> 00:07:23,968
whose tumors turn out to be malignant, as
 denoted by the crosses. So, let's say you
他们的瘤是恶性的，用叉表示。

84
00:07:23,968 --> 00:07:32,027
have a friend who tragically has a
tumor. And maybe, their tumor size and age
假设你有一个朋友，很不幸有一个瘤。可能，他的瘤的尺寸和她的年记在这儿。

85
00:07:32,027 --> 00:07:37,657
falls around there. So given a data set
like this, what the learning algorithm
因此，给定如此的数据集，学习算法能做的

86
00:07:37,657 --> 00:07:42,462
might do is throw the straight line
through the data to try to separate out
可能就是在数据上给出这样一条直线，设法将恶性瘤和良性瘤分开。

87
00:07:42,462 --> 00:07:47,710
the malignant tumors from the benign one s
and, so the learning algorithm may decide
从而，学习算法可能决定

88
00:07:47,710 --> 00:07:53,004
to throw the straight line like that to
separate out the two classes of tumors.
用这样的直线来分离这两类瘤。

89
00:07:53,004 --> 00:07:57,644
And. You know, with this, hopefully you
 can decide that your friend's tumor is
有了这个，很幸运地，你就可以决定你朋友的瘤很有可能是什么样的，

90
00:07:57,644 --> 00:08:02,322
more likely to if it's over there,
that hopefully your learning algorithm
假设它在这儿，你的学习算法将会说

91
00:08:02,322 --> 00:08:07,305
will say that your friend's tumor falls on
 this benign side and is therefore more
你朋友的瘤位于良性区域，因此，是良性的可能性比恶性的大。

92
00:08:07,305 --> 00:08:12,044
likely to be benign than malignant. In
this example we had two features, namely,
在这个例子中，我们有两种特征，

93
00:08:12,044 --> 00:08:17,147
the age of the patient and the size of the
tumor. In other machine learning problems
病人的年级和瘤的尺寸。在其它的机器学习算法中，可能会有更多的特征。

94
00:08:17,147 --> 00:08:21,454
we will often have more features, and my
friends that work on this problem, they
我的朋友做了这样的问题，

95
00:08:21,454 --> 00:08:25,849
actually use other features like these,
which is clump thickness, the clump thickness of
他们实际上使用了其它的特征，如肿块的厚度，

96
00:08:25,849 --> 00:08:30,299
the breast tumor. Uniformity of cell size
of the tumor. Uniformity of cell shape of
还有瘤细胞的尺寸的均匀性，瘤细胞形状的均匀性等，

97
00:08:30,299 --> 00:08:34,911
the tumor, and so on, and other features
 as well. And it turns out one of the interes-,
以及其它的特征。一个有趣的，

98
00:08:34,911 --> 00:08:39,907
most interesting learning algorithms that
we'll see in this class is a learning
或更有趣的学习算法是，我们会看到，

99
00:08:39,907 --> 00:08:45,153
algorithm that can deal with, not just two
or three or five features, but an infinite
一个学习算法可处理，不仅仅两到三个或五个特征，而是无穷多的特征。

100
00:08:45,153 --> 00:08:50,150
number of features. On this slide, I've
listed a total of five different features.
在这个幻灯片中，我列出了总共五种不同的特征。

101
00:08:50,150 --> 00:08:54,482
Right, two on the axes and three more up here.
But it turns out that for some learning
两个在轴上，三个在这儿。但是，这说明对某些学习问题，

102
00:08:54,482 --> 00:08:58,497
problems, what you really want is not to
use, like, three or five features. But
你真正想要的不是使用像三个还是五个特征，

103
00:08:58,497 --> 00:09:02,566
instead, you want to use an infinite
number of features, an infinite number of
而是，你想使用一个无穷多数量的特征集，一个无穷多数量的属性集。

104
00:09:02,566 --> 00:09:06,211
attributes, so that your learning
algorithm has lots of attributes or
因此，你的学习算法有很多的属性或特征或线索来做预测。

105
00:09:06,211 --> 00:09:10,333
features or cues with which to make those
predictions. So how do you deal with an
因此，你如何来处理这些特征呢。

106
00:09:10,333 --> 00:09:14,439
infinite number of features. How do you even
store an infinite number of
你如何处理无穷多数量的特征，如何在计算机中存储无穷多数量的事物，

107
00:09:14,439 --> 00:09:18,290
things on the computer when your
computer is gonna run out of memory. It
这时，你的计算机可能会溢出。

108
00:09:18,290 --> 00:09:22,188
turns out that when we talk about an
algorithm called the Support Vector
以支持向量机算法为例，

109
00:09:22,188 --> 00:09:26,675
Machine, there will be a neat mathematical
 trick that will allow a computer to deal
就有一个灵巧的数学技巧，它允许计算机处理无穷多的特征。

110
00:09:26,675 --> 00:09:31,214
with an infinite number of features. Imagine
that I didn't just write down two features
假设我没有在这儿写下两个特征，

111
00:09:31,214 --> 00:09:35,487
here and three features on the right. But, imagine that
I wrote down an infinitely long list, I
也没有在这写三个特征。 假设我写下无穷长的列表，

112
00:09:35,487 --> 00:09:39,866
just kept writing more and more and more
features. Like an infinitely long list of
我不停地写更多更多的特征，无限长的特征列表。

113
00:09:39,866 --> 00:09:44,192
features. Turns out, we'll be able to come
up with an algorithm that can deal with
结果是，我们能提出一个算法来处理它。

114
00:09:44,192 --> 00:09:49,701
that. So, just to recap. In this
 class we'll talk about supervised
概括一下，在这节课上，我们仅讨论监督学习。

115
00:09:49,701 --> 00:09:54,167
learning. And the idea is that, in
 supervised learning, in every example in
思想是，在监督学习中，数据集中的每个例子，

116
00:09:54,167 --> 00:09:58,880
our data set, we are told what is the
"correct answer" that we would have
算法将预测得到例子的“正确答案”。

117
00:09:58,880 --> 00:10:03,960
quite liked the algorithms have predicted
on that example. Such as the price of the
像房子的价格，

118
00:10:03,960 --> 00:10:08,428
house, or whether a tumor is malignant or
benign. We also talked about the
或瘤是恶性的还是良性的。

119
00:10:08,428 --> 00:10:13,202
regression problem. And by regression,
that means that our goal is to predict a
我们也讨论了回归问题。回归是指我们的目标是预测一个连续的输出值。

120
00:10:13,202 --> 00:10:17,977
continuous valued output. And we talked
 about the classification problem, where
我们讨论了分类问题，

121
00:10:17,977 --> 00:10:22,690
the goal is to predict a discrete value
output. Just a quick wrap up question:
目的是预测离散的输出值。

122
00:10:22,690 --> 00:10:27,541
Suppose you're running a company and you
want to develop learning algorithms to
假设你运作一个公司，并且你想开发学习算法来处理两个问题中的一个。

123
00:10:27,541 --> 00:10:32,618
address each of two problems. In the first
problem, you have a large inventory of
第一个问题，你有一个相同项目的清单。

124
00:10:32,618 --> 00:10:38,113
identical items. So imagine that you have
thousands of copies of some identical
假设你有某些相同项目的成千上万的副本要卖，

125
00:10:38,113 --> 00:10:43,607
items to sell and you want to predict how
many of these items you sell within the
你想预测下在接下来的三个月内你会卖出这些项目中的多少个。

126
00:10:43,607 --> 00:10:49,172
next three months. In the second problem,
problem two, you'd like--  you have lots of
第二个问题是，你有很多用户，

127
00:10:49,172 --> 00:10:54,145
users and you want to write software to
examine each individual of your
你想要写一个软件来检查每一个客户的账户，

128
00:10:54,145 --> 00:10:59,193
customer's accounts, so each one of your
customer's accounts; and for each account,
你客户账户的每个账户，

129
00:10:59,193 --> 00:11:04,178
decide whether or not the account has been
hacked or compromised. So, for each of
并且决定这个账户被入侵还是被破坏了。

130
00:11:04,178 --> 00:11:08,914
these problems, should they be treated as
a classification problem, or as a
对每一个这种问题，应该被认为是分类问题还是回归问题？

131
00:11:08,914 --> 00:11:14,087
regression problem? When the video pauses,
please use your mouse to select whichever
当视频暂停时，用你的鼠标选择四个选项中的正确答案。

132
00:11:14,087 --> 00:11:20,884
of these four options on the left you
 think is the correct answer. So hopefully,

133
00:11:20,884 --> 00:11:25,871
you got that this is the answer. For
problem one, I would treat this as a
很高兴，你选择了正确答案。问题以，我会把它处理为回归问题，

134
00:11:25,871 --> 00:11:31,058
regression problem, because if I have, you
know, thousands of items, well, I would
因为，假设我有成千上万的项目，我很可能将它看成一个实数，

135
00:11:31,058 --> 00:11:36,071
probably just treat this as a real value,
as a continuous value. And
即一个连续的值。

136
00:11:36,290 --> 00:11:41,837
treat, therefore, the number of items I sell,
as a continuous value. And for the
即把我要卖的项目数量看成一个连续的值。

137
00:11:41,837 --> 00:11:47,748
second problem, I would treat that as a
classification problem, because I might
第二个问题，我将看成一个分类问题，

138
00:11:47,748 --> 00:11:53,659
say, set the value I want to predict with
zero, to denote the account has not been
因为，我可能会设置我要预测的值为0，表示账户没有没入侵；

139
00:11:53,659 --> 00:11:58,850
hacked. And set the value one to denote an
account that has been hacked into. So just
设置值为1，表示账户已经被入侵。

140
00:11:58,850 --> 00:12:03,287
like, you know, breast cancer, is,
zero is benign, one is malignant. So I
因此，正如你知道的，乳腺癌是0的就是良性，1就是恶性。

141
00:12:03,287 --> 00:12:08,150
might set this be zero or one depending on
whether it's been hacked, and have an
因此，我可能会设置它为0或1，看它是否被入侵。

142
00:12:08,150 --> 00:12:13,134
algorithm try to predict each one of these
two discrete values. And because there's a
并用一个算法来预测这两个离散值中的一个。

143
00:12:13,134 --> 00:12:17,693
small number of discrete values, I would
 therefore treat it as a classification
因为，只有少量的离散值，因此，我把它作为一个分类问题。

144
00:12:17,693 --> 00:12:23,075
problem. So, that's it for supervised
 learning and in the next video I'll talk
这就是监督学习。下个视频中，我们将讨论非监督学习，

145
00:12:23,075 --> 00:12:28,325
about unsupervised learning, which is the
other major category of learning algorithms.
它是学习问题的另一大类。

