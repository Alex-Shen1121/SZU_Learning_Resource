1
00:00:00,000 --> 00:00:04,100
In the previous video, we gave the
mathematical definition of the cost
在以前的视频，我们给了代价函数的数学定义
(字幕整理：中国海洋大学 黄海广,haiguang2000@qq.com )

2
00:00:04,100 --> 00:00:08,616
function. In this video, let's look at
some examples, to get back to intuition
在这段视频中，让我们来看看一些例子，在直觉上理解下

3
00:00:08,616 --> 00:00:14,466
about what the cost function is doing, and
why we want to use it. To recap, here's
代价函数是用来做什么的，和为什么我们要使用它。总括来说

4
00:00:14,466 --> 00:00:19,396
what we had last time. We want to fit a
straight line to our data, so we had this
这是我们上节课学到的 我们希望找到与数据拟合的直线

5
00:00:19,396 --> 00:00:23,958
formed as a hypothesis with these
parameters theta zero and theta one, and
所以 根据这些参数θ0和θ1形成为一个假设

6
00:00:23,958 --> 00:00:28,888
with different choices of the parameters
we end up with different straight line
随着选择不同的参数，我们得到不同的直线

7
00:00:31,323 --> 00:00:33,758
fits. So the data which are fit
like so, and there's a cost function, and
就会有像这样的数据对应 这存在一个代价函数

8
00:00:33,758 --> 00:00:38,554
that was our optimization objective.
[sound] So this video, in order to better
这是我们的优化目标 所以这段视频中，为了更好地

9
00:00:38,554 --> 00:00:43,293
visualize the cost function J, I'm going
to work with a simplified hypothesis
可视化代价函数J，我要使用一个简化的假设函数

10
00:00:43,293 --> 00:00:48,220
function, like that shown on the right. So
I'm gonna use my simplified hypothesis,
如显示在右侧的那个 所以 我会用我的简化假设

11
00:00:48,220 --> 00:00:53,275
which is just theta one times X. We can,
if you want, think of this as setting the
这个假设仅包含这些参数θ1*x，如果你愿意，你可以认为这是设置

12
00:00:53,275 --> 00:00:58,721
parameter theta zero equal to 0. So I
have only one parameter theta one and
参数θ0=0 于是，我只有一个参数θ1

13
00:00:58,721 --> 00:01:04,372
my cost function is similar to before
except that now H of X that is now equal
我的代价函数和以前差不太多 只不过 h(x)现在直接等于

14
00:01:04,372 --> 00:01:10,309
to just theta one times X. And I have only
one parameter theta one and so my
θ1乘x 我只有θ1一个参数了，所以我们的

15
00:01:10,309 --> 00:01:16,246
optimization objective is to minimize j of
theta one. In pictures what this means is
优化目标是尽量减少J(θ1) 在图片中，意思就是

16
00:01:16,246 --> 00:01:21,611
that if theta zero equals zero that
corresponds to choosing only hypothesis
如果θ0=0，相当于只选择经过远点的假设函数

17
00:01:21,611 --> 00:01:27,176
functions that pass through the origin,
that pass through the point (0, 0). Using
也就是通过点(0,0)

18
00:01:27,176 --> 00:01:33,415
this simplified definition of a hypothesizing cost
function let's try to understand the cost
应用这个假设代价函数的简化定义，让我们去更好的了解

19
00:01:33,415 --> 00:01:40,178
function concept better. It turns out that
two key functions we want to understand.
代价函数概念 这里有两个关键函数我们想要了解

20
00:01:40,178 --> 00:01:46,432
The first is the hypothesis function, and
the second is a cost function. So, notice
第一个是假设函数 第二个是代价函数

21
00:01:46,432 --> 00:01:52,068
that the hypothesis, right, H of X. For a
face value of theta one, this is a
注意一下这个假设函数h(x)，是的，系数为θ1，这是一个关于x的函数

22
00:01:52,068 --> 00:01:58,168
function of X. So the hypothesis is a
function of, what is the size of the house
因此这个函数是一个函数实现房子x的尺寸的判定

23
00:01:58,168 --> 00:02:03,959
X. In contrast, the cost function, J,
that's a function of the parameter, theta
与此相反，代价函数J的参数是θ1

24
00:02:03,959 --> 00:02:09,993
one, which controls the slope of the
straight line. Let's plot these functions
它控制该直线的斜率。让我们来绘制这些函数

25
00:02:09,993 --> 00:02:15,481
and try to understand them both better.
Let's start with the hypothesis. On the left,
并试着去更好的了解他们 让我们以假设函数开始 在左边

26
00:02:15,481 --> 00:02:20,283
let's say here's my training set with
three points at (1, 1), (2, 2), and (3, 3). Let's
这里是我的训练集有三个点(1,1)，(2,2)，(3,3)

27
00:02:20,283 --> 00:02:25,338
pick a value theta one, so when theta one
equals one, and if that's my choice for
让我们选个θ1的值 所以当θ1=1，如果这是我的选择

28
00:02:25,338 --> 00:02:30,392
theta one, then my hypothesis is going to
look like this straight line over here.
那么我的假设就是在这里的这条直线。

29
00:02:30,392 --> 00:02:35,234
And I'm gonna point out, when I'm plotting
my hypothesis function. X-axis, my
我会指出，当我绘制我的假设函数。 X轴，我的横轴

30
00:02:35,234 --> 00:02:40,525
horizontal axis is labeled X, is labeled
you know, size of the house over here.
横轴标记为X，标记你知道的，在这里的房子的大小。

31
00:02:40,525 --> 00:02:46,551
Now, of temporary, set
theta one equals one, what I want to do is
现在，暂时的，θ1=1，我想要弄清楚的是

32
00:02:46,551 --> 00:02:52,430
figure out what is j of theta one, when
theta one equals one. So let's go ahead
当θ1=1时，J(θ1)的值 所以，让我们继续前进

33
00:02:52,430 --> 00:02:58,781
and compute what the cost function has
for. You'll devalue one. Well, as usual,
计算出代价函数 你会降低它的价值 嗯，像往常一样

34
00:02:58,781 --> 00:03:05,761
my cost function is defined as follows,
right? Some from, some of 'em are training
我的代价函数定义如下，对不对？一些训练集

35
00:03:05,761 --> 00:03:13,840
sets of this usual squared error term.
And, this is therefore equal to. And this.
误差项平方的和 这因此 等于 而这个

36
00:03:14,740 --> 00:03:25,066
Of theta one x I minus y I and if you
simplify this turns out to be. That. Zero
θ1*x(i)-y(i)，如果你简化这个会得到

37
00:03:25,066 --> 00:03:31,995
Squared to zero squared to zero squared which
is of course, just equal to zero. Now,
平方值为0 平方为0 平方 这是当然的就等于零

38
00:03:31,995 --> 00:03:39,098
inside the cost function. It turns out each
of these terms here is equal to zero. Because
在代价函数中，事实上，这些在这里是等于零

39
00:03:39,098 --> 00:03:46,288
for the specific training set I have or my
3 training examples are (1, 1), (2, 2), (3,3). If theta
对于特定的训练集 我有我的3个训练样例(1,1)，(2,2)，(3,3)

40
00:03:46,288 --> 00:03:54,667
one is equal to one. Then h of x. H of x
i. Is equal to y I exactly, let me write
如果θ1=1 那么h( x(i) )=y(i) 让我写的更清楚一点，好吗

41
00:03:54,667 --> 00:04:04,164
this better. Right? And so, h of x minus
y, each of these terms is equal to zero,
所有的h(x)-y结果都是0

42
00:04:04,164 --> 00:04:14,821
which is why I find that j of one is equal
to zero. So, we now know that j of one Is
也就是J(1)=0 所以，我们现在知道J(1)=0

43
00:04:14,821 --> 00:04:20,504
equal to zero. Let's plot that. What I'm
gonna do on the right is plot my cost
让我描绘出它 我在右侧画出我的代价函数

44
00:04:20,504 --> 00:04:26,187
function j. And notice, because my cost
function is a function of my parameter
注意，因为我的代价函数的参数是θ1

45
00:04:26,187 --> 00:04:32,017
theta one, when I plot my cost function,
the horizontal axis is now labeled with
当我绘制我的代价函数，横轴是现在标为θ1

46
00:04:32,017 --> 00:04:38,069
theta one. So I have j of one zero
zero so let's go ahead and plot that. End
所以，我们现在有J(1)=0让我们继续描绘

47
00:04:38,069 --> 00:04:46,464
up with. An X over there. Now lets look at
some other examples. Theta-1 can take on a
那边的X  现在，让我们看看其他一些例子。 θ1可以采取

48
00:04:46,464 --> 00:04:52,470
range of different values. Right? So
theta-1 can take on the negative values,
一个范围内的不同值，对吗？因此，θ1可以取负数

49
00:04:52,470 --> 00:04:58,876
zero, positive values. So what if theta-1
is equal to 0.5. What happens then? Let's
0，正数 那么，如果θ1=0.5会发生什么？让我们

50
00:04:58,876 --> 00:05:05,442
go ahead and plot that. I'm now going to
set theta-1 equals 0.5, and in that case my
继续前进，绘制 我现在要设置θ1=0.5，在这种情况下

51
00:05:05,442 --> 00:05:11,688
hypothesis now looks like this. As a line
with slope equals to 0.5, and, lets
假设现在看起来是这样。为一条斜率等于0.5的直线

52
00:05:11,688 --> 00:05:17,855
compute J, of 0.5. So that is going to be
one over 2M of, my usual cost function.
我们计算J(0.5) 所以这将是我平常代价函数的是1/2M

53
00:05:17,855 --> 00:05:23,769
It turns out that the cost function is
going to be the sum of square values of
事实证明，代价函数是这些线的长度的平方值的总和

54
00:05:23,769 --> 00:05:29,609
the height of this line. Plus the sum of
square of the height of that line, plus
把这些线的长度的平方相加

55
00:05:29,609 --> 00:05:34,783
the sum of square of the height of that
line, right? ?Cause just this vertical
这些线的高度的平方的总和，对不对？是垂直距离

56
00:05:34,783 --> 00:05:42,854
distance, that's the difference between,
you know, Y. I. and the predicted value, H
也就是 你知道的，y与预测值间的距离

57
00:05:42,854 --> 00:05:48,772
of XI, right? So the first example
is going to be 0.5 minus one squared.
预测值是h( x(i) )，对不对？因此，第一个例子将是(0.5-1)的平方

58
00:05:49,033 --> 00:05:55,647
Because my hypothesis predicted 0.5.
Whereas, the actual value was one. For my
因为我的假设值是0.5 然而，实际值是1

59
00:05:55,647 --> 00:06:02,436
second example, I get, one minus two
squared, because my hypothesis predicted
第二个例子中，我得到(1-2)^2，因为我的假设值为1

60
00:06:02,436 --> 00:06:09,663
one, but the actual housing price was two.
And then finally, plus. 1.5 minus three
但实际的住房价格是2。最后再加上(1.5-3)的平方

61
00:06:09,663 --> 00:06:17,263
squared. And so that's equal to one over
two times three. Because, M when trading
所以等于1/(2*3) 因为M有3组例子

62
00:06:17,263 --> 00:06:24,274
set size, right, have three training
examples. In that, that's times
在那，化简为

63
00:06:24,274 --> 00:06:33,011
simplifying for the parentheses it's 3.5.
So that's 3.5 over six which is about
(3.5)  因此 结果为3.5/6 约定于0.68

64
00:06:33,011 --> 00:06:41,085
0.68. So now we know that j of 0.5 is
about 0.68. Lets go and plot that. Oh
所以，现在我们知道，J(0.5)约等于0.68。让我们去绘制它

65
00:06:41,085 --> 00:06:50,308
excuse me, math error, it's actually 0.58. So
we plot that which is maybe about over
对不起，数学错误，它实际上是0.58。因此，我们把它绘制在这里

66
00:06:50,308 --> 00:07:00,293
there. Okay? Now, let's do one more. How
about if theta one is equal to zero, what
好吗？现在，让我们再做一个 当θ1=0会如何呢

67
00:07:00,293 --> 00:07:08,975
is J of zero equal to? It turns out that
if theta one is equal to zero, then H of X
事实证明，如果θ1=0，则h(x)

68
00:07:08,975 --> 00:07:16,916
is just equal to, you know, this flat
line, right, that just goes horizontally
恰好等于 一条水平线，没错，平行

69
00:07:16,916 --> 00:07:26,882
like this. And so, measuring the errors.
We have that J of zero is equal to one
像这样 所以计算误差。我们有J(0)

70
00:07:26,882 --> 00:07:34,659
over two M, times one squared plus two
squared plus three squared, which is, One
=1/2M ( 1^2 + 2^2 + 3^2 ) = 1/6*14约等于2.3

71
00:07:34,659 --> 00:07:41,555
six times fourteen which is about 2.3. So
let's go ahead and plot as well. So it
所以，让我们继续前进，并绘制好。因此

72
00:07:41,555 --> 00:07:47,622
ends up with a value around 2.3
and of course we can keep on doing this
得到与2.3左右的值，当然，我们可以继续这样做

73
00:07:47,622 --> 00:07:53,335
for other values of theta one. It turns
out that you can have you know negative
为θ1赋上其他值 事实证明，你也可以给θ1赋值负数

74
00:07:53,335 --> 00:07:59,327
values of theta one as well so if theta
one is negative then h of x would be equal
如果θ1是负数 比如h(x)将等于

75
00:07:59,327 --> 00:08:05,179
to say minus 0.5 times x then theta
one is minus 0.5 and so that corresponds
-0.5x 当θ1=-0.5

76
00:08:05,179 --> 00:08:10,188
to a hypothesis with a
slope of negative 0.5. And you can
对应的假设函数是斜率为-0.5的直线

77
00:08:10,188 --> 00:08:15,694
actually keep on computing these errors.
This turns out to be, you know, for 0.5,
我们可以继续计算这些误差 你知道，当这个值为0.5，

78
00:08:15,694 --> 00:08:21,520
it turns out to have really high error. It
works out to be something, like, 5.25. And
就会有非常高的误差 最后得到的数是，5.25

79
00:08:21,520 --> 00:08:28,087
so on, and the different values of theta
one, you can compute these things, right?
继续，随着θ1的值不同，你可以计算出这些东西，对不对？

80
00:08:28,087 --> 00:08:34,413
And it turns out that you, your computed
range of values, you get something like
而事实证明，你在一定范围计算一些值，你会得到这样的东西

81
00:08:34,413 --> 00:08:40,499
that. And by computing the range of
values, you can actually slowly create
计算的值的范围，你可以慢慢地创建

82
00:08:40,499 --> 00:08:50,999
out. What does function J of Theta say and
that's what J of Theta is. To recap, for
函数J(θ1)到底是什么 总的来说

83
00:08:50,999 --> 00:08:57,851
each value of theta one, right? Each value
of theta one corresponds to a different
对于每个θ1的值 都对应着假设函数的一个值

84
00:08:57,851 --> 00:09:04,448
hypothesis, or to a different straight
line fit on the left. And for each value
或对应着一条不同的直线，如左侧 并且每个值

85
00:09:04,448 --> 00:09:11,723
of theta one, we could then derive a
different value of j of theta one. And for
θ1 我们可以得出一个不同的J(θ1)的值

86
00:09:11,723 --> 00:09:19,354
example, you know, theta one=1,
corresponded to this straight line
例如，您知道，θ1= 1，符合这条直线

87
00:09:19,354 --> 00:09:27,846
straight through the data. Whereas theta
one=0.5. And this point shown in magenta
正好经过数据  然而当θ1= 0.5 对应的可能就是红色画出的这条线

88
00:09:27,846 --> 00:09:35,340
corresponded to maybe that line, and theta
one=zero which is shown in blue that corresponds to
当θ1=0对应的是蓝色画出的水平线

89
00:09:35,340 --> 00:09:41,527
this horizontal line. Right, so for each
value of theta one we wound up with a
对，所以我们每个值θ1都对应J(θ1)一个值

90
00:09:41,527 --> 00:09:48,516
different value of J of theta one and we
could then use this to trace out this plot
然后，我们可以使用这个可以在右侧描绘出这个图形

91
00:09:48,516 --> 00:09:54,461
on the right. Now you remember, the
optimization objective for our learning
现在，你还记得在我们学习算法时的优化目标

92
00:09:54,461 --> 00:10:01,963
algorithm is we want to choose the value
of theta one. That minimizes J of theta one.
是我们通过选择θ1的价值 最大限度地减少J(θ1)

93
00:10:01,963 --> 00:10:08,076
Right? This was our objective function for
the linear regression. Well, looking at
这是我们的线性回归的目标函数的

94
00:10:08,076 --> 00:10:13,710
this curve, the value that minimizes j of
theta one is, you know, theta one equals
这条曲线中 使J(θ1)最小的值是1，你知道 当θ1=1

95
00:10:13,710 --> 00:10:19,132
to one. And low and behold, that is indeed
the best possible straight line fit
看呀，这确实是能最好的拟合数据的直线

96
00:10:19,132 --> 00:10:24,624
through our data, by setting theta one
equals one. And just, for this particular
通过设置θ1=1 这只是以个特殊的训练集

97
00:10:24,624 --> 00:10:30,328
training set, we actually end up fitting
it perfectly. And that's why minimizing j
我们实际上已经完美的满足了它 这就是为什么要

98
00:10:30,328 --> 00:10:36,447
of theta one corresponds to finding a
straight line that fits the data well. So,
通使J(θ1)最小来找到一个最拟合数据的直线 因此

99
00:10:36,447 --> 00:10:40,884
to wrap up. In this video, we looked up
some plots. To understand the cost
在这段视频中，我们看到了一些图形 要了解代价函数

100
00:10:40,884 --> 00:10:45,259
function. To do so, we simplify the
algorithm. So that it only had one
要做到这一点，我们简化了算法，使它只能有一个参数θ1

101
00:10:45,259 --> 00:10:50,258
parameter theta one. And we set the
parameter theta zero to be only zero. In the next video.
我们设置参数θ0=0 在接下来的视频

102
00:10:50,258 --> 00:10:54,445
We'll go back to the original problem
formulation and look at some
我们会回到原来的问题的公式

103
00:10:54,445 --> 00:10:59,570
visualizations involving both theta zero
and theta one. That is without setting theta
看一些涉及到θ0和θ1的图 这是没有设置θ0=0

104
00:10:59,570 --> 00:11:04,757
zero to zero. And hopefully that will give
you, an even better sense of what the cost
希望这会让你在代价函数J是如何

105
00:11:04,757 --> 00:11:09,257
function j is doing in the original
linear regression formulation.
在最初的线性回归公式中工作的有更好地理解

