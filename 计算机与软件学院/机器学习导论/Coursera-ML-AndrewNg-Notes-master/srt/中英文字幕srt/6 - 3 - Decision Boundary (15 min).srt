1
00:00:00,133 --> 00:00:02,423
In the last video, we talked
在过去的视频中 我们谈到
(字幕整理：中国海洋大学 黄海广,haiguang2000@qq.com )

2
00:00:02,423 --> 00:00:06,653
about the hypothesis representation for logistic progression.
逻辑回归中假设函数的表示方法

3
00:00:06,700 --> 00:00:07,963
What I'd like to do now is
现在 我想

4
00:00:07,963 --> 00:00:09,389
tell you about something called the
告诉大家一个叫做

5
00:00:09,389 --> 00:00:11,370
decision boundary, and this
决策边界(decision boundary)的概念

6
00:00:11,380 --> 00:00:12,894
will give us a better sense
这个概念能更好地帮助我们

7
00:00:12,894 --> 00:00:15,017
of what the logistic regression
理解逻辑回归的

8
00:00:15,030 --> 00:00:17,870
hypothesis function is computing.
假设函数在计算什么

9
00:00:17,870 --> 00:00:20,080
To recap, this is
让我们回忆一下

10
00:00:20,080 --> 00:00:21,264
what we wrote out last time,
这是我们上次写下的公式

11
00:00:21,280 --> 00:00:22,663
where we said that the
当时我们说

12
00:00:22,663 --> 00:00:24,916
hypothesis is represented as
假设函数可以表示为

13
00:00:24,930 --> 00:00:26,119
H of X equals G of
h(x)=g(θTx)

14
00:00:26,119 --> 00:00:28,363
theta transpose X, where G
其中函数g

15
00:00:28,363 --> 00:00:29,871
is this function called the
被称为S形函数（sigmoid function）

16
00:00:29,871 --> 00:00:32,729
sigmoid function, which looks like this.
看起来是应该是这样的形状

17
00:00:32,750 --> 00:00:35,131
So, it slowly increases from zero
它从零开始慢慢增加至1

18
00:00:35,131 --> 00:00:38,996
to one, asymptoting at one.
逐渐逼近1

19
00:00:38,996 --> 00:00:40,391
What I want to do now is
现在让我们

20
00:00:40,391 --> 00:00:42,452
try to understand better when
更进一步来理解

21
00:00:42,452 --> 00:00:44,054
this hypothesis will make
这个假设函数何时

22
00:00:44,070 --> 00:00:45,327
predictions that Y is
会将y预测为1

23
00:00:45,327 --> 00:00:47,049
equal to one versus when it
什么时候又会将

24
00:00:47,049 --> 00:00:48,361
might make predictions that Y
y预测为0

25
00:00:48,361 --> 00:00:50,602
is equal to zero and understand
让我们更好的理解

26
00:00:50,630 --> 00:00:52,351
better what the hypothesis function
假设函数的应该是怎样的

27
00:00:52,351 --> 00:00:56,622
looks like, particularly when we have more than one feature.
特别是当我们的数据有多个特征时

28
00:00:56,640 --> 00:00:59,064
Concretely, this hypothesis is
具体地说 这个假设函数

29
00:00:59,064 --> 00:01:00,827
out putting estimates of the
输出的是

30
00:01:00,827 --> 00:01:02,057
probability that Y is
给定x时

31
00:01:02,060 --> 00:01:05,493
equal to one given X is prime.
y=1的概率

32
00:01:05,530 --> 00:01:06,807
So if we wanted to
因此 如果我们想

33
00:01:06,807 --> 00:01:08,181
predict is Y equal to
预测y=1

34
00:01:08,181 --> 00:01:09,478
one or is Y equal
还是等于0

35
00:01:09,478 --> 00:01:12,217
to zero here's something we might do.
我们可以这样做

36
00:01:12,240 --> 00:01:14,737
When ever the hypothesis its that
只要该假设函数

37
00:01:14,737 --> 00:01:16,412
the problem with y being one
输出y=1的概率

38
00:01:16,412 --> 00:01:17,570
is greater than or equal
大于或等于0.5

39
00:01:17,570 --> 00:01:19,340
to 0.5 so this means
那么这表示

40
00:01:19,350 --> 00:01:21,068
that it is more likely to
y更有可能

41
00:01:21,068 --> 00:01:22,295
be y equals one than y
等于1而不是0

42
00:01:22,295 --> 00:01:26,509
equals zero then let's predict Y equals one.
因此 我们预测y=1

43
00:01:26,509 --> 00:01:27,942
And otherwise, if the probability
在另一种情况下 如果

44
00:01:27,960 --> 00:01:30,168
of, the estimated probability of
预测y=1

45
00:01:30,180 --> 00:01:31,898
Y being one is less
的概率

46
00:01:31,898 --> 00:01:35,025
than 0.5, then let's predict Y equals zero.
小于0.5 那么我们应该预测y=0

47
00:01:35,025 --> 00:01:36,277
And I chose a greater
在这里 我选择大于等于

48
00:01:36,277 --> 00:01:39,666
than or equal to here and less than here.
在这里我选择小于

49
00:01:39,670 --> 00:01:41,010
If H of X is equal
如果h(x)的值

50
00:01:41,010 --> 00:01:43,063
to 0.5 exactly, then
正好等于0.5 那么

51
00:01:43,063 --> 00:01:44,670
we could predict positive or
我们可以预测为1

52
00:01:44,670 --> 00:01:45,820
negative vector but a put a
也可以预测为0

53
00:01:45,820 --> 00:01:47,464
great deal on to here
但是这里我选择了大于等于

54
00:01:47,464 --> 00:01:49,220
so we default maybe to predicting
因此我们默认

55
00:01:49,220 --> 00:01:51,459
a positive if your
如果h(x)等于0.5的话

56
00:01:51,459 --> 00:01:52,883
vector is 0.5 but that's
预测选择为1

57
00:01:52,883 --> 00:01:56,675
a detail that really doesn't matter that much.
这只是一个细节 不用太在意

58
00:01:56,680 --> 00:01:58,136
What I want to do is understand
下面 我希望大家能够

59
00:01:58,140 --> 00:01:59,273
better when it is
清晰地理解

60
00:01:59,273 --> 00:02:01,187
exactly that H of
什么时候h(x)

61
00:02:01,187 --> 00:02:02,927
X will be greater or equal
将大于或等于

62
00:02:02,927 --> 00:02:04,666
to 0.5, so that
0.5 从而

63
00:02:04,666 --> 00:02:09,111
we end up predicting Y is equal to one.
我们最终预测y=1

64
00:02:09,530 --> 00:02:11,525
If we look at this plot
如果我们看看

65
00:02:11,540 --> 00:02:14,208
of the sigmoid function, we'll notice
S形函数的曲线图 我们会注意到

66
00:02:14,208 --> 00:02:17,094
that the sigmoid function, G
S函数

67
00:02:17,094 --> 00:02:18,981
of Z, is greater than
只要z大于

68
00:02:18,981 --> 00:02:21,019
or equal to 0.5
或等于0时

69
00:02:21,030 --> 00:02:24,296
whenever Z is
g(z)就将大于

70
00:02:24,300 --> 00:02:25,994
greater than or equal to zero.
或等于0.5

71
00:02:25,994 --> 00:02:28,163
So is in this half of
因此 在曲线图的这半边

72
00:02:28,163 --> 00:02:29,963
the figure that, G takes
g的取值

73
00:02:29,963 --> 00:02:32,522
on values that are 0.5 and higher.
大于或等于0.5

74
00:02:32,522 --> 00:02:34,482
This is not clear, that's the 0.5.
因为这个交点就是0.5

75
00:02:34,482 --> 00:02:35,957
So when Z is
因此 当z大于0时

76
00:02:35,970 --> 00:02:38,352
positive, G of Z,
g(z) 也就是这个

77
00:02:38,352 --> 00:02:41,959
the sigmoid function, is greater than or equal to 0.5.
S形函数 是大于或等于0.5的

78
00:02:41,959 --> 00:02:44,226
Since the hypothesis for
由于逻辑回归的

79
00:02:44,226 --> 00:02:46,428
logistic regression is H of
假设函数h(x)

80
00:02:46,428 --> 00:02:48,525
X equals G of theta
等于g(θTx)

81
00:02:48,525 --> 00:02:50,964
transpose X. This is
因此

82
00:02:50,964 --> 00:02:52,163
therefore going to be greater
函数值将会

83
00:02:52,180 --> 00:02:54,338
than or equal to 0.5
大于或等于0.5

84
00:02:54,338 --> 00:02:58,329
whenever theta transpose
只要θ转置乘以x

85
00:02:58,340 --> 00:03:01,642
X is greater than or equal to zero.
大于或等于0

86
00:03:01,642 --> 00:03:03,470
So what was shown, right,
因此 我们看到

87
00:03:03,470 --> 00:03:05,835
because here theta transpose X
因为这里θ转置x

88
00:03:05,835 --> 00:03:08,113
takes the row of Z.
取代了z的位置

89
00:03:08,120 --> 00:03:09,543
So what we're shown is that
所以我们看到

90
00:03:09,543 --> 00:03:11,077
our hypothesis is going
我们的假设函数

91
00:03:11,077 --> 00:03:13,191
to predict Y equals one
将会预测y=1

92
00:03:13,200 --> 00:03:15,420
whenever theta transpose X
只要θ转置乘以x

93
00:03:15,420 --> 00:03:17,924
is greater than or equal to 0.
大于或等于0

94
00:03:17,924 --> 00:03:20,016
Let's now consider the other
现在让我们来考虑

95
00:03:20,016 --> 00:03:22,380
case of when a hypothesis
假设函数

96
00:03:22,380 --> 00:03:25,043
will predict Y is equal to 0.
预测y=0的情况

97
00:03:25,043 --> 00:03:27,210
Well, by similar argument, H
类似的

98
00:03:27,210 --> 00:03:28,987
of X is going to be
h(θ)将会

99
00:03:28,987 --> 00:03:30,709
less than 0.5 whenever G
小于0.5 只要

100
00:03:30,730 --> 00:03:32,266
of Z is less than
g(z)小于0.5

101
00:03:32,266 --> 00:03:34,711
0.5 because the range
这是因为

102
00:03:34,720 --> 00:03:36,468
of values of Z that
z的定义域上

103
00:03:36,480 --> 00:03:38,013
calls Z to take on
导致g(z)取值

104
00:03:38,020 --> 00:03:42,626
values less that 0.5, well that's when Z is negative.
小于0.5的部分 是z小于0的部分

105
00:03:42,626 --> 00:03:44,916
So when G of Z is less than 0.5.
所以当g(z)小于0.5时

106
00:03:44,916 --> 00:03:46,874
Our hypothesis will predict
我们的假设函数将会预测

107
00:03:46,874 --> 00:03:48,876
that Y is equal to zero, and
y=0

108
00:03:48,876 --> 00:03:50,540
by similar argument to what
根据与之前

109
00:03:50,540 --> 00:03:52,608
we had earlier, H of
类似的原因

110
00:03:52,608 --> 00:03:54,293
X is equal G of
h(x)等于

111
00:03:54,320 --> 00:03:56,932
theta transpose X. And
g(θTx)

112
00:03:56,932 --> 00:03:58,739
so, we'll predict Y equals
因此 只要

113
00:03:58,739 --> 00:04:01,029
zero whenever this quantity
θ转置乘以x小于0

114
00:04:01,029 --> 00:04:04,937
theta transpose X is less than zero.
我们就预测y等于0

115
00:04:04,940 --> 00:04:06,461
To summarize what we just
总结一下我们刚才所讲的

116
00:04:06,470 --> 00:04:08,377
worked out, we saw that if
我们看到

117
00:04:08,377 --> 00:04:09,900
we decide to predict whether
如果我们要决定

118
00:04:09,900 --> 00:04:11,076
Y is equal to one or
预测y=1

119
00:04:11,076 --> 00:04:12,396
Y is equal to zero,
还是y=0

120
00:04:12,400 --> 00:04:14,216
depending on whether the estimated
取决于

121
00:04:14,216 --> 00:04:15,807
probability is greater than
y=1的概率

122
00:04:15,807 --> 00:04:17,845
or equal 0.5, or whether
大于或等于0.5

123
00:04:17,845 --> 00:04:19,602
it's less than 0.5, then
还是小于0.5

124
00:04:19,602 --> 00:04:20,935
that's the same as saying that
这其实就等于说

125
00:04:20,935 --> 00:04:22,920
will predict Y equals 1
我们将预测y=1

126
00:04:22,920 --> 00:04:25,010
whenever theta transpose axis greater
只需要θ转置乘以x

127
00:04:25,010 --> 00:04:26,002
than or equal to 0,
大于或等于0

128
00:04:26,002 --> 00:04:27,815
and we will predict Y is
另一方面我们将预测y=0

129
00:04:27,815 --> 00:04:30,025
equal to zero whenever theta transpose X
只需要θ转置乘以x

130
00:04:30,025 --> 00:04:32,953
is less than zero.
小于0

131
00:04:32,953 --> 00:04:34,192
Let's use this to better
通过这些 我们能更好地

132
00:04:34,192 --> 00:04:36,890
understand how the hypothesis
理解如何利用逻辑回归的假设函数

133
00:04:36,890 --> 00:04:40,029
of logistic regression makes those predictions.
来进行预测

134
00:04:40,040 --> 00:04:41,535
Now, let's suppose we have
现在假设我们有

135
00:04:41,535 --> 00:04:43,113
a training set like that shown
一个训练集

136
00:04:43,113 --> 00:04:45,165
on the slide, and suppose
就像幻灯片上的这个

137
00:04:45,165 --> 00:04:47,278
our hypothesis is H of
接下来我们假设我们的假设函数是

138
00:04:47,278 --> 00:04:48,678
X equals G of theta
h(x)等于g()

139
00:04:48,678 --> 00:04:50,254
zero, plus theta one X1
括号里面是θ0加上θ1x1

140
00:04:50,260 --> 00:04:52,854
plus theta two X2.
加上θ2乘以x2

141
00:04:52,854 --> 00:04:54,516
We haven't talked yet about how
目前我们还没有谈到

142
00:04:54,516 --> 00:04:56,725
to fit the parameters of this model.
如何拟合此模型中的参数

143
00:04:56,725 --> 00:04:59,355
We'll talk about that in the next video.
我们将在下一个视频中讨论这个问题

144
00:04:59,355 --> 00:05:01,770
But suppose that variable procedure
但是假设我们

145
00:05:01,770 --> 00:05:03,575
to be specified, we end
已经拟合好了参数

146
00:05:03,575 --> 00:05:06,224
up choosing the following values for the parameters.
我们最终选择了如下值

147
00:05:06,224 --> 00:05:07,861
Let's say we choose theta zero
比方说 我们选择θ0

148
00:05:07,861 --> 00:05:09,750
equals three, theta one
等于-3 θ1

149
00:05:09,750 --> 00:05:13,553
equals one, theta two equals one.
等于1 θ2等于1

150
00:05:13,553 --> 00:05:15,430
So this means that my parameter
因此 这意味着我的

151
00:05:15,430 --> 00:05:17,263
vector is going to be
参数向量将是

152
00:05:17,263 --> 00:05:22,963
theta equals minus 311.
θ等于[-3 1 1]

153
00:05:24,140 --> 00:05:27,055
So, we're given this
这样 我们有了

154
00:05:27,060 --> 00:05:30,115
choice of my hypothesis parameters,
这样的一个参数选择

155
00:05:30,115 --> 00:05:32,243
let's try to figure out where
让我们试着找出

156
00:05:32,280 --> 00:05:33,778
a hypothesis will end up
假设函数何时将

157
00:05:33,778 --> 00:05:35,493
predicting y equals 1 and where it
预测y等于1

158
00:05:35,493 --> 00:05:39,055
will end up predicting y equals 0.
何时又将预测y等于0

159
00:05:39,060 --> 00:05:40,660
Using the formulas that we
使用我们在

160
00:05:40,660 --> 00:05:42,900
worked on the previous slide, we know
在上一张幻灯片上展示的公式 我们知道

161
00:05:42,900 --> 00:05:44,539
that Y equals 1 is
y更有可能是1

162
00:05:44,539 --> 00:05:45,849
more likely, that is the
或者说

163
00:05:45,849 --> 00:05:47,404
probability that Y equals
y等于1的概率

164
00:05:47,404 --> 00:05:48,943
1 is greater than 0.5
大于0.5

165
00:05:48,950 --> 00:05:51,553
or greater than or equal to 0.5.
或者大于等于0.5

166
00:05:51,570 --> 00:05:55,256
Whenever theta transpose x
只要θ转置x

167
00:05:55,256 --> 00:05:57,211
is greater than zero.
大于0

168
00:05:57,230 --> 00:05:58,729
And this formula that I
我刚刚加了下划线的

169
00:05:58,729 --> 00:06:00,846
just underlined minus three
这个公式

170
00:06:00,850 --> 00:06:03,033
plus X1 plus X2 is,
-3加上x1再加上x2

171
00:06:03,033 --> 00:06:05,216
of course, theta transpose
当然就是θ转置x

172
00:06:05,220 --> 00:06:07,014
X when theta is equal
这是当θ等于

173
00:06:07,014 --> 00:06:09,746
to this value of the parameters
我们选择的这个参数值时

174
00:06:09,760 --> 00:06:12,516
that we just chose.
θ转置乘以x的表达

175
00:06:12,516 --> 00:06:14,640
So, for any example, for
因此 举例来说

176
00:06:14,640 --> 00:06:16,426
any example with features X1
对于任何样本

177
00:06:16,426 --> 00:06:19,300
and X2 that satisfy this
只要x1和x2满足

178
00:06:19,300 --> 00:06:21,187
equation that minus 3
这个等式 也就是-3

179
00:06:21,187 --> 00:06:23,526
plus X1 plus X2
加上x1再加x2

180
00:06:23,530 --> 00:06:24,723
is greater than or equal to 0.
大于等于0

181
00:06:24,723 --> 00:06:27,028
Our hypothesis will think
我们的假设函数就会认为

182
00:06:27,028 --> 00:06:28,066
that Y equals 1 is
y等于1

183
00:06:28,066 --> 00:06:32,463
more likely, or will predict that Y is equal to one.
的可能性较大 或者说将预测y=1

184
00:06:32,463 --> 00:06:34,505
We can also take minus three
我们也可以

185
00:06:34,505 --> 00:06:35,752
and bring this to the right
将-3放到不等式右边

186
00:06:35,760 --> 00:06:37,703
and rewrite this as X1
并改写为x1

187
00:06:37,740 --> 00:06:41,435
plus X2 is greater than or equal to three.
加号x2大于等于3

188
00:06:41,435 --> 00:06:43,584
And so, equivalently, we found
这样是等价的 我们发现

189
00:06:43,590 --> 00:06:45,826
that this hypothesis will predict
这一假设函数将预测

190
00:06:45,826 --> 00:06:47,561
Y equals one whenever X1
y=1 只要

191
00:06:47,561 --> 00:06:51,854
plus X2 is greater than or equal to three.
x1+x2大于等于3

192
00:06:51,870 --> 00:06:54,893
Let's see what that means on the figure.
让我们来看看这在图上是什么意思

193
00:06:54,893 --> 00:06:57,209
If I write down the equation,
如果我写下等式

194
00:06:57,209 --> 00:07:00,217
X1 plus X2 equals three,
x1+x2等于3

195
00:07:00,230 --> 00:07:03,356
this defines the equation of a straight line.
这将定义一条直线

196
00:07:03,360 --> 00:07:05,040
And if I draw what that straight
如果我画出这条直线

197
00:07:05,040 --> 00:07:07,695
line looks like, it gives
它将表示为

198
00:07:07,730 --> 00:07:10,116
me the following line which passes
这样一条线 它通过

199
00:07:10,116 --> 00:07:11,627
through 3 and 3 on
通过x1轴上的3

200
00:07:11,627 --> 00:07:14,946
the X1 and the X2 axis.
和x2轴上的3

201
00:07:15,886 --> 00:07:17,250
So the part of the input space,
因此 这部分的输入样本空间

202
00:07:17,270 --> 00:07:18,827
the part of the
这一部分的

203
00:07:18,827 --> 00:07:21,553
X1, X2 plane that corresponds
X1-X2平面

204
00:07:21,553 --> 00:07:24,948
to when X1 plus X2 is greater than or equal to three.
对应x1加x2大于等于3

205
00:07:24,948 --> 00:07:27,195
That's going to be this very top plane.
这将是上面这个半平面

206
00:07:27,210 --> 00:07:29,442
That is everything to the
也就是所有

207
00:07:29,442 --> 00:07:30,701
up, and everything to the upper
上方和所有右侧的部分

208
00:07:30,701 --> 00:07:34,109
right portion of this magenta line that I just drew.
相对我画的这条洋红色线来说

209
00:07:34,109 --> 00:07:35,584
And so, the region where our
所以

210
00:07:35,610 --> 00:07:37,135
hypothesis will predict Y
我们的假设函数预测

211
00:07:37,135 --> 00:07:38,324
equals 1 is this
y等于1的区域

212
00:07:38,330 --> 00:07:40,023
region, you know, is
就是这片区域

213
00:07:40,023 --> 00:07:41,586
really this huge region, this
是这个巨大的区域

214
00:07:41,620 --> 00:07:44,393
half-space over to the upper right.
是右上方的这个半平面

215
00:07:44,393 --> 00:07:45,483
And let me just write that down.
让我把它写下来

216
00:07:45,483 --> 00:07:47,395
I'm gonna call this the Y
我将称它为

217
00:07:47,395 --> 00:07:50,263
equals one region, and in
y=1区域

218
00:07:50,263 --> 00:07:54,293
contrast the region where
与此相对

219
00:07:54,293 --> 00:07:56,500
X1 plus X2 is
x1加x2

220
00:07:56,510 --> 00:07:58,691
less than three, that's when
小于3的区域

221
00:07:58,691 --> 00:08:00,090
we will predict that Y,
也就是我们预测

222
00:08:00,110 --> 00:08:01,988
Y is equal to zero, and
y等于0的区域

223
00:08:01,988 --> 00:08:04,679
that corresponds to this region.
是这一片区域

224
00:08:04,710 --> 00:08:06,096
You know, itt's really a half-plane, but
你看到 这也是一个半平面

225
00:08:06,096 --> 00:08:08,530
that region on the left is
左侧的这个半平面

226
00:08:08,530 --> 00:08:11,736
the region where our hypothesis predict Y equals 0.
是我们的假设函数预测y等于0的区域

227
00:08:11,740 --> 00:08:13,431
I want to give
我想给这条线一个名字

228
00:08:13,431 --> 00:08:16,475
this line, this magenta line that I drew a name.
就是我刚刚画的这条洋红色线

229
00:08:16,475 --> 00:08:19,458
This line there is called
这条线被称为

230
00:08:19,458 --> 00:08:24,648
the decision boundary.
决策边界（decision boundary）

231
00:08:24,648 --> 00:08:27,085
And concretely, this straight line
具体地说 这条直线

232
00:08:27,085 --> 00:08:28,468
X1 plus X equals 3.
满足x1+x2=3

233
00:08:28,470 --> 00:08:31,170
That corresponds to the set of points.
它对应一系列的点

234
00:08:31,170 --> 00:08:33,334
So that corresponds to the region
它对应

235
00:08:33,334 --> 00:08:34,606
where H of X is equal
h(x)等于

236
00:08:34,606 --> 00:08:37,000
to 0.5 exactly and
0.5的区域

237
00:08:37,000 --> 00:08:38,731
the decision boundary, that is
决策边界 也就是

238
00:08:38,750 --> 00:08:40,696
this straight line, that's the
这条直线

239
00:08:40,720 --> 00:08:42,772
line that separates the region
将整个平面分成了两部分

240
00:08:42,772 --> 00:08:44,659
where the hypothesis predicts Y equals
其中一片区域假设函数预测y等于1

241
00:08:44,659 --> 00:08:46,433
one from the region
而另一片区域

242
00:08:46,433 --> 00:08:49,773
where the hypothesis predicts that Y is equal to 0.
假设函数预测y等于0

243
00:08:49,773 --> 00:08:51,387
And just to be clear.
我想澄清一下

244
00:08:51,390 --> 00:08:53,353
The decision boundary is a
决策边界是

245
00:08:53,353 --> 00:08:57,458
property of the hypothesis
假设函数的一个属性

246
00:08:57,458 --> 00:09:00,705
including the parameters theta 0, theta 1, theta 2.
它包括参数θ0 θ1 θ2

247
00:09:00,720 --> 00:09:03,216
And in the figure I drew a training set.
在这幅图中 我画了一个训练集

248
00:09:03,240 --> 00:09:06,455
I drew a data set in order to help the visualization.
我画了一组数据 让它更加可视化

249
00:09:06,480 --> 00:09:07,721
But even if we take
但是 即使我们

250
00:09:07,721 --> 00:09:09,276
away the data set, you know
去掉这个数据集

251
00:09:09,280 --> 00:09:11,076
decision boundary and a
这条决策边界

252
00:09:11,076 --> 00:09:12,299
region where we predict Y
和我们预测y等于1

253
00:09:12,300 --> 00:09:14,321
equals 1 versus Y equals zero.
与y等于0的区域

254
00:09:14,321 --> 00:09:15,513
That's a property of the
它们都是

255
00:09:15,513 --> 00:09:16,838
hypothesis and of the
假设函数的属性

256
00:09:16,838 --> 00:09:18,804
parameters of the hypothesis, and
决定于其参数

257
00:09:18,820 --> 00:09:22,163
not a property of the data set.
它不是数据集的属性

258
00:09:22,163 --> 00:09:23,606
Later on, of course, we'll talk
当然 我们后面还将讨论

259
00:09:23,606 --> 00:09:24,683
about how to fit the
如何拟合参数

260
00:09:24,683 --> 00:09:26,736
parameters and there we'll
那时 我们将

261
00:09:26,736 --> 00:09:28,222
end up using the training set,
使用训练集

262
00:09:28,222 --> 00:09:32,547
or using our data, to determine the value of the parameters.
使用我们的数据 来确定参数的取值

263
00:09:32,563 --> 00:09:34,550
But once we have particular values
但是 一旦我们有确定的参数取值

264
00:09:34,550 --> 00:09:37,283
for the parameters: theta 0, theta 1, theta 2.
有确定的θ0 θ1 θ2

265
00:09:37,290 --> 00:09:39,645
Then that completely defines
我们就将完全确定

266
00:09:39,645 --> 00:09:41,721
the decision boundary and we
决策边界

267
00:09:41,721 --> 00:09:43,117
don't actually need to plot
这时 我们实际上并不需要

268
00:09:43,117 --> 00:09:44,886
a training set in order
在绘制决策边界的时候

269
00:09:44,886 --> 00:09:48,180
to plot the decision boundary.
绘制训练集

270
00:09:49,620 --> 00:09:50,626
Let's now look at a more
现在 让我们看一个

271
00:09:50,626 --> 00:09:52,398
complex example where, as
更复杂的例子

272
00:09:52,420 --> 00:09:54,039
usual, I have crosses to
和往常一样 我使用十字 (X)

273
00:09:54,040 --> 00:09:55,932
denote my positive examples and
表示我的正样本

274
00:09:55,932 --> 00:09:58,926
O's to denote my negative examples.
圆圈 (O) 的表示我的负样本

275
00:09:58,926 --> 00:10:00,696
Given a training set like this,
给定这样的一个训练集

276
00:10:00,710 --> 00:10:02,873
how can I get logistic regression
我怎样才能使用逻辑回归

277
00:10:02,900 --> 00:10:05,550
to fit this sort of data?
拟合这些数据呢？

278
00:10:05,550 --> 00:10:07,168
Earlier, when we were talking about
早些时候 当我们谈论

279
00:10:07,168 --> 00:10:09,120
polynomial regression or when
多项式回归

280
00:10:09,120 --> 00:10:10,993
we're linear regression, we talked
或线性回归时

281
00:10:10,993 --> 00:10:12,530
about how we can add extra
我们谈到可以添加额外的

282
00:10:12,530 --> 00:10:15,561
higher order polynomial terms to the features.
高阶多项式项

283
00:10:15,561 --> 00:10:18,996
And we can do the same for logistic regression.
同样我们也可以对逻辑回归使用相同的方法

284
00:10:18,996 --> 00:10:22,220
Concretely, let's say my hypothesis looks like this.
具体地说 假如我的假设函数是这样的

285
00:10:22,220 --> 00:10:23,718
Where I've added two extra
我已经添加了两个额外的特征

286
00:10:23,718 --> 00:10:27,691
features, X1 squared and X2 squared, to my features.
x1平方和x2平方

287
00:10:27,691 --> 00:10:29,811
So that I now have 5 parameters,
所以 我现在有5个参数

288
00:10:29,811 --> 00:10:32,676
theta 0 through theta 4.
θ0 到 θ4

289
00:10:32,676 --> 00:10:34,936
As before, we'll defer to
之前讲过 我们会

290
00:10:34,936 --> 00:10:37,398
the next video our discussion
在下一个视频中讨论

291
00:10:37,420 --> 00:10:39,289
on how to automatically choose
如何自动选择

292
00:10:39,289 --> 00:10:42,511
values for the parameters theta 0 through theta 4.
参数θ0到θ4的取值

293
00:10:42,511 --> 00:10:44,326
But let's say that
但是 假设我

294
00:10:44,326 --> 00:10:46,691
very procedure to be specified,
已经使用了这个方法

295
00:10:46,691 --> 00:10:49,243
I end up choosing theta 0
我最终选择θ0等于-1

296
00:10:49,243 --> 00:10:51,324
equals minus 1, theta 1
θ1等于0

297
00:10:51,324 --> 00:10:52,921
equals 0, theta 2
θ2等于0

298
00:10:52,921 --> 00:10:55,664
equals 0, theta 3 equals
θ3等于1

299
00:10:55,664 --> 00:10:58,039
1, and theta 4 equals 1.
θ4等于1

300
00:10:58,039 --> 00:11:00,223
What this means
这意味着

301
00:11:00,223 --> 00:11:02,160
is that with this particular choice
在这个参数选择下

302
00:11:02,160 --> 00:11:04,566
of parameters, my parameter
我的参数向量

303
00:11:04,566 --> 00:11:09,422
vector theta looks like minus 1, 0, 0, 1, 1.
θ将是[-1 0 0 1 1]

304
00:11:10,550 --> 00:11:12,356
Following our earlier discussion, this
根据我们前面的讨论

305
00:11:12,356 --> 00:11:14,439
means that my hypothesis will predict
这意味着我的假设函数将预测

306
00:11:14,439 --> 00:11:16,407
that Y is equal to 1
y=1

307
00:11:16,407 --> 00:11:18,259
whenever minus 1 plus X1
只要-1加x1平方

308
00:11:18,259 --> 00:11:21,088
squared plus X2 squared is greater than or equal to 0.
加x2平方大于等于0

309
00:11:21,088 --> 00:11:24,184
This is whenever theta transpose
也就是θ转置

310
00:11:24,184 --> 00:11:26,346
times my theta transpose
我的θ转置

311
00:11:26,350 --> 00:11:30,030
my features is greater than or equal to 0.
乘以特征变量大于等于0的时候

312
00:11:30,060 --> 00:11:31,685
And if I take minus
如果我将

313
00:11:31,690 --> 00:11:32,950
1 and just bring this to
-1放到不等式右侧

314
00:11:32,950 --> 00:11:34,810
the right, I'm saying that
我可以说

315
00:11:34,810 --> 00:11:36,642
my hypothesis will predict that
我的假设函数将预测

316
00:11:36,642 --> 00:11:38,100
Y is equal to 1
y=1

317
00:11:38,120 --> 00:11:40,710
whenever X1 squared plus
只要x1平方加

318
00:11:40,710 --> 00:11:43,648
X2 squared is greater than or equal to 1.
x2的平方大于等于1

319
00:11:43,648 --> 00:11:47,990
So, what does decision boundary look like?
那么决策边界是什么样子的呢？

320
00:11:47,990 --> 00:11:49,767
Well, if you were to plot the
好吧 如果我们绘制

321
00:11:49,780 --> 00:11:51,905
curve for X1 squared plus
x1平方加

322
00:11:51,905 --> 00:11:53,665
X2 squared equals 1.
x2的平方等于1的曲线

323
00:11:53,665 --> 00:11:55,531
Some of you will
你们有些人已经

324
00:11:55,531 --> 00:11:58,294
that is the equation for
知道这个方程对应

325
00:11:58,294 --> 00:12:01,296
a circle of radius
半径为1

326
00:12:01,296 --> 00:12:04,163
1 centered around the origin.
原点为中心的圆

327
00:12:04,163 --> 00:12:08,382
So, that is my decision boundary.
所以 这就是我们的决策边界

328
00:12:10,410 --> 00:12:12,190
And everything outside the
圆外面的一切

329
00:12:12,250 --> 00:12:14,207
circle I'm going to predict
我将预测

330
00:12:14,207 --> 00:12:15,404
as Y equals 1.
y=1

331
00:12:15,404 --> 00:12:17,706
So out here is, you know, my
所以这里就是

332
00:12:17,706 --> 00:12:19,337
Y equals 1 region.
y等于1的区域

333
00:12:19,360 --> 00:12:22,693
I'm going to predict Y equals 1 out here.
我们在这里预测y=1

334
00:12:22,693 --> 00:12:24,294
And inside the circle is where
而在圆里面

335
00:12:24,310 --> 00:12:27,786
I'll predict Y is equal to 0.
我会预测y=0

336
00:12:27,790 --> 00:12:30,060
So, by adding these more
因此 通过增加这些

337
00:12:30,060 --> 00:12:33,163
complex or these polynomial terms to my features as well.
复杂的多项式特征变量

338
00:12:33,163 --> 00:12:35,040
I can get more complex decision
我可以得到更复杂的决定边界

339
00:12:35,040 --> 00:12:36,550
boundaries that don't just
而不只是

340
00:12:36,550 --> 00:12:39,560
try to separate the positive and negative examples of a straight line.
用直线分开正负样本

341
00:12:39,560 --> 00:12:41,317
I can get in this example
在这个例子中 我可以得到

342
00:12:41,317 --> 00:12:44,258
a decision boundary that's a circle.
一个圆形的决策边界

343
00:12:44,258 --> 00:12:46,010
Once again the decision boundary
再次强调 决策边界

344
00:12:46,010 --> 00:12:47,888
is a property not of
不是训练集的属性

345
00:12:47,888 --> 00:12:51,636
the training set, but of the hypothesis and of the parameters.
而是假设本身及其参数的属性

346
00:12:51,640 --> 00:12:53,115
So long as we've
只要我们

347
00:12:53,115 --> 00:12:55,389
given my parameter vector theta,
给定了参数向量θ

348
00:12:55,389 --> 00:12:57,185
that defines the decision
圆形的决定边界

349
00:12:57,185 --> 00:12:59,208
boundary which is the circle.
就确定了

350
00:12:59,210 --> 00:13:03,052
But the training set is not what we use to define decision boundary.
我们不是用训练集来定义的决策边界

351
00:13:03,052 --> 00:13:06,563
The training set may be used to fit the parameters theta.
我们用训练集来拟合参数θ

352
00:13:06,563 --> 00:13:08,632
We'll talk about how to do that later.
以后我们将谈论如何做到这一点

353
00:13:08,632 --> 00:13:09,858
But once you have the
但是 一旦你有

354
00:13:09,858 --> 00:13:13,638
parameters theta, that is what defines the decision boundary.
参数θ它就确定了决策边界

355
00:13:13,638 --> 00:13:16,388
Let me put back the training set
让我重新显示训练集

356
00:13:16,400 --> 00:13:18,587
just for visualization.
以方便可视化

357
00:13:18,587 --> 00:13:22,313
And finally, let's look at a more complex example.
最后 让我们来看看一个更复杂的例子

358
00:13:22,320 --> 00:13:23,303
So can we come up
我们可以得到

359
00:13:23,303 --> 00:13:26,538
with even more complex decision boundaries and this?
更复杂的决策边界吗？

360
00:13:26,538 --> 00:13:28,418
If I have even higher
如果我有

361
00:13:28,420 --> 00:13:31,155
order polynomial terms, so things
高阶多项式特征变量

362
00:13:31,155 --> 00:13:34,505
like X1 squared, X1
比如x1平方

363
00:13:34,505 --> 00:13:36,604
squared X2, X1 squared
x1平方乘以x2 x1平方乘以x2平方

364
00:13:36,604 --> 00:13:37,826
X2 squared, and so on.
等等

365
00:13:37,826 --> 00:13:39,001
If I have much higher order
如果我有更高阶

366
00:13:39,001 --> 00:13:41,574
polynomials, then it's possible
多项式 那么可以证明

367
00:13:41,574 --> 00:13:42,856
to show that you can get
你将得到

368
00:13:42,856 --> 00:13:45,268
even more complex decision boundaries and
更复杂的决策边界

369
00:13:45,268 --> 00:13:46,963
logistic regression can be
而逻辑回归

370
00:13:46,963 --> 00:13:48,480
used to find the zero boundaries
可以用于找到决策边界

371
00:13:48,500 --> 00:13:50,093
that may, for example, be
例如

372
00:13:50,093 --> 00:13:52,085
an ellipse like that, or
这样一个椭圆

373
00:13:52,085 --> 00:13:53,503
maybe with a different setting of
或者参数不同的椭圆

374
00:13:53,503 --> 00:13:55,453
the parameters, maybe you
也许你

375
00:13:55,453 --> 00:13:57,834
can get instead a different decision boundary that
可以得到一个不同的决定边界

376
00:13:57,840 --> 00:13:59,776
may even look like, you know, some funny
像这个样子

377
00:13:59,776 --> 00:14:04,145
shape like that.
一些有趣的形状

378
00:14:04,145 --> 00:14:06,423
Or for even more complex examples
或者更为复杂的例子

379
00:14:06,423 --> 00:14:08,915
you can also get decision boundaries
你也可以得到决策边界

380
00:14:08,950 --> 00:14:10,381
that can look like, you know,
看起来这样

381
00:14:10,390 --> 00:14:12,045
more complex shapes like that.
这样更复杂的形状

382
00:14:12,045 --> 00:14:13,365
Where everything in here you
在这个区域

383
00:14:13,365 --> 00:14:15,453
predict Y equals 1, and
你预测y=1

384
00:14:15,453 --> 00:14:17,531
everything outside you predict Y equals 0.
在这个区域外面你预测y=0

385
00:14:17,531 --> 00:14:19,556
So these higher order polynomial
因此 这些高阶多项式

386
00:14:19,560 --> 00:14:23,060
features you can get very complex decision boundaries.
特征变量 可以让你得到非常复杂的决策边界

387
00:14:23,070 --> 00:14:24,786
So with these visualizations, I
因此 通过这些可视化图形

388
00:14:24,786 --> 00:14:26,163
hope that gives you a
我希望告诉你

389
00:14:26,163 --> 00:14:28,623
what's the range of hypothesis
什么范围的假设函数

390
00:14:28,623 --> 00:14:30,676
functions we can represent using
我们可以使用

391
00:14:30,676 --> 00:14:34,966
the representation that we have for logistic regression.
逻辑回归来表示

392
00:14:34,966 --> 00:14:37,713
Now that we know what H of X can represent.
现在我们知道了h(x)表示什么

393
00:14:37,713 --> 00:14:39,004
What I'd like to do next in
在下一个视频中

394
00:14:39,004 --> 00:14:40,560
the following video is talk
我将介绍

395
00:14:40,560 --> 00:14:44,096
about how to automatically choose the parameters theta.
如何自动选择参数θ

396
00:14:44,110 --> 00:14:45,570
So that given a training
使我们能在给定一个训练集时

397
00:14:45,570 --> 00:14:49,359
set we can automatically fit the parameters to our data.
我们可以根据数据自动拟合参数

