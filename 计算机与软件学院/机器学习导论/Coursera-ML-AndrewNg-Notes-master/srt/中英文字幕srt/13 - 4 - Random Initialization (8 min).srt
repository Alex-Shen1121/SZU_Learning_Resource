1
00:00:00,170 --> 00:00:01,340
In this video, I'd like
在这个视频中 我想要
(字幕整理：中国海洋大学 黄海广,haiguang2000@qq.com )

2
00:00:01,450 --> 00:00:03,230
to talk about how to initialize
讨论一下如何初始化

3
00:00:04,580 --> 00:00:05,970
K-means and more importantly, this will
K均值聚类方法 更重要的是 这将

4
00:00:06,170 --> 00:00:07,240
lead into a discussion of
引导我们讨论

5
00:00:07,550 --> 00:00:10,210
how to make K-means avoid local optima as well.
如何避开局部最优来构建K均值聚类方法

6
00:00:10,740 --> 00:00:12,390
Here's the K-means clustering algorithm
这是一个我们之前讨论过的

7
00:00:12,950 --> 00:00:14,420
that we talked about earlier.
K均值聚类算法

8
00:00:15,760 --> 00:00:16,760
One step that we never really
其中我们之前没有

9
00:00:17,260 --> 00:00:18,350
talked much about was this step
讨论得太多的是这一步

10
00:00:18,820 --> 00:00:21,560
of how you randomly initialize the cluster centroids.
如何初始化聚类中心这一步

11
00:00:22,390 --> 00:00:23,490
There are few different ways that
有几种不同的方法

12
00:00:23,710 --> 00:00:25,350
one can imagine using to randomly
可以用来随机

13
00:00:25,960 --> 00:00:26,860
initialize the cluster centroids.
初始化聚类中心

14
00:00:27,510 --> 00:00:28,580
But, it turns out that
但是 事实证明

15
00:00:28,720 --> 00:00:29,820
there is one method that is
有一种方法比其他

16
00:00:30,050 --> 00:00:31,700
much more recommended than most
大多数可能考虑到的方法

17
00:00:32,080 --> 00:00:33,830
of the other options one might think about.
更加被推荐

18
00:00:34,400 --> 00:00:35,250
So, let me tell you about
接下来就告诉你这个

19
00:00:35,590 --> 00:00:38,160
that option since it's what often seems to work best.
方法 因为它可能是效果最好的一种方法

20
00:00:39,550 --> 00:00:42,210
Here's how I usually initialize my cluster centroids.
这里展示了我通常是如何初始化我的聚类中心的

21
00:00:43,300 --> 00:00:44,710
When running K-means, you should have
当运行K均值方法时 你需要有

22
00:00:45,140 --> 00:00:47,160
the number of cluster centroids, K,
一个聚类中心数值K

23
00:00:47,430 --> 00:00:48,520
set to be less than the
K值要比

24
00:00:48,590 --> 00:00:50,090
number of training examples M. It
训练样本的数量m小

25
00:00:50,170 --> 00:00:51,210
would be really weird to run
如果运行一个

26
00:00:51,430 --> 00:00:52,600
K-means with a number
K均值

27
00:00:52,870 --> 00:00:54,270
of cluster centroids that's, you know,
聚类中心数值

28
00:00:54,520 --> 00:00:55,790
equal or greater than the number of examples you have, right?
等于或者大于样本数的K均值聚类方法会很奇怪

29
00:00:58,080 --> 00:00:59,010
So the way I
我通常用来

30
00:00:59,150 --> 00:01:00,510
usually initialize K-means is,
初始化K均值聚类的方法是

31
00:01:00,770 --> 00:01:02,510
I would randomly pick k training
随机挑选K个训练

32
00:01:02,990 --> 00:01:05,170
examples. So, and, what
样本 然后

33
00:01:05,610 --> 00:01:06,730
I do is then set Mu1
我要做的是设定μ1

34
00:01:06,850 --> 00:01:09,320
of MuK equal to these k examples.
到μk让它们等于这个K个样本

35
00:01:10,610 --> 00:01:11,470
Let me show you a concrete example.
让我展示一个具体的例子

36
00:01:12,560 --> 00:01:14,190
Lets say that k is
我们假设 K

37
00:01:14,470 --> 00:01:16,600
equal to 2 and so
等于2 那么

38
00:01:17,070 --> 00:01:19,520
on this example on the right let's say I want to find two clusters.
就在这个例子的右边 假设我们想找到两个聚类

39
00:01:21,170 --> 00:01:22,060
So, what I'm going to
那么为了初始化

40
00:01:22,200 --> 00:01:23,350
do in order to initialize
聚类中心

41
00:01:23,770 --> 00:01:25,340
my cluster centroids is, I'm
我要做的是

42
00:01:25,470 --> 00:01:27,320
going to randomly pick a couple examples.
随机挑选几个样本

43
00:01:27,760 --> 00:01:28,960
And let's say, I pick
比如说 我挑选了

44
00:01:29,230 --> 00:01:31,060
this one and I pick that one.
这个和这个

45
00:01:31,230 --> 00:01:32,320
And the way I'm going
我要

46
00:01:32,380 --> 00:01:34,100
to initialize my cluster centroids
初始化聚类中心的方法

47
00:01:34,310 --> 00:01:35,190
is, I'm just going to initialize
就是 我只需要初始化

48
00:01:36,200 --> 00:01:38,930
my cluster centroids to be right on top of those examples.
聚类中心正确的样本中

49
00:01:39,530 --> 00:01:40,430
So that's my first cluster centroid
因此这是我的第一个聚类中心

50
00:01:41,410 --> 00:01:43,230
and that's my second cluster centroid, and
这是我的第二个聚类中心

51
00:01:43,390 --> 00:01:45,770
that's one random initialization of K-means.
这就是一个随机初始化K均值聚类的方法

52
00:01:48,540 --> 00:01:50,480
The one I drew looks like a particularly good one.
刚刚我画的看上去是相当不错的一个例子

53
00:01:50,890 --> 00:01:51,810
And sometimes I might get less
但是有时候我可能不会

54
00:01:52,040 --> 00:01:53,370
lucky and maybe I'll end
那么幸运 也许我最后

55
00:01:53,510 --> 00:01:54,900
up picking that as my first
会挑选到 这一个是我第一个

56
00:01:55,330 --> 00:01:58,420
random initial example, and that as my second one.
挑选到的初始化样本 而这是第二个

57
00:01:59,050 --> 00:02:01,380
And here I'm picking two examples because k equals 2.
这就是我所挑选的两个样本 因为K等于2

58
00:02:01,590 --> 00:02:03,590
Some we have randomly picked two
我们随机挑选了两个

59
00:02:03,890 --> 00:02:05,030
training examples and if
训练样本 如果

60
00:02:05,100 --> 00:02:06,660
I chose those two then I'll
我挑选这两个 那么我

61
00:02:06,830 --> 00:02:08,040
end up with, may be
结果就有可能得到

62
00:02:08,250 --> 00:02:09,200
this as my first cluster
这个是第一个聚类

63
00:02:09,510 --> 00:02:10,980
centroid and that as
中心 这个是

64
00:02:11,140 --> 00:02:13,560
my second initial location of the cluster centroid.
第二个聚类中心

65
00:02:14,150 --> 00:02:15,690
So, that's how you can randomly
这就是如何随机

66
00:02:16,070 --> 00:02:17,560
initialize the cluster centroids.
初始化聚类中心

67
00:02:17,810 --> 00:02:19,670
And so at initialization, your
因此在初始化时 你的第一个

68
00:02:19,860 --> 00:02:21,110
first cluster centroid Mu1 will
聚类中心μ1

69
00:02:21,270 --> 00:02:23,350
be equal to x(i) for
等于x(i)

70
00:02:23,520 --> 00:02:25,870
some randomly value of i and
对于某一个随机的i值

71
00:02:26,980 --> 00:02:27,660
Mu2 will be equal to x(j)
μ2等于x(j)

72
00:02:29,240 --> 00:02:30,980
for some different randomly chosen value
对应另一个随机选择的不同的

73
00:02:31,380 --> 00:02:32,830
of j and so on,
j的值 等等

74
00:02:32,910 --> 00:02:34,440
if you have more clusters and more cluster centroid.
如果你有更多的聚类和更多的聚类中心的话

75
00:02:35,680 --> 00:02:37,540
And sort of the side common.
通常

76
00:02:38,110 --> 00:02:39,240
I should say that in the
应该这样说

77
00:02:39,320 --> 00:02:40,840
earlier video where I first
前面的视频中 在我第一次

78
00:02:41,150 --> 00:02:43,030
illustrated K-means with the animation.
用动画说明K均值方法时

79
00:02:44,310 --> 00:02:45,070
In that set of slides.
在那些幻灯片中

80
00:02:45,900 --> 00:02:46,890
Only for the purpose of illustration.
仅仅是为了说明

81
00:02:47,590 --> 00:02:48,690
I actually used a different
我实际上用了一种不同的

82
00:02:49,240 --> 00:02:51,750
method of initialization for my cluster centroids.
初始化方法来初始化聚类中心

83
00:02:52,460 --> 00:02:53,790
But the method described on
而这张幻灯片中描述的方法

84
00:02:53,900 --> 00:02:55,940
this slide, this is really the recommended way.
是真正被推荐的方法

85
00:02:56,430 --> 00:02:58,850
And the way that you should probably use, when you implement K-means.
这种方法在你实现K均值聚类的时候可能会用到

86
00:03:00,090 --> 00:03:01,560
So, as they suggested perhaps
根据推荐

87
00:03:02,070 --> 00:03:04,090
by these two illustrations on the right.
也许通过这右边的两个图

88
00:03:04,930 --> 00:03:06,050
You might really guess that K-means
你可能会猜到K均值方法

89
00:03:06,530 --> 00:03:08,130
can end up converging to
最终可能会得到

90
00:03:08,260 --> 00:03:10,150
different solutions depending on
不同的结果 取决于

91
00:03:10,860 --> 00:03:12,470
exactly how the clusters
聚类簇的初始化方法

92
00:03:12,990 --> 00:03:15,170
were initialized, and so, depending on the random initialization.
因此也就取决于随机的初始化

93
00:03:16,280 --> 00:03:18,180
K-means can end up at different solutions.
K均值方法最后可能得到不同的结果

94
00:03:18,930 --> 00:03:22,560
And, in particular, K-means can actually end up at local optima.
尤其是如果K均值方法落在局部最优的时候

95
00:03:23,650 --> 00:03:24,920
If you're given the data sale like this.
如果给你一些数据 比如说这些

96
00:03:25,400 --> 00:03:26,370
Well, it looks like, you know, there
这看起来好像有

97
00:03:26,660 --> 00:03:28,340
are three clusters, and so,
3个聚类 那么

98
00:03:28,780 --> 00:03:30,090
if you run K-means and if
如果你运行K均值方法 如果

99
00:03:30,150 --> 00:03:31,380
it ends up at a good
它最后得到一个

100
00:03:31,820 --> 00:03:32,910
local optima this might be
局部最优 这可能是

101
00:03:33,040 --> 00:03:35,830
really the global optima, you might end up with that cluster ring.
真正的全局最优 你可能会得到这样的聚类结果

102
00:03:36,820 --> 00:03:38,440
But if you had a particularly
但是如果你运气特别

103
00:03:39,110 --> 00:03:41,630
unlucky, random initialization, K-means
不好 随机初始化 K均值方法

104
00:03:42,100 --> 00:03:43,660
can also get stuck at different
也可能会卡在不同的

105
00:03:44,180 --> 00:03:45,740
local optima. So, in
局部最优上面 因此在

106
00:03:45,850 --> 00:03:47,330
this example on the left
有变的这个例子中

107
00:03:47,620 --> 00:03:48,700
it looks like this blue cluster has captured
看上去蓝色的聚类捕捉到了

108
00:03:49,470 --> 00:03:51,700
a lot of points of the left and then the they were on the green clusters
左边的很多点 而且它们在绿色的聚类中

109
00:03:52,050 --> 00:03:54,810
each is captioned on the relatively small number of points.
每一个聚类都捕捉到了相对较少的点

110
00:03:55,020 --> 00:03:56,480
And so, this corresponds to
这与

111
00:03:56,640 --> 00:03:58,470
a bad local optima because it
不好的局部最优相对应 因为

112
00:03:58,530 --> 00:04:00,060
has basically taken these two
它基本上是基于这两个

113
00:04:00,470 --> 00:04:01,560
clusters and used them into
聚类的 并且它们

114
00:04:01,780 --> 00:04:03,440
1 and furthermore, has
进一步合并成了1个聚类 而

115
00:04:04,150 --> 00:04:06,070
split the second cluster into
把第二个聚类分割成了

116
00:04:06,580 --> 00:04:09,170
two separate sub-clusters like
两个像这样的小的聚类

117
00:04:09,380 --> 00:04:10,270
so, and it has also
它也把

118
00:04:10,720 --> 00:04:12,280
taken the second cluster and
第二个聚类

119
00:04:12,540 --> 00:04:14,220
split it into two
分割成了两个

120
00:04:14,460 --> 00:04:16,630
separate sub-clusters like so, and
分别的像这样的小聚类簇

121
00:04:16,760 --> 00:04:17,880
so, both of these
这两个

122
00:04:18,000 --> 00:04:18,970
examples on the lower
右下方的例子

123
00:04:19,220 --> 00:04:20,890
right correspond to different local
对应与K均值方法的

124
00:04:21,250 --> 00:04:22,440
optima of K-means and in fact,
不同的局部最优 实际上

125
00:04:22,890 --> 00:04:24,440
in this example here,
这里的这个例子

126
00:04:25,070 --> 00:04:26,150
the cluster, the red cluster
这个红色的簇

127
00:04:26,550 --> 00:04:27,870
has captured only a single optima example.
只捕捉到了一个最好的样本

128
00:04:28,380 --> 00:04:29,810
And the term local
这个局部

129
00:04:30,200 --> 00:04:31,000
optima, by the way, refers
最优项 顺便提一下 代表

130
00:04:31,490 --> 00:04:32,930
to local optima of this
这个失真函数J

131
00:04:33,190 --> 00:04:35,940
distortion function J, and
的局部最优

132
00:04:36,320 --> 00:04:38,380
what these solutions on the
这些在右下方的解

133
00:04:38,590 --> 00:04:39,830
lower left, what these local
这些局部

134
00:04:40,120 --> 00:04:41,420
optima correspond to is
最优所对应的是

135
00:04:41,530 --> 00:04:42,880
really solutions where K-means
真正的K均值方法

136
00:04:43,330 --> 00:04:44,050
has gotten stuck to the local
所遇到的局部

137
00:04:44,600 --> 00:04:45,940
optima and it's not doing
最优 且

138
00:04:46,170 --> 00:04:47,940
a very good job minimizing this
通过最小化这个

139
00:04:48,110 --> 00:04:50,030
distortion function J. So,
失真函数J并不能得到很好的结果 因此

140
00:04:50,540 --> 00:04:52,250
if you're worried about K-means getting
如果你担心K均值方法会遇到

141
00:04:52,540 --> 00:04:53,810
stuck in local optima, if
局部最优的问题 如果

142
00:04:53,970 --> 00:04:55,110
you want to increase the odds
你想提高

143
00:04:55,330 --> 00:04:56,950
of K-means finding the best
K均值方法找到最

144
00:04:57,230 --> 00:04:58,480
possible clustering, like that shown
有可能的聚类的几率的话

145
00:04:58,730 --> 00:05:00,290
on top here, what we
就像这上面所展示的 我们能做的

146
00:05:00,350 --> 00:05:02,820
can do, is try multiple, random initializations.
是尝试多次 随机的初始化

147
00:05:03,580 --> 00:05:04,820
So, instead of just initializing K-means
而不是仅仅初始化一次K均值方法

148
00:05:05,430 --> 00:05:06,460
once and hopping that that
就希望它会得到

149
00:05:06,670 --> 00:05:07,680
works, what we can do
很好的结果 我们能做的是

150
00:05:08,040 --> 00:05:10,020
is, initialize K-means lots of
初始化K均值很多次

151
00:05:10,130 --> 00:05:10,990
times and run K-means lots of
并运行K均值方法很多次

152
00:05:11,890 --> 00:05:12,870
times, and use that to
通过多次尝试

153
00:05:12,950 --> 00:05:13,840
try to make sure we get
来保证我们最终能得到

154
00:05:14,110 --> 00:05:15,640
as good a solution, as
一个足够好的结果 一个

155
00:05:15,800 --> 00:05:18,380
good a local or global optima as possible.
尽可能局部或全局最优的结果

156
00:05:19,480 --> 00:05:22,460
Concretely, here's how you could go about doing that.
具体地 这就是你能够做的

157
00:05:22,720 --> 00:05:23,500
Let's say, I decide to run
假如我决定运行

158
00:05:23,700 --> 00:05:24,800
K-meanss a hundred times
K均值方法一百次

159
00:05:25,160 --> 00:05:26,790
so I'll execute this loop
那么我就需要执行这个循环

160
00:05:27,060 --> 00:05:28,900
a hundred times and it's
100次

161
00:05:29,330 --> 00:05:30,830
fairly typical a number of
这是一个相当典型的次数数字

162
00:05:30,920 --> 00:05:31,910
times when came to will be
有时会是

163
00:05:32,160 --> 00:05:33,670
something from 50 up to may be 1000.
从50到1000之间

164
00:05:35,090 --> 00:05:36,730
So, let's say you decide to say K-means one hundred times.
假设说有决定运行K均值方法100次

165
00:05:38,220 --> 00:05:39,100
So what that means is that
那么这就意味这

166
00:05:39,170 --> 00:05:41,490
we would randomnly initialize K-means.
我们要随机初始化K均值方法

167
00:05:42,350 --> 00:05:43,250
And for each of
对于这些

168
00:05:43,340 --> 00:05:44,710
these one hundred random intializations
100次随机初始化的每一次

169
00:05:45,370 --> 00:05:47,040
we would run K-means and
我们需要运行K均值方法

170
00:05:47,220 --> 00:05:48,200
that would give us a set
我们会得到一系列

171
00:05:48,430 --> 00:05:50,270
of clusteringings, and a set of cluster
聚类结果 和一系列聚类

172
00:05:50,590 --> 00:05:51,940
centroids, and then we
中心 之后

173
00:05:52,040 --> 00:05:53,760
would then compute the distortion J,
我们可以计算失真函数J

174
00:05:54,500 --> 00:05:55,600
that is compute this cause function on
用我们得到的

175
00:05:56,910 --> 00:05:58,260
the set of cluster assignments
这些聚类结果

176
00:05:58,720 --> 00:05:59,910
and cluster centroids that we got.
和聚类中心来计算这样一个结果函数

177
00:06:01,000 --> 00:06:03,470
Finally, having done this whole procedure a hundred times.
最后 完成整个过程100次之后

178
00:06:04,450 --> 00:06:06,330
You will have a hundred different ways
你会得到这个100种

179
00:06:06,710 --> 00:06:08,990
of clustering the data and then
聚类数据的这些方法

180
00:06:09,240 --> 00:06:10,310
finally what you do
最后你要做的是

181
00:06:10,590 --> 00:06:11,510
is all of these hundred
在所有这100种

182
00:06:11,820 --> 00:06:13,210
ways you have found of clustering the data,
用于聚类的方法中

183
00:06:13,800 --> 00:06:16,050
just pick one, that gives us the lowest cost.
选取能够给我们代价最小的一个

184
00:06:16,400 --> 00:06:18,480
That gives us the lowest distortion.
给我们最低畸变值的一个

185
00:06:18,960 --> 00:06:20,610
And it turns out that
事实证明

186
00:06:21,170 --> 00:06:22,490
if you are running K-means with
如果你运行K均值方法时

187
00:06:22,670 --> 00:06:24,520
a fairly small number of
所用的聚类数相当小

188
00:06:24,630 --> 00:06:25,260
clusters , so you know if the number
那么如果聚类

189
00:06:25,520 --> 00:06:26,700
of clusters is anywhere from
数是从

190
00:06:26,760 --> 00:06:28,180
two up to maybe 10 -
2到10之间的任何数的话

191
00:06:28,980 --> 00:06:30,650
then doing multiple random initializations
做多次的随机初始化

192
00:06:31,460 --> 00:06:32,880
can often, can sometimes make
通常能够保证

193
00:06:32,990 --> 00:06:34,430
sure that you find a better local optima.
你能有一个较好的局部最优解

194
00:06:34,690 --> 00:06:37,680
Make sure you find the better clustering data.
保证你能找到更好的聚类数据

195
00:06:37,870 --> 00:06:38,930
But if K is very large, so, if
但是如果K非常大的话 如果

196
00:06:39,080 --> 00:06:40,000
K is much greater than 10,
K比10大很多

197
00:06:40,160 --> 00:06:41,010
certainly if K were, you
当然如果K是

198
00:06:41,080 --> 00:06:42,340
know, if you were trying to
如果你尝试去

199
00:06:42,400 --> 00:06:44,050
find hundreds of clusters, then,
找到成百上千个聚类 那么

200
00:06:45,840 --> 00:06:47,310
having multiple random initializations is
有多个随机初始化就

201
00:06:47,940 --> 00:06:49,220
less likely to make a huge difference
不太可能会有太大的影响

202
00:06:49,360 --> 00:06:50,400
and there is a much
更有

203
00:06:50,590 --> 00:06:51,910
higher chance that your first
可能你的第一次

204
00:06:52,320 --> 00:06:53,610
random initialization will give
随机初始化就会给

205
00:06:53,730 --> 00:06:55,380
you a pretty decent solution already
你相当好的结果

206
00:06:56,590 --> 00:06:58,070
and doing, doing multiple random
做多次随机

207
00:06:58,680 --> 00:07:00,060
initializations will probably give
初始化可能会给

208
00:07:00,260 --> 00:07:02,500
you a slightly better solution but, but maybe not that much.
你稍微好一点的结果 但是不会好太多

209
00:07:02,780 --> 00:07:04,230
But it's really in the regime of where
但是在这样一个

210
00:07:04,540 --> 00:07:05,810
you have a relatively small number
聚类数相对较小的体系里

211
00:07:06,090 --> 00:07:07,740
of clusters, especially if you
特别是如果你

212
00:07:08,040 --> 00:07:09,080
have, maybe 2 or 3
有2个或者3个

213
00:07:09,150 --> 00:07:10,550
or 4 clusters that random
或者4个聚类的话 随机

214
00:07:11,140 --> 00:07:13,790
initialization could make a huge difference in terms of
初始化会有较大的影响

215
00:07:14,190 --> 00:07:15,090
making sure you do a good
可以保证你在

216
00:07:15,170 --> 00:07:16,920
job minimizing the distortion
最小化失真函数的时候得到一个很小的值

217
00:07:17,560 --> 00:07:18,730
function and giving you a good clustering.
并且能得到一个很好的聚类结果

218
00:07:21,390 --> 00:07:22,560
So, that's K-means
这就是随机初始化

219
00:07:22,640 --> 00:07:23,300
with random initialization.
的K均值初始化方法

220
00:07:24,350 --> 00:07:25,570
If you're trying to learn a
如果你尝试学习一种

221
00:07:25,710 --> 00:07:26,950
clustering with a relatively small
聚类数目相对较小

222
00:07:27,310 --> 00:07:28,250
number of clusters, 2, 3,
的聚类方法 如2,3

223
00:07:28,400 --> 00:07:30,540
4, 5, maybe, 6, 7, using
4,5,6,7 用

224
00:07:31,660 --> 00:07:34,040
multiple random initializations can
多次随机初始化

225
00:07:34,380 --> 00:07:36,830
sometimes, help you find much better clustering of the data.
有时能够帮助你找到更好的数据聚类结果

226
00:07:37,680 --> 00:07:39,650
But, even if you are learning a large number of clusters, the initialization, the random
但是 尽管你有很多聚类数目 初始化

227
00:07:40,350 --> 00:07:43,280
initialization method that I describe here.
我在这里介绍的随机初始化

228
00:07:43,520 --> 00:07:45,110
That should give K-means a
它会给K均值方法一个

229
00:07:45,370 --> 00:07:46,680
reasonable starting point to start
合理的起始点来开始

230
00:07:47,030 --> 00:07:48,580
from for finding a good set of clusters.
并找到一个好的聚类结果 【教育无边界字幕组】翻译: 星星之火 校对/审核:所罗门捷列夫

